{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Preparation Pipeline\n",
    "\n",
    "This notebook documents the full preprocessing pipeline from externally provided experimental data\n",
    "to the final machine-learning–ready feature sets (`ml_features_*.csv`).\n",
    "\n",
    "**Note on data availability**\n",
    "All data files used in this pipeline contain either (i) the experimental stimulus texts or (ii) eye-tracking data collected during the reading experiment.\n",
    "These materials were collected and are currently being prepared for publication by Oksana Ivchenko.\n",
    "Because they are not yet publicly released, I cannot redistribute them here.\n",
    "\n",
    "For transparency, this notebook documents all preprocessing steps.\n",
    "Synthetic placeholder files are provided in the repository so that the pipeline can still be executed to illustrate the workflow end-to-end.\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs (external, not created by this repository)\n",
    "\n",
    "- **`ETdata_oksana_for_constantin_2025-04-22-old.csv`**\n",
    "  AOI-level eye-tracking feature export from the experiment (provided by Oksana Ivchenko).\n",
    "  Contains participant and session metadata, text identifiers, and aggregated eye-tracking measures per Area of Interest (AOI).\n",
    "  This file represents processed AOI features, not raw gaze streams.\n",
    "\n",
    "- **`eye_tracking_data.csv`**\n",
    "  Annotated stimulus texts and AOI metadata (provided by supervisor).\n",
    "  Contains word and sentence indices, AOI surface forms, and annotation tags.\n",
    "  Does not include participant information or eye-tracking features.\n",
    "\n",
    "- **`ann_materials_with-lemmas_2025-04-22.csv`**\n",
    "  Supervisor-corrected materials annotation.\n",
    "  Derived from an earlier corrected version of the automatically parsed materials.\n",
    "\n",
    "---\n",
    "\n",
    "## Outputs (generated by this pipeline)\n",
    "\n",
    "The following intermediate and final datasets are created from the external inputs.\n",
    "They can all be regenerated by running the steps in this notebook.\n",
    "\n",
    "### Stimulus Texts Branch\n",
    "- `et_data_indexed_freq.csv`\n",
    "  Text materials with word and lemma frequency counts.\n",
    "- `et_data_indexed_freq_deps_pos_lemmas.csv` / `et_data_indexed_freq_deps_pos_lemmas_v2.csv`\n",
    "  Texts enriched with POS tags, lemmas, and dependency parses using spaCy.\n",
    "- `et_data_lemmas_indexed.csv`\n",
    "  Materials with unique lemma IDs and counts, used as the basis for AOI dictionaries.\n",
    "- `aoi_dict.json`\n",
    "  AOIs grouped by base key, storing concatenated text and AOI–ID tuples.\n",
    "- `slim_dict.json`\n",
    "  Simplified AOI dictionary used for API prompting.\n",
    "\n",
    "### API Annotation Branch\n",
    "- `responses.csv`\n",
    "  Raw API responses from the tokenization step.\n",
    "- `responses_with_lemmas.csv`\n",
    "  API responses from the lemmatization and POS tagging step.\n",
    "- `parsed_responses.csv`\n",
    "  Structured token/lemma/POS table parsed from API outputs.\n",
    "- `materials_parsed.csv` (=`parsed_materials.csv`)\n",
    "  Materials aligned with AOIs after API annotation.\n",
    "- `corrected_parsed_materials.csv`\n",
    "  Manually corrected version of `materials_parsed.csv`.\n",
    "- `ann_materials_with-lemmas_2025-04-22.csv`\n",
    "  Supervisor-corrected materials.\n",
    "- `ann_materials_with_lemmas_and_frequencies_2025_06_17.csv`\n",
    "  Supervisor-corrected materials enriched with lemma frequency counts.\n",
    "\n",
    "### Merged Dataset\n",
    "- `et_data_merged_with_ann_materials_25_06_17.csv`\n",
    "  Eye-tracking features merged with enriched annotated materials via `id.global.aoi`.\n",
    "\n",
    "### Aggregated Features for Machine Learning\n",
    "- `ml_features_all_*.csv`\n",
    "  Aggregated features across all AOIs.\n",
    "- `ml_features_medical_*.csv`\n",
    "  Aggregated features restricted to medical AOIs.\n",
    "- `ml_features_non_medical_*.csv`\n",
    "  Aggregated features restricted to non-medical AOIs.\n",
    "- `ml_features_content_*.csv`\n",
    "  Aggregated features restricted to content-word AOIs.\n"
   ],
   "id": "51fb9431ce9e9abe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Setup and Imports\n",
    "\n",
    "This section loads the core Python libraries required for data preparation.\n",
    "At this stage, only general-purpose packages are needed:\n",
    "\n",
    "- **pandas**: data manipulation and tabular processing\n",
    "- **pathlib**: convenient and platform-independent file paths\n",
    "\n",
    "As the pipeline develops, additional imports (e.g. for linguistic annotation, machine learning, or visualization) will be added here.\n"
   ],
   "id": "e4e205c4cccc1ea6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T15:39:11.964775Z",
     "start_time": "2025-10-20T15:39:09.838855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from pathlib import Path\n",
    "import re, json, spacy\n",
    "\n",
    "from tensorflow.python.tpu.ops.gen_tpu_embedding_ops import merge_dedup_data"
   ],
   "id": "86735e400904f49",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-20 17:39:10.641782: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-20 17:39:10.643220: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-20 17:39:10.670833: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-20 17:39:10.671798: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-20 17:39:11.311275: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Synthetic Example Data for Reproducibility\n",
    "\n",
    "Because the original experimental eye-tracking data and annotated materials\n",
    "cannot be publicly distributed, this notebook generates a pair of **synthetic datasets**\n",
    "that reproduce the structure, schema, and processing logic of the real inputs.\n",
    "These files allow the entire preprocessing pipeline to be executed end-to-end\n",
    "without access to restricted data.\n",
    "\n",
    "### Files Produced\n",
    "\n",
    "**1. `00_input_eye_tracking_data_dummy.csv`**\n",
    "Simulates the structure of the original `eye_tracking_data.csv`.\n",
    "Each row represents an annotated *Area of Interest* (AOI), corresponding to a word or short phrase\n",
    "within clinical or medical reading materials.\n",
    "The dataset covers both *original* and *simplified* text versions.\n",
    "\n",
    "**2. `00_input_eye_tracking_data_with_metrics_dummy.csv`**\n",
    "Extends the AOI dataset by adding **synthetic participant-level eye-tracking metrics**,\n",
    "mimicking the original file `ETdata_oksana_for_constantin_2025-04-22-old.csv`.\n",
    "Each AOI entry is replicated across multiple participants, and artificial\n",
    "fixation, visit, and saccade measures are generated to preserve the numerical range and\n",
    "distribution of the real experimental data.\n",
    "\n",
    "### Design Characteristics\n",
    "- Preserves the **column names and data types** of the original files.\n",
    "- Includes French AOI content with:\n",
    "  - contractions and apostrophes (*d’orthopédie*, *l’hôpital*),\n",
    "  - diacritics (*âgé*, *orthopédie*),\n",
    "  - punctuation within AOIs (*clinique.*, *fracture.*).\n",
    "- Simulates both **medical** and **clinical** text types in *original* and *simplified* versions.\n",
    "- Introduces **word repetitions** to validate frequency-counting logic.\n",
    "- Adds **participant metadata** (age, expertise, study background) for multiple simulated readers.\n",
    "- Generates randomized but realistic numeric ranges for fixation and visit metrics.\n",
    "\n",
    "### Purpose\n",
    "These synthetic datasets serve as **reproducible stand-ins** for confidential experimental data.\n",
    "They make it possible to:\n",
    "- test and validate preprocessing functions,\n",
    "- run all feature-extraction and merging steps, and\n",
    "- publish the complete pipeline transparently while keeping participant data private.\n",
    "\n",
    "The real experimental data remains unpublished and cannot be shared\n",
    "until released by its original collector.\n"
   ],
   "id": "11d5e6313c763490"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T15:39:20.238617Z",
     "start_time": "2025-10-20T15:39:20.194658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def create_dummy_eye_tracking_data_french_realistic(output_dir=\"raw\"):\n",
    "    \"\"\"\n",
    "    Create a minimal synthetic dataset mimicking the structure of\n",
    "    `eye_tracking_data.csv` used in the experiment.\n",
    "\n",
    "    The dataset simulates French clinical and medical reading materials,\n",
    "    including both \"original\" and \"simplified\" text versions. Each entry\n",
    "    represents an annotated Area of Interest (AOI), corresponding to\n",
    "    individual words or short word groups.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_dir : str or Path, optional\n",
    "        Directory where the synthetic dataset will be saved (default: 'raw/').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Synthetic AOI-level dataset preserving the schema of the original\n",
    "        experimental input file.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Representative AOI data drawn from simulated original and simplified texts\n",
    "    data = {\n",
    "        \"Media\": [\n",
    "            \"clinical_001_original_1\",\n",
    "            \"clinical_001_original_1\",\n",
    "            \"clinical_001_original_1\",\n",
    "            \"clinical_001_original_1\",\n",
    "            \"clinical_001_original_2\",\n",
    "            \"medical_002_simplified_1\",\n",
    "            \"medical_002_simplified_1\",\n",
    "            \"medical_002_simplified_1\",\n",
    "            \"medical_002_simplified_1\",\n",
    "            \"medical_002_simplified_1\"\n",
    "        ],\n",
    "        \"textid\": [1, 1, 1, 1, 1, 2, 2, 2, 2, 2],\n",
    "        \"text.type\": [\"clinical\"] * 5 + [\"medical\"] * 5,\n",
    "        \"text_version\": [\"original\"] * 5 + [\"simplified\"] * 5,\n",
    "        \"screenid\": [1, 1, 1, 1, 2, 1, 1, 1, 1, 1],\n",
    "        \"Sentence_index\": [1, 1, 2, 2, 2, 1, 1, 1, 2, 2],\n",
    "        \"Word_index\": [1, 2, 1, 2, 3, 1, 2, 3, 1, 2],\n",
    "        \"word_id_screen\": [1, 2, 3, 4, 5, 1, 2, 3, 4, 5],\n",
    "        \"word_id_text\": [1, 2, 3, 4, 5, 1, 2, 3, 4, 5],\n",
    "        \"AOI_ann\": [\n",
    "            \"Cas\", \"clinique.\", \"Un\", \"patient\", \"âgé\",\n",
    "            \"Le\", \"patient\", \"souffre\", \"d’orthopédie\", \"fracture.\"\n",
    "        ],\n",
    "        \"AOI_nopunct\": [\n",
    "            \"Cas\", \"clinique\", \"Un\", \"patient\", \"âgé\",\n",
    "            \"Le\", \"patient\", \"souffre\", \"d’orthopédie\", \"fracture\"\n",
    "        ],\n",
    "        \"AOI_length\": [3, 8, 2, 7, 3, 2, 7, 7, 11, 8],\n",
    "        \"is.in.bracket\": [False] * 10,\n",
    "        \"id.phrase.in.brackets\": [None] * 10,\n",
    "        \"AOI.that.in.fact.should.be\": [None] * 10,\n",
    "        # Medical-tagging placeholders left empty for later use\n",
    "        \"tag\": [None] * 10,\n",
    "        \"tag.type\": [1, None, None, None, None, 2, None, 5, 6, 7],\n",
    "        \"tag.id\": [None] * 10,\n",
    "        \"annotated_text\": [\n",
    "            \"NA\", \"NA\", \"NA\", \"NA\", \"NA\",\n",
    "            \"NA\", \"NA\", \"NA\", \"orthopédie\", \"fracture\"\n",
    "        ],\n",
    "        \"Sentence_id_current\": [1, 1, 2, 2, 2, 1, 1, 1, 2, 2],\n",
    "        \"Sentence_id_match\": [1, 1, 2, 2, 2, 1, 1, 1, 2, 2],\n",
    "        \"id.piece\": list(range(10)),\n",
    "        \"filename.ann\": [\n",
    "            \"001_tagged_finished_words_original.csv\"\n",
    "        ] * 5 + [\n",
    "            \"002_tagged_finished_words_simplified.csv\"\n",
    "        ] * 5,\n",
    "        \"id.global.aoi\": [\n",
    "            \"clinical_001_original_1-1-1\",\n",
    "            \"clinical_001_original_1-1-2\",\n",
    "            \"clinical_001_original_1-2-1\",\n",
    "            \"clinical_001_original_1-2-2\",\n",
    "            \"clinical_001_original_2-2-3\",\n",
    "            \"medical_002_simplified_1-1-1\",\n",
    "            \"medical_002_simplified_1-1-2\",\n",
    "            \"medical_002_simplified_1-1-3\",\n",
    "            \"medical_002_simplified_1-2-1\",\n",
    "            \"medical_002_simplified_1-2-2\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Construct the DataFrame\n",
    "    df_dummy = pd.DataFrame(data)\n",
    "\n",
    "    # Save the dataset to file\n",
    "    output_file = output_path / \"00_input_eye_tracking_data_dummy.csv\"\n",
    "    df_dummy.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "\n",
    "    print(f\"Dummy AOI dataset created: {df_dummy.shape}\")\n",
    "    return df_dummy\n",
    "\n",
    "\n",
    "def create_dummy_eye_tracking_data_from_aoi(\n",
    "    input_file=\"raw/00_input_eye_tracking_data_dummy.csv\",\n",
    "    output_dir=\"raw\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate synthetic eye-tracking metrics aligned to a dummy AOI dataset.\n",
    "\n",
    "    This function extends the AOI-level dummy data by assigning randomised\n",
    "    fixation and saccade metrics, simulating the structure of the original\n",
    "    experimental eye-tracking data. Each AOI entry is replicated for several\n",
    "    participants, with metadata describing demographic and study background.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_file : str or Path\n",
    "        Path to the dummy AOI dataset.\n",
    "    output_dir : str or Path\n",
    "        Directory where the synthetic eye-tracking dataset will be saved.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Synthetic eye-tracking dataset with participant-level variation.\n",
    "    \"\"\"\n",
    "    # Load AOI-level base data\n",
    "    df_aoi = pd.read_csv(input_file, sep=\"\\t\", dtype=str).fillna(\"\")\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Define participant metadata\n",
    "    participants_info = [\n",
    "        {\n",
    "            \"Participant\": \"Participant10\",\n",
    "            \"set1\": \"1_A\",\n",
    "            \"set2\": \"A\",\n",
    "            \"age\": 22,\n",
    "            \"sex\": \"W\",\n",
    "            \"Payment\": \"X\",\n",
    "            \"is.expert\": \"non-expert\",\n",
    "            \"educational_background\": \"Licence\",\n",
    "            \"field_of_study\": \"3ème année de psychologie\",\n",
    "            \"current_situation\": \"étudiante\",\n",
    "            \"current_situation_old\": \"student\",\n",
    "            \"Participant_commnts_new\": \"NA\",\n",
    "        },\n",
    "        {\n",
    "            \"Participant\": \"Participant11\",\n",
    "            \"set1\": \"1_B\",\n",
    "            \"set2\": \"B\",\n",
    "            \"age\": 26,\n",
    "            \"sex\": \"M\",\n",
    "            \"Payment\": \"X\",\n",
    "            \"is.expert\": \"expert\",\n",
    "            \"educational_background\": \"Master\",\n",
    "            \"field_of_study\": \"5ème année de médecine\",\n",
    "            \"current_situation\": \"interne à l’hôpital\",\n",
    "            \"current_situation_old\": \"medical_resident\",\n",
    "            \"Participant_commnts_new\": \"NA\",\n",
    "        },\n",
    "        {\n",
    "            \"Participant\": \"Participant12\",\n",
    "            \"set1\": \"2_A\",\n",
    "            \"set2\": \"A\",\n",
    "            \"age\": 24,\n",
    "            \"sex\": \"W\",\n",
    "            \"Payment\": \"Y\",\n",
    "            \"is.expert\": \"non-expert\",\n",
    "            \"educational_background\": \"Licence\",\n",
    "            \"field_of_study\": \"sciences cognitives\",\n",
    "            \"current_situation\": \"étudiante\",\n",
    "            \"current_situation_old\": \"student\",\n",
    "            \"Participant_commnts_new\": \"NA\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # Build full participant-level dataset\n",
    "    for pinfo in participants_info:\n",
    "        for _, row in df_aoi.iterrows():\n",
    "            # Copy all original AOI columns\n",
    "            base_row = row.to_dict()\n",
    "\n",
    "            # Generate synthetic fixation metrics\n",
    "            total_fix = np.random.randint(100, 800)\n",
    "            avg_fix = round(np.random.uniform(100, total_fix), 2)\n",
    "            min_fix = np.random.randint(50, int(avg_fix))\n",
    "            max_fix = np.random.randint(int(avg_fix), total_fix + 100)\n",
    "            num_fix = np.random.randint(1, 4)\n",
    "            ttf = np.random.randint(8000, 16000)\n",
    "            dur_first = np.random.randint(50, 400)\n",
    "            pupil = round(np.random.uniform(2.2, 3.2), 5)\n",
    "            num_visit = np.random.randint(1, 3)\n",
    "\n",
    "            # --- Whole-fixation metrics ---\n",
    "            total_whole_fix = total_fix + np.random.randint(50, 150)\n",
    "            avg_whole_fix = round(np.random.uniform(80, total_whole_fix / num_fix), 2)\n",
    "            min_whole_fix = np.random.randint(40, int(avg_whole_fix))\n",
    "            max_whole_fix = np.random.randint(int(avg_whole_fix), total_whole_fix + 100)\n",
    "            num_whole_fix = num_fix + np.random.randint(0, 2)\n",
    "            ttf_whole = ttf + np.random.randint(-200, 200)\n",
    "            dur_first_whole = np.random.randint(60, 400)\n",
    "            pupil_whole = round(np.random.uniform(2.1, 3.3), 5)\n",
    "\n",
    "            # Add participant-level and metric information\n",
    "            base_row.update({\n",
    "                \"list\": pinfo[\"set1\"],\n",
    "                \"Participant\": pinfo[\"Participant\"],\n",
    "                \"Participant_unique\": f\"{pinfo['Participant']}-{pinfo['set1']}\",\n",
    "                \"age\": pinfo[\"age\"],\n",
    "                \"sex\": pinfo[\"sex\"],\n",
    "                \"Payment\": pinfo[\"Payment\"],\n",
    "                \"is.expert\": pinfo[\"is.expert\"],\n",
    "                \"educational_background\": pinfo[\"educational_background\"],\n",
    "                \"field_of_study\": pinfo[\"field_of_study\"],\n",
    "                \"current_situation\": pinfo[\"current_situation\"],\n",
    "                \"current_situation_old\": pinfo[\"current_situation_old\"],\n",
    "                \"Participant_commnts_new\": pinfo[\"Participant_commnts_new\"],\n",
    "                \"Recording\": f\"Recording{random.randint(10,20)}\",\n",
    "                \"situation_actuelle\": pinfo[\"current_situation\"],\n",
    "                \"Timeline\": f\"Timeline_{pinfo['set1']}\",\n",
    "                \"Media_old\": f\"{row['Media'].split('_')[0]}_{row['textid']}_{row['text_version']}\",\n",
    "                \"id.global.aoi.participant\": f\"{pinfo['Participant']}-{row['id.global.aoi']}\",\n",
    "                # Fixation metrics\n",
    "                \"Total_duration_of_fixations\": total_fix,\n",
    "                \"Average_duration_of_fixations\": avg_fix,\n",
    "                \"Minimum_duration_of_fixations\": min_fix,\n",
    "                \"Maximum_duration_of_fixations\": max_fix,\n",
    "                \"Number_of_fixations\": num_fix,\n",
    "                \"Time_to_first_fixation\": ttf,\n",
    "                \"Duration_of_first_fixation\": dur_first,\n",
    "                \"Average_pupil_diameter\": pupil,\n",
    "                # Whole-fixation metrics\n",
    "                \"Total_duration_of_whole_fixations\": total_whole_fix,\n",
    "                \"Average_duration_of_whole_fixations\": avg_whole_fix,\n",
    "                \"Minimum_duration_of_whole_fixations\": min_whole_fix,\n",
    "                \"Maximum_duration_of_whole_fixations\": max_whole_fix,\n",
    "                \"Number_of_whole_fixations\": num_whole_fix,\n",
    "                \"Time_to_first_whole_fixation\": ttf_whole,\n",
    "                \"Duration_of_first_whole_fixation\": dur_first_whole,\n",
    "                \"Average_whole-fixation_pupil_diameter\": pupil_whole,\n",
    "                # Visit metrics\n",
    "                \"Total_duration_of_Visit\": total_fix,\n",
    "                \"Average_duration_of_Visit\": avg_fix,\n",
    "                \"Minimum_duration_of_Visit\": min_fix,\n",
    "                \"Maximum_duration_of_Visit\": max_fix,\n",
    "                \"Number_of_Visits\": num_visit,\n",
    "                \"Time_to_first_Visit\": ttf,\n",
    "                \"Duration_of_first_Visit\": dur_first,\n",
    "                # Saccades and regressions\n",
    "                \"Number_of_saccades_in_AOI\": np.random.randint(0, 3),\n",
    "                \"Time_to_entry_saccade\": ttf - 40,\n",
    "                \"Time_to_exit_saccade\": ttf + 50,\n",
    "                \"Peak_velocity_of_entry_saccade\": round(np.random.uniform(180, 280), 2),\n",
    "                \"Peak_velocity_of_exit_saccade\": round(np.random.uniform(180, 280), 2),\n",
    "                \"First-pass_first_fixation_duration\": dur_first,\n",
    "                \"First-pass_duration\": total_fix,\n",
    "                \"Regression-path_duration\": 0,\n",
    "                \"Selective_regression-path_duration\": 0,\n",
    "                \"First-pass_regression\": 0,\n",
    "                \"Re-reading_duration\": 0,\n",
    "                # Eye openness\n",
    "                \"Average_eye_openness\": round(np.random.uniform(2.4, 2.8), 3),\n",
    "                \"Average_whole-fixation_eye_openness\": round(np.random.uniform(2.3, 2.7), 3),\n",
    "                # Filename placeholder\n",
    "                \"filename.et\": \"eye_tracking_metrics_dummy.xlsx\",\n",
    "            })\n",
    "\n",
    "            rows.append(base_row)\n",
    "\n",
    "    # Assemble final DataFrame\n",
    "    df_eye = pd.DataFrame(rows)\n",
    "\n",
    "    # Export to file\n",
    "    output_file = output_path / \"00_input_eye_tracking_data_with_metrics_dummy.csv\"\n",
    "    df_eye.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "    print(f\"Created dummy eye-tracking data aligned with AOIs: {df_eye.shape} rows\")\n",
    "    return df_eye\n",
    "\n",
    "\n",
    "# Generate both synthetic datasets\n",
    "dummy_df = create_dummy_eye_tracking_data_french_realistic()\n",
    "dummy_eye_df = create_dummy_eye_tracking_data_from_aoi(\"raw/00_input_eye_tracking_data_dummy.csv\")"
   ],
   "id": "cf4177e0aeab94bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy AOI dataset created: (10, 24)\n",
      "Created dummy eye-tracking data aligned with AOIs: (30, 78) rows\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 1: Build AOI dictionary\n",
    "\n",
    "**Input**\n",
    "`00_input_eye_tracking_data.csv` (external, provided by supervisor)\n",
    "This file contains the annotated stimulus texts at the AOI (word) level,\n",
    "with identifiers such as `id.global.aoi` and surface forms in `AOI_ann`.\n",
    "\n",
    "**Process**\n",
    "AOIs are grouped by their *base key* (sentence-level identifier, derived from `id.global.aoi`).\n",
    "For each base key, the function collects:\n",
    "- `ids`: the list of AOI identifiers (`id.global.aoi`)\n",
    "- `concatenated_aoi`: the corresponding AOI words joined into a sentence-like string\n",
    "\n",
    "This creates a sentence-level view of the data that is used later for:\n",
    "- linguistic annotation with spaCy,\n",
    "- alignment of eye-tracking data with parsed text,\n",
    "- API-based annotation (slim dictionary).\n",
    "\n",
    "**Output**\n",
    "`01_aoi_dictionary.json`\n",
    "A JSON file where each entry is keyed by a base AOI ID and contains:\n",
    "- `ids`: list of `(id.global.aoi)` values for the AOIs in that sentence,\n",
    "- `concatenated_aoi`: the concatenated AOI text string.\n"
   ],
   "id": "913fa138affa62d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T15:39:21.105455Z",
     "start_time": "2025-10-20T15:39:21.093836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_base_key(aoi_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the base key from an AOI ID by stripping trailing digits.\n",
    "    Example:\n",
    "        clinical_114_original_1-2-8 → clinical_114_original_1-2-\n",
    "    \"\"\"\n",
    "    match = re.search(r\"(.*?)(\\d+)$\", aoi_id)\n",
    "    return match.group(1) if match else aoi_id\n",
    "\n",
    "\n",
    "def build_aoi_dict(input_file: str, output_dir=\"raw\", output_name=\"01_aoi_dictionary.json\"):\n",
    "    \"\"\"\n",
    "    Build an AOI dictionary directly from the eye_tracking_data (or lemma-indexed) file.\n",
    "\n",
    "    Each entry stores:\n",
    "        - ids: list of AOI IDs\n",
    "        - concatenated_aoi: AOI words concatenated into a sentence-like string\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_file : str\n",
    "        Path to the input TSV file (e.g., 'raw/00_input_eye_tracking_data.csv' or\n",
    "        'raw/04_indexed_lemmas.csv').\n",
    "    output_dir : str or Path, optional\n",
    "        Directory where the output JSON file will be saved. Defaults to 'raw/'.\n",
    "    output_name : str, optional\n",
    "        Filename for the AOI dictionary JSON. Defaults to '05_aoi_dictionary.json'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The AOI dictionary with base keys as entries.\n",
    "    \"\"\"\n",
    "    input_path = Path(input_file)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = output_path / output_name\n",
    "\n",
    "    df = pd.read_csv(input_path, sep=\"\\t\", quoting=0)\n",
    "    result_dict = {}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        aoi_id = row[\"id.global.aoi\"]\n",
    "        aoi_content = str(row[\"AOI_ann\"])\n",
    "        base_key = get_base_key(aoi_id)\n",
    "\n",
    "        if base_key not in result_dict:\n",
    "            result_dict[base_key] = {\"ids\": [], \"concatenated_aoi\": \"\"}\n",
    "\n",
    "        result_dict[base_key][\"ids\"].append(aoi_id)\n",
    "\n",
    "        if result_dict[base_key][\"concatenated_aoi\"]:\n",
    "            result_dict[base_key][\"concatenated_aoi\"] += \" \" + aoi_content\n",
    "        else:\n",
    "            result_dict[base_key][\"concatenated_aoi\"] = aoi_content\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"AOI dictionary saved to: {output_file}\")\n",
    "    print(f\"Number of entries: {len(result_dict)}\")\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "# Example usage\n",
    "aoi_dict = build_aoi_dict(\"raw/00_input_eye_tracking_data_dummy.csv\")"
   ],
   "id": "804ce56f42b884d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AOI dictionary saved to: raw/01_aoi_dictionary.json\n",
      "Number of entries: 5\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2: Add unique word identifiers and frequency counts\n",
    "\n",
    "**Input**\n",
    "`00_input_eye_tracking_data.csv`\n",
    "Text-level AOI annotation file with columns such as:\n",
    "`['Media', 'textid', 'text.type', 'text_version', 'screenid',\n",
    " 'Sentence_index', 'Word_index', 'word_id_screen', 'word_id_text',\n",
    " 'AOI_ann', 'AOI_nopunct', 'AOI_length', 'is.in.bracket',\n",
    " 'id.phrase.in.brackets', 'tag', 'tag.type', 'tag.id',\n",
    " 'annotated_text', 'Sentence_id_current', 'Sentence_id_match',\n",
    " 'id.piece', 'filename.ann', 'id.global.aoi']`\n",
    "\n",
    "**Output**\n",
    "`02_indexed_words.csv`\n",
    "Same data, enriched with:\n",
    "- `unique_word_text_id`: unique word IDs per text (case-insensitive, punctuation removed)\n",
    "- `unique_word_screen_id`: unique word IDs per screen (within each text)\n",
    "- `word_count_text`: frequency of each word in the text\n",
    "- `word_count_screen`: frequency of each word on the screen\n"
   ],
   "id": "edd41aa719bcb86"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T15:39:22.150062Z",
     "start_time": "2025-10-20T15:39:22.143707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def assign_word_ids_by_group(group: pd.DataFrame, id_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assign unique IDs to words within a given group (e.g., per text or per screen).\n",
    "\n",
    "    Words are matched case-insensitively based on 'AOI_nopunct'.\n",
    "    Each unique lowercase word receives an incrementing integer ID.\n",
    "    \"\"\"\n",
    "    word_to_id = {}\n",
    "    ids = []\n",
    "    id_counter = 1\n",
    "\n",
    "    # Temporary lowercase representation for case-insensitive ID assignment\n",
    "    group[\"temp_word\"] = group[\"AOI_nopunct\"].str.lower()\n",
    "\n",
    "    for word in group[\"temp_word\"]:\n",
    "        if word not in word_to_id:\n",
    "            word_to_id[word] = id_counter\n",
    "            id_counter += 1\n",
    "        ids.append(word_to_id[word])\n",
    "\n",
    "    group[id_name] = ids\n",
    "    group.drop(columns=[\"temp_word\"], inplace=True)\n",
    "    group[id_name] = group[id_name].astype(int)\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "def preprocess_materials(input_file: str, output_dir=\"raw\", output_name=\"01_indexed_words.csv\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main preprocessing routine for annotated eye-tracking materials.\n",
    "\n",
    "    Steps\n",
    "    -----\n",
    "    1. Assign unique word IDs per text and per screen.\n",
    "    2. Count word occurrences in each text and on each screen.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_file : str\n",
    "        Path to the TSV file containing the AOI-level annotated materials\n",
    "        (e.g., 'raw/00_input_eye_tracking_data.csv').\n",
    "    output_dir : str or Path, optional\n",
    "        Directory where the enriched dataset will be saved.\n",
    "        Defaults to 'raw/'.\n",
    "    output_name : str, optional\n",
    "        Filename for the enriched output.\n",
    "        Defaults to '01_indexed_words.csv'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The enriched dataframe with word IDs and frequency annotations.\n",
    "    \"\"\"\n",
    "    input_path = Path(input_file)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = output_path / output_name\n",
    "\n",
    "    df = pd.read_csv(input_path, sep=\"\\t\", quoting=0)\n",
    "\n",
    "    # Assign unique word IDs per text\n",
    "    for text_file in df[\"filename.ann\"].unique():\n",
    "        text_group = df[df[\"filename.ann\"] == text_file].copy()\n",
    "        group = assign_word_ids_by_group(text_group, \"unique_word_text_id\")\n",
    "        df.loc[df[\"filename.ann\"] == text_file, \"unique_word_text_id\"] = group[\"unique_word_text_id\"]\n",
    "\n",
    "    # Assign unique word IDs per screen (within each text)\n",
    "    for text_file in df[\"filename.ann\"].unique():\n",
    "        screens = df[df[\"filename.ann\"] == text_file][\"screenid\"].unique()\n",
    "        for screen in screens:\n",
    "            screen_group = df[(df[\"filename.ann\"] == text_file) & (df[\"screenid\"] == screen)].copy()\n",
    "            group = assign_word_ids_by_group(screen_group, \"unique_word_screen_id\")\n",
    "            df.loc[(df[\"filename.ann\"] == text_file) & (df[\"screenid\"] == screen),\n",
    "                   \"unique_word_screen_id\"] = group[\"unique_word_screen_id\"]\n",
    "\n",
    "    # Compute word frequency within text and screen scopes\n",
    "    df[\"word_count_text\"] = df.groupby([\"filename.ann\", \"unique_word_text_id\"])[\"unique_word_text_id\"].transform(\"count\")\n",
    "    df[\"word_count_screen\"] = df.groupby([\"filename.ann\", \"screenid\", \"unique_word_screen_id\"])[\"unique_word_screen_id\"].transform(\"count\")\n",
    "\n",
    "    # Save enriched dataset\n",
    "    df.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "    print(f\"Preprocessed data saved to: {output_file}\")\n",
    "    print(f\"Data shape: {df.shape}\")\n",
    "\n",
    "    return df"
   ],
   "id": "e8633d54f27206d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T15:39:22.437583Z",
     "start_time": "2025-10-20T15:39:22.404014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_indexed = preprocess_materials(\n",
    "    input_file=\"raw/00_input_eye_tracking_data_dummy.csv\",\n",
    "    output_dir=\"raw\",\n",
    "    output_name=\"02_indexed_words.csv\"\n",
    ")\n",
    "df_indexed"
   ],
   "id": "4d10826b5858493e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to: raw/02_indexed_words.csv\n",
      "Data shape: (10, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                      Media  textid text.type text_version  screenid  \\\n",
       "0   clinical_001_original_1       1  clinical     original         1   \n",
       "1   clinical_001_original_1       1  clinical     original         1   \n",
       "2   clinical_001_original_1       1  clinical     original         1   \n",
       "3   clinical_001_original_1       1  clinical     original         1   \n",
       "4   clinical_001_original_2       1  clinical     original         2   \n",
       "5  medical_002_simplified_1       2   medical   simplified         1   \n",
       "6  medical_002_simplified_1       2   medical   simplified         1   \n",
       "7  medical_002_simplified_1       2   medical   simplified         1   \n",
       "8  medical_002_simplified_1       2   medical   simplified         1   \n",
       "9  medical_002_simplified_1       2   medical   simplified         1   \n",
       "\n",
       "   Sentence_index  Word_index  word_id_screen  word_id_text       AOI_ann  \\\n",
       "0               1           1               1             1           Cas   \n",
       "1               1           2               2             2     clinique.   \n",
       "2               2           1               3             3            Un   \n",
       "3               2           2               4             4       patient   \n",
       "4               2           3               5             5           âgé   \n",
       "5               1           1               1             1            Le   \n",
       "6               1           2               2             2       patient   \n",
       "7               1           3               3             3       souffre   \n",
       "8               2           1               4             4  d’orthopédie   \n",
       "9               2           2               5             5     fracture.   \n",
       "\n",
       "   ... annotated_text  Sentence_id_current  Sentence_id_match  id.piece  \\\n",
       "0  ...            NaN                    1                  1         0   \n",
       "1  ...            NaN                    1                  1         1   \n",
       "2  ...            NaN                    2                  2         2   \n",
       "3  ...            NaN                    2                  2         3   \n",
       "4  ...            NaN                    2                  2         4   \n",
       "5  ...            NaN                    1                  1         5   \n",
       "6  ...            NaN                    1                  1         6   \n",
       "7  ...            NaN                    1                  1         7   \n",
       "8  ...     orthopédie                    2                  2         8   \n",
       "9  ...       fracture                    2                  2         9   \n",
       "\n",
       "                               filename.ann                 id.global.aoi  \\\n",
       "0    001_tagged_finished_words_original.csv   clinical_001_original_1-1-1   \n",
       "1    001_tagged_finished_words_original.csv   clinical_001_original_1-1-2   \n",
       "2    001_tagged_finished_words_original.csv   clinical_001_original_1-2-1   \n",
       "3    001_tagged_finished_words_original.csv   clinical_001_original_1-2-2   \n",
       "4    001_tagged_finished_words_original.csv   clinical_001_original_2-2-3   \n",
       "5  002_tagged_finished_words_simplified.csv  medical_002_simplified_1-1-1   \n",
       "6  002_tagged_finished_words_simplified.csv  medical_002_simplified_1-1-2   \n",
       "7  002_tagged_finished_words_simplified.csv  medical_002_simplified_1-1-3   \n",
       "8  002_tagged_finished_words_simplified.csv  medical_002_simplified_1-2-1   \n",
       "9  002_tagged_finished_words_simplified.csv  medical_002_simplified_1-2-2   \n",
       "\n",
       "   unique_word_text_id  unique_word_screen_id word_count_text  \\\n",
       "0                  1.0                    1.0               1   \n",
       "1                  2.0                    2.0               1   \n",
       "2                  3.0                    3.0               1   \n",
       "3                  4.0                    4.0               1   \n",
       "4                  5.0                    1.0               1   \n",
       "5                  1.0                    1.0               1   \n",
       "6                  2.0                    2.0               1   \n",
       "7                  3.0                    3.0               1   \n",
       "8                  4.0                    4.0               1   \n",
       "9                  5.0                    5.0               1   \n",
       "\n",
       "   word_count_screen  \n",
       "0                  1  \n",
       "1                  1  \n",
       "2                  1  \n",
       "3                  1  \n",
       "4                  1  \n",
       "5                  1  \n",
       "6                  1  \n",
       "7                  1  \n",
       "8                  1  \n",
       "9                  1  \n",
       "\n",
       "[10 rows x 28 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Media</th>\n",
       "      <th>textid</th>\n",
       "      <th>text.type</th>\n",
       "      <th>text_version</th>\n",
       "      <th>screenid</th>\n",
       "      <th>Sentence_index</th>\n",
       "      <th>Word_index</th>\n",
       "      <th>word_id_screen</th>\n",
       "      <th>word_id_text</th>\n",
       "      <th>AOI_ann</th>\n",
       "      <th>...</th>\n",
       "      <th>annotated_text</th>\n",
       "      <th>Sentence_id_current</th>\n",
       "      <th>Sentence_id_match</th>\n",
       "      <th>id.piece</th>\n",
       "      <th>filename.ann</th>\n",
       "      <th>id.global.aoi</th>\n",
       "      <th>unique_word_text_id</th>\n",
       "      <th>unique_word_screen_id</th>\n",
       "      <th>word_count_text</th>\n",
       "      <th>word_count_screen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clinical_001_original_1</td>\n",
       "      <td>1</td>\n",
       "      <td>clinical</td>\n",
       "      <td>original</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cas</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>001_tagged_finished_words_original.csv</td>\n",
       "      <td>clinical_001_original_1-1-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clinical_001_original_1</td>\n",
       "      <td>1</td>\n",
       "      <td>clinical</td>\n",
       "      <td>original</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>clinique.</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>001_tagged_finished_words_original.csv</td>\n",
       "      <td>clinical_001_original_1-1-2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clinical_001_original_1</td>\n",
       "      <td>1</td>\n",
       "      <td>clinical</td>\n",
       "      <td>original</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Un</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>001_tagged_finished_words_original.csv</td>\n",
       "      <td>clinical_001_original_1-2-1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clinical_001_original_1</td>\n",
       "      <td>1</td>\n",
       "      <td>clinical</td>\n",
       "      <td>original</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>patient</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>001_tagged_finished_words_original.csv</td>\n",
       "      <td>clinical_001_original_1-2-2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clinical_001_original_2</td>\n",
       "      <td>1</td>\n",
       "      <td>clinical</td>\n",
       "      <td>original</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>âgé</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>001_tagged_finished_words_original.csv</td>\n",
       "      <td>clinical_001_original_2-2-3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>medical_002_simplified_1</td>\n",
       "      <td>2</td>\n",
       "      <td>medical</td>\n",
       "      <td>simplified</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Le</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>002_tagged_finished_words_simplified.csv</td>\n",
       "      <td>medical_002_simplified_1-1-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>medical_002_simplified_1</td>\n",
       "      <td>2</td>\n",
       "      <td>medical</td>\n",
       "      <td>simplified</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>patient</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>002_tagged_finished_words_simplified.csv</td>\n",
       "      <td>medical_002_simplified_1-1-2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>medical_002_simplified_1</td>\n",
       "      <td>2</td>\n",
       "      <td>medical</td>\n",
       "      <td>simplified</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>souffre</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>002_tagged_finished_words_simplified.csv</td>\n",
       "      <td>medical_002_simplified_1-1-3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>medical_002_simplified_1</td>\n",
       "      <td>2</td>\n",
       "      <td>medical</td>\n",
       "      <td>simplified</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>d’orthopédie</td>\n",
       "      <td>...</td>\n",
       "      <td>orthopédie</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>002_tagged_finished_words_simplified.csv</td>\n",
       "      <td>medical_002_simplified_1-2-1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>medical_002_simplified_1</td>\n",
       "      <td>2</td>\n",
       "      <td>medical</td>\n",
       "      <td>simplified</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>fracture.</td>\n",
       "      <td>...</td>\n",
       "      <td>fracture</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>002_tagged_finished_words_simplified.csv</td>\n",
       "      <td>medical_002_simplified_1-2-2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 3: Add word frequencies\n",
    "\n",
    "**Input**\n",
    "`02_indexed_words.csv`\n",
    "\n",
    "**Output**\n",
    "`03_word_frequencies.csv`\n",
    "Enriched with:\n",
    "- `total_word_count_text`: total number of words in each text\n",
    "- `word_text_frequency`: relative frequency of each word within the text\n"
   ],
   "id": "57475b5e263aef23"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T15:39:22.941534Z",
     "start_time": "2025-10-20T15:39:22.937732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_word_frequencies(\n",
    "    input_file=\"raw/02_indexed_words.csv\",\n",
    "    output_dir=\"raw\",\n",
    "    output_name=\"03_word_frequencies.csv\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute total word counts and relative frequencies per text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_file : str\n",
    "        Path to the indexed word dataset (from preprocess_materials).\n",
    "    output_dir : str or Path, optional\n",
    "        Directory where the output file will be saved. Defaults to 'raw/'.\n",
    "    output_name : str, optional\n",
    "        Output filename. Defaults to '03_word_frequencies.csv'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with additional columns:\n",
    "            - total_word_count_text\n",
    "            - word_text_frequency\n",
    "    \"\"\"\n",
    "    input_path = Path(input_file)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = output_path / output_name\n",
    "\n",
    "    df = pd.read_csv(input_path, sep=\"\\t\", quoting=0)\n",
    "\n",
    "    # Compute total word count per text\n",
    "    word_counts = df.groupby(\"filename.ann\")[\"word_id_text\"].max().reset_index()\n",
    "    word_counts.columns = [\"filename.ann\", \"total_word_count_text\"]\n",
    "\n",
    "    # Merge back into dataframe\n",
    "    df = df.merge(word_counts, on=\"filename.ann\")\n",
    "\n",
    "    # Compute relative frequency\n",
    "    df[\"word_text_frequency\"] = df[\"word_count_text\"] / df[\"total_word_count_text\"]\n",
    "\n",
    "    # Save enriched dataset\n",
    "    df.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "    print(f\"Word frequency data saved to: {output_file}\")\n",
    "    print(f\"Data shape: {df.shape}\")\n",
    "\n",
    "    return df"
   ],
   "id": "3cf77d10c3aff382",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T15:39:23.144382Z",
     "start_time": "2025-10-20T15:39:23.122535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_freq = compute_word_frequencies(\n",
    "    input_file=\"raw/02_indexed_words.csv\",\n",
    "    output_dir=\"raw\",\n",
    "    output_name=\"03_word_frequencies.csv\"\n",
    ")\n",
    "df_freq.head()"
   ],
   "id": "30f5d153973f819b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word frequency data saved to: raw/03_word_frequencies.csv\n",
      "Data shape: (10, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                     Media  textid text.type text_version  screenid  \\\n",
       "0  clinical_001_original_1       1  clinical     original         1   \n",
       "1  clinical_001_original_1       1  clinical     original         1   \n",
       "2  clinical_001_original_1       1  clinical     original         1   \n",
       "3  clinical_001_original_1       1  clinical     original         1   \n",
       "4  clinical_001_original_2       1  clinical     original         2   \n",
       "\n",
       "   Sentence_index  Word_index  word_id_screen  word_id_text    AOI_ann  ...  \\\n",
       "0               1           1               1             1        Cas  ...   \n",
       "1               1           2               2             2  clinique.  ...   \n",
       "2               2           1               3             3         Un  ...   \n",
       "3               2           2               4             4    patient  ...   \n",
       "4               2           3               5             5        âgé  ...   \n",
       "\n",
       "  Sentence_id_match  id.piece                            filename.ann  \\\n",
       "0                 1         0  001_tagged_finished_words_original.csv   \n",
       "1                 1         1  001_tagged_finished_words_original.csv   \n",
       "2                 2         2  001_tagged_finished_words_original.csv   \n",
       "3                 2         3  001_tagged_finished_words_original.csv   \n",
       "4                 2         4  001_tagged_finished_words_original.csv   \n",
       "\n",
       "                 id.global.aoi  unique_word_text_id  unique_word_screen_id  \\\n",
       "0  clinical_001_original_1-1-1                  1.0                    1.0   \n",
       "1  clinical_001_original_1-1-2                  2.0                    2.0   \n",
       "2  clinical_001_original_1-2-1                  3.0                    3.0   \n",
       "3  clinical_001_original_1-2-2                  4.0                    4.0   \n",
       "4  clinical_001_original_2-2-3                  5.0                    1.0   \n",
       "\n",
       "   word_count_text  word_count_screen total_word_count_text  \\\n",
       "0                1                  1                     5   \n",
       "1                1                  1                     5   \n",
       "2                1                  1                     5   \n",
       "3                1                  1                     5   \n",
       "4                1                  1                     5   \n",
       "\n",
       "   word_text_frequency  \n",
       "0                  0.2  \n",
       "1                  0.2  \n",
       "2                  0.2  \n",
       "3                  0.2  \n",
       "4                  0.2  \n",
       "\n",
       "[5 rows x 30 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Media</th>\n",
       "      <th>textid</th>\n",
       "      <th>text.type</th>\n",
       "      <th>text_version</th>\n",
       "      <th>screenid</th>\n",
       "      <th>Sentence_index</th>\n",
       "      <th>Word_index</th>\n",
       "      <th>word_id_screen</th>\n",
       "      <th>word_id_text</th>\n",
       "      <th>AOI_ann</th>\n",
       "      <th>...</th>\n",
       "      <th>Sentence_id_match</th>\n",
       "      <th>id.piece</th>\n",
       "      <th>filename.ann</th>\n",
       "      <th>id.global.aoi</th>\n",
       "      <th>unique_word_text_id</th>\n",
       "      <th>unique_word_screen_id</th>\n",
       "      <th>word_count_text</th>\n",
       "      <th>word_count_screen</th>\n",
       "      <th>total_word_count_text</th>\n",
       "      <th>word_text_frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clinical_001_original_1</td>\n",
       "      <td>1</td>\n",
       "      <td>clinical</td>\n",
       "      <td>original</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cas</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>001_tagged_finished_words_original.csv</td>\n",
       "      <td>clinical_001_original_1-1-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clinical_001_original_1</td>\n",
       "      <td>1</td>\n",
       "      <td>clinical</td>\n",
       "      <td>original</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>clinique.</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>001_tagged_finished_words_original.csv</td>\n",
       "      <td>clinical_001_original_1-1-2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clinical_001_original_1</td>\n",
       "      <td>1</td>\n",
       "      <td>clinical</td>\n",
       "      <td>original</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Un</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>001_tagged_finished_words_original.csv</td>\n",
       "      <td>clinical_001_original_1-2-1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clinical_001_original_1</td>\n",
       "      <td>1</td>\n",
       "      <td>clinical</td>\n",
       "      <td>original</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>patient</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>001_tagged_finished_words_original.csv</td>\n",
       "      <td>clinical_001_original_1-2-2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clinical_001_original_2</td>\n",
       "      <td>1</td>\n",
       "      <td>clinical</td>\n",
       "      <td>original</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>âgé</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>001_tagged_finished_words_original.csv</td>\n",
       "      <td>clinical_001_original_2-2-3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 4: Add POS, lemmas, and dependency features\n",
    "\n",
    "**Input**\n",
    "- `03_word_frequencies.csv`\n",
    "- `aoi_dict.json` (AOIs grouped with concatenated text)\n",
    "\n",
    "**Output**\n",
    "`04_spacy_annotations.csv`\n",
    "Enriched with:\n",
    "- `pos` (part of speech from spaCy)\n",
    "- `lemma` (canonical form)\n",
    "- `left_dependents` / `right_dependents` (syntactic dependents from dependency parse)\n"
   ],
   "id": "1caf8a1cd5df344f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T15:39:26.954639Z",
     "start_time": "2025-10-20T15:39:23.702575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "# Load French language model\n",
    "try:\n",
    "    nlp = spacy.load(\"fr_core_news_lg\")\n",
    "except OSError:\n",
    "    raise RuntimeError(\n",
    "        \"The spaCy model 'fr_core_news_lg' is not installed. \"\n",
    "        \"Install it via: python -m spacy download fr_core_news_lg\"\n",
    "    )\n",
    "\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove punctuation (except apostrophes) and normalize spacing/apostrophes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Input string to normalize.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Cleaned text with standardized apostrophes and single spacing.\n",
    "    \"\"\"\n",
    "    text = text.replace(\"’\", \"'\")\n",
    "    text = re.sub(r\"[^\\w\\s']\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def is_contraction_prefix(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if token looks like a French contraction (prefix + apostrophe).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Token text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the token ends with an apostrophe, indicating a contraction.\n",
    "    \"\"\"\n",
    "    return re.match(r\"^.+[’']\", text) is not None\n",
    "\n",
    "\n",
    "def process_french_words(df: pd.DataFrame, sentence_dict: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Align AOIs with spaCy tokens and add POS, lemma, and dependency features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing AOI-level text with columns:\n",
    "        'id.global.aoi' and 'AOI_ann'.\n",
    "    sentence_dict : dict\n",
    "        Dictionary of AOI sentence groups, e.g. from build_aoi_dict().\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The input DataFrame with added columns:\n",
    "        'pos', 'lemma', 'left_dependents', 'right_dependents'.\n",
    "    \"\"\"\n",
    "    # Initialize columns to store annotations\n",
    "    for col in [\"left_dependents\", \"right_dependents\", \"pos\", \"lemma\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    # Iterate through each AOI sentence group\n",
    "    for base_key, data in sentence_dict.items():\n",
    "        doc = nlp(data[\"concatenated_aoi\"])\n",
    "        id_globals = sorted(data[\"ids\"])\n",
    "\n",
    "        # Collect AOIs for this group\n",
    "        aoi_matches = []\n",
    "        for id_global in id_globals:\n",
    "            matching_rows = df[df[\"id.global.aoi\"] == id_global]\n",
    "            for idx, row in matching_rows.iterrows():\n",
    "                aoi_matches.append((row[\"AOI_ann\"], idx))\n",
    "\n",
    "        matched_tokens = [False] * len(doc)\n",
    "\n",
    "        # Align each AOI with a spaCy token\n",
    "        for aoi_word, idx in aoi_matches:\n",
    "            aoi_norm = normalize_text(aoi_word)\n",
    "            token_info = None\n",
    "\n",
    "            # Exact or normalized match\n",
    "            for i, token in enumerate(doc):\n",
    "                if not matched_tokens[i] and (token.text == aoi_word or normalize_text(token.text) == aoi_norm):\n",
    "                    token_info = token\n",
    "                    matched_tokens[i] = True\n",
    "                    break\n",
    "\n",
    "            # Fallback: fuzzy match or contraction handling\n",
    "            if not token_info:\n",
    "                for i, token in enumerate(doc):\n",
    "                    token_norm = normalize_text(token.text)\n",
    "                    if (aoi_norm == token_norm or aoi_norm in token_norm or token_norm in aoi_norm) and not matched_tokens[i]:\n",
    "                        token_info = token\n",
    "                        matched_tokens[i] = True\n",
    "                        break\n",
    "                    if i < len(doc) - 1 and is_contraction_prefix(token.text):\n",
    "                        contraction = normalize_text(token.text + doc[i + 1].text)\n",
    "                        if aoi_norm == contraction:\n",
    "                            token_info = doc[i + 1]\n",
    "                            matched_tokens[i + 1] = True\n",
    "                            break\n",
    "\n",
    "            # Assign extracted linguistic features\n",
    "            if token_info:\n",
    "                df.at[idx, \"left_dependents\"] = len([c for c in token_info.children if c.i < token_info.i])\n",
    "                df.at[idx, \"right_dependents\"] = len([c for c in token_info.children if c.i > token_info.i])\n",
    "                df.at[idx, \"pos\"] = token_info.pos_\n",
    "                df.at[idx, \"lemma\"] = token_info.lemma_\n",
    "\n",
    "    return df"
   ],
   "id": "b70d02ca32c31d09",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T15:39:27.034947Z",
     "start_time": "2025-10-20T15:39:26.984527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the required input files\n",
    "df_freq = pd.read_csv(\"raw/03_word_frequencies.csv\", sep=\"\\t\", quoting=0)\n",
    "with open(\"raw/01_aoi_dictionary.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    aoi_dict = json.load(f)\n",
    "\n",
    "# Process with spaCy\n",
    "df_spacy = process_french_words(df_freq, aoi_dict)\n",
    "\n",
    "# Save enriched dataset\n",
    "output_path = Path(\"raw/04_spacy_annotations.csv\")\n",
    "df_spacy.to_csv(output_path, sep=\"\\t\", index=False)\n",
    "print(f\"Saved {output_path.name} (shape: {df_spacy.shape})\")\n",
    "\n",
    "df_spacy.head()\n"
   ],
   "id": "2067415f4329a295",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 04_spacy_annotations.csv (shape: (10, 34))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                     Media  textid text.type text_version  screenid  \\\n",
       "0  clinical_001_original_1       1  clinical     original         1   \n",
       "1  clinical_001_original_1       1  clinical     original         1   \n",
       "2  clinical_001_original_1       1  clinical     original         1   \n",
       "3  clinical_001_original_1       1  clinical     original         1   \n",
       "4  clinical_001_original_2       1  clinical     original         2   \n",
       "\n",
       "   Sentence_index  Word_index  word_id_screen  word_id_text    AOI_ann  ...  \\\n",
       "0               1           1               1             1        Cas  ...   \n",
       "1               1           2               2             2  clinique.  ...   \n",
       "2               2           1               3             3         Un  ...   \n",
       "3               2           2               4             4    patient  ...   \n",
       "4               2           3               5             5        âgé  ...   \n",
       "\n",
       "  unique_word_text_id  unique_word_screen_id  word_count_text  \\\n",
       "0                 1.0                    1.0                1   \n",
       "1                 2.0                    2.0                1   \n",
       "2                 3.0                    3.0                1   \n",
       "3                 4.0                    4.0                1   \n",
       "4                 5.0                    1.0                1   \n",
       "\n",
       "   word_count_screen  total_word_count_text  word_text_frequency  \\\n",
       "0                  1                      5                  0.2   \n",
       "1                  1                      5                  0.2   \n",
       "2                  1                      5                  0.2   \n",
       "3                  1                      5                  0.2   \n",
       "4                  1                      5                  0.2   \n",
       "\n",
       "   left_dependents  right_dependents   pos     lemma  \n",
       "0                0                 2  NOUN       cas  \n",
       "1                0                 0   ADJ  clinique  \n",
       "2                0                 0   DET        un  \n",
       "3                1                 0  NOUN   patient  \n",
       "4                0                 0   ADJ       âgé  \n",
       "\n",
       "[5 rows x 34 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Media</th>\n",
       "      <th>textid</th>\n",
       "      <th>text.type</th>\n",
       "      <th>text_version</th>\n",
       "      <th>screenid</th>\n",
       "      <th>Sentence_index</th>\n",
       "      <th>Word_index</th>\n",
       "      <th>word_id_screen</th>\n",
       "      <th>word_id_text</th>\n",
       "      <th>AOI_ann</th>\n",
       "      <th>...</th>\n",
       "      <th>unique_word_text_id</th>\n",
       "      <th>unique_word_screen_id</th>\n",
       "      <th>word_count_text</th>\n",
       "      <th>word_count_screen</th>\n",
       "      <th>total_word_count_text</th>\n",
       "      <th>word_text_frequency</th>\n",
       "      <th>left_dependents</th>\n",
       "      <th>right_dependents</th>\n",
       "      <th>pos</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clinical_001_original_1</td>\n",
       "      <td>1</td>\n",
       "      <td>clinical</td>\n",
       "      <td>original</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cas</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>cas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clinical_001_original_1</td>\n",
       "      <td>1</td>\n",
       "      <td>clinical</td>\n",
       "      <td>original</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>clinique.</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>clinique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clinical_001_original_1</td>\n",
       "      <td>1</td>\n",
       "      <td>clinical</td>\n",
       "      <td>original</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Un</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>DET</td>\n",
       "      <td>un</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clinical_001_original_1</td>\n",
       "      <td>1</td>\n",
       "      <td>clinical</td>\n",
       "      <td>original</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>patient</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>patient</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clinical_001_original_2</td>\n",
       "      <td>1</td>\n",
       "      <td>clinical</td>\n",
       "      <td>original</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>âgé</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>âgé</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 5: Assign lemma IDs and counts\n",
    "\n",
    "**Input**\n",
    "- `04_spacy_annotations.csv`\n",
    "\n",
    "**Process**\n",
    "- Assign unique lemma IDs per text (`unique_lemma_text_id`)\n",
    "- Assign unique lemma IDs per screen (`unique_lemma_screen_id`)\n",
    "- Count lemma occurrences in texts and on screens\n",
    "\n",
    "**Output**\n",
    "`05_indexed_lemmas.csv`\n",
    "Enriched with:\n",
    "- `unique_lemma_text_id`\n",
    "- `unique_lemma_screen_id`\n",
    "- `lemma_count_text`\n",
    "- `lemma_count_screen`\n"
   ],
   "id": "c11c7f77800add20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T15:39:27.076871Z",
     "start_time": "2025-10-20T15:39:27.070955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def assign_ids_by_group(group: pd.DataFrame, value_col: str, id_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assign unique IDs to values within a given group (e.g., lemmas per text).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    group : pd.DataFrame\n",
    "        Subset of the main DataFrame for a specific text or screen.\n",
    "    value_col : str\n",
    "        Column name containing the values to assign IDs to (e.g., 'lemma').\n",
    "    id_name : str\n",
    "        Name of the new column to store the assigned IDs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The same group with a new integer ID column.\n",
    "    \"\"\"\n",
    "    value_to_id = {}\n",
    "    ids = []\n",
    "    id_counter = 1\n",
    "\n",
    "    for val in group[value_col]:\n",
    "        if val not in value_to_id:\n",
    "            value_to_id[val] = id_counter\n",
    "            id_counter += 1\n",
    "        ids.append(value_to_id[val])\n",
    "\n",
    "    group[id_name] = ids\n",
    "    return group\n",
    "\n",
    "\n",
    "def preprocess_lemmas(input_file: str, output_dir=\"raw\", output_name=\"05_indexed_lemmas.csv\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assign lemma IDs per text and screen, and count lemma frequencies.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_file : str\n",
    "        Path to the CSV file containing annotated AOI data with lemma information.\n",
    "    output_dir : str or Path, optional\n",
    "        Directory where the output will be saved. Defaults to 'raw/'.\n",
    "    output_name : str, optional\n",
    "        Output filename. Defaults to '05_indexed_lemmas.csv'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The enriched DataFrame with added lemma ID and frequency columns.\n",
    "    \"\"\"\n",
    "    input_path = Path(input_file)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = output_path / output_name\n",
    "\n",
    "    df = pd.read_csv(input_path, sep=\"\\t\", quoting=0)\n",
    "\n",
    "    # Assign unique lemma IDs per text\n",
    "    for text in df[\"filename.ann\"].unique():\n",
    "        text_group = df[df[\"filename.ann\"] == text].copy()\n",
    "        group = assign_ids_by_group(text_group, \"lemma\", \"unique_lemma_text_id\")\n",
    "        df.loc[df[\"filename.ann\"] == text, \"unique_lemma_text_id\"] = group[\"unique_lemma_text_id\"]\n",
    "\n",
    "    # Assign unique lemma IDs per screen\n",
    "    for text in df[\"filename.ann\"].unique():\n",
    "        for screen in df[df[\"filename.ann\"] == text][\"screenid\"].unique():\n",
    "            screen_group = df[(df[\"filename.ann\"] == text) & (df[\"screenid\"] == screen)].copy()\n",
    "            group = assign_ids_by_group(screen_group, \"lemma\", \"unique_lemma_screen_id\")\n",
    "            df.loc[(df[\"filename.ann\"] == text) & (df[\"screenid\"] == screen),\n",
    "                   \"unique_lemma_screen_id\"] = group[\"unique_lemma_screen_id\"]\n",
    "\n",
    "    # Count lemma frequencies\n",
    "    df[\"lemma_count_text\"] = df.groupby([\"filename.ann\", \"unique_lemma_text_id\"])[\"unique_lemma_text_id\"].transform(\"count\")\n",
    "    df[\"lemma_count_screen\"] = df.groupby([\"filename.ann\", \"screenid\", \"unique_lemma_screen_id\"])[\"unique_lemma_screen_id\"].transform(\"count\")\n",
    "\n",
    "    # Save enriched data\n",
    "    df.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "    print(f\"Saved {output_file} with shape {df.shape}\")\n",
    "\n",
    "    return df"
   ],
   "id": "96c97e81f90c31a",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T15:39:27.203502Z",
     "start_time": "2025-10-20T15:39:27.177988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_lemmas = preprocess_lemmas(\n",
    "    input_file=\"raw/04_spacy_annotations.csv\",\n",
    "    output_dir=\"raw\",\n",
    "    output_name=\"05_indexed_lemmas.csv\"\n",
    ")\n",
    "df_lemmas.head()"
   ],
   "id": "78eaa21e14f2ee19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved raw/05_indexed_lemmas.csv with shape (10, 38)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                     Media  textid text.type text_version  screenid  \\\n",
       "0  clinical_001_original_1       1  clinical     original         1   \n",
       "1  clinical_001_original_1       1  clinical     original         1   \n",
       "2  clinical_001_original_1       1  clinical     original         1   \n",
       "3  clinical_001_original_1       1  clinical     original         1   \n",
       "4  clinical_001_original_2       1  clinical     original         2   \n",
       "\n",
       "   Sentence_index  Word_index  word_id_screen  word_id_text    AOI_ann  ...  \\\n",
       "0               1           1               1             1        Cas  ...   \n",
       "1               1           2               2             2  clinique.  ...   \n",
       "2               2           1               3             3         Un  ...   \n",
       "3               2           2               4             4    patient  ...   \n",
       "4               2           3               5             5        âgé  ...   \n",
       "\n",
       "  total_word_count_text  word_text_frequency  left_dependents  \\\n",
       "0                     5                  0.2                0   \n",
       "1                     5                  0.2                0   \n",
       "2                     5                  0.2                0   \n",
       "3                     5                  0.2                1   \n",
       "4                     5                  0.2                0   \n",
       "\n",
       "   right_dependents   pos     lemma  unique_lemma_text_id  \\\n",
       "0                 2  NOUN       cas                   1.0   \n",
       "1                 0   ADJ  clinique                   2.0   \n",
       "2                 0   DET        un                   3.0   \n",
       "3                 0  NOUN   patient                   4.0   \n",
       "4                 0   ADJ       âgé                   5.0   \n",
       "\n",
       "   unique_lemma_screen_id lemma_count_text  lemma_count_screen  \n",
       "0                     1.0                1                   1  \n",
       "1                     2.0                1                   1  \n",
       "2                     3.0                1                   1  \n",
       "3                     4.0                1                   1  \n",
       "4                     1.0                1                   1  \n",
       "\n",
       "[5 rows x 38 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Media</th>\n",
       "      <th>textid</th>\n",
       "      <th>text.type</th>\n",
       "      <th>text_version</th>\n",
       "      <th>screenid</th>\n",
       "      <th>Sentence_index</th>\n",
       "      <th>Word_index</th>\n",
       "      <th>word_id_screen</th>\n",
       "      <th>word_id_text</th>\n",
       "      <th>AOI_ann</th>\n",
       "      <th>...</th>\n",
       "      <th>total_word_count_text</th>\n",
       "      <th>word_text_frequency</th>\n",
       "      <th>left_dependents</th>\n",
       "      <th>right_dependents</th>\n",
       "      <th>pos</th>\n",
       "      <th>lemma</th>\n",
       "      <th>unique_lemma_text_id</th>\n",
       "      <th>unique_lemma_screen_id</th>\n",
       "      <th>lemma_count_text</th>\n",
       "      <th>lemma_count_screen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clinical_001_original_1</td>\n",
       "      <td>1</td>\n",
       "      <td>clinical</td>\n",
       "      <td>original</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cas</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>cas</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clinical_001_original_1</td>\n",
       "      <td>1</td>\n",
       "      <td>clinical</td>\n",
       "      <td>original</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>clinique.</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>clinique</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clinical_001_original_1</td>\n",
       "      <td>1</td>\n",
       "      <td>clinical</td>\n",
       "      <td>original</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Un</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>DET</td>\n",
       "      <td>un</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clinical_001_original_1</td>\n",
       "      <td>1</td>\n",
       "      <td>clinical</td>\n",
       "      <td>original</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>patient</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>patient</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clinical_001_original_2</td>\n",
       "      <td>1</td>\n",
       "      <td>clinical</td>\n",
       "      <td>original</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>âgé</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>âgé</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"\n",
    "  background-color:#2b3a42;\n",
    "  color:#f1f1f1;\n",
    "  border-left: 6px solid #5da3a3;\n",
    "  padding: 14px;\n",
    "  border-radius: 5px;\n",
    "  line-height: 1.6;\n",
    "\">\n",
    "\n",
    "<h2 style=\"color:#ffffff;\">Step 6: Continue in External Notebook – <code>01_api_annotation_pipeline.ipynb</code></h2>\n",
    "\n",
    "The lemma-indexed dataset produced above (<code>et_data_lemmas_indexed.csv</code>)\n",
    "contains all automatically derived linguistic features from the local spaCy pipeline.\n",
    "\n",
    "To further enrich the materials with context-aware linguistic annotations\n",
    "(tokenization, lemmatization, and POS tagging), proceed to the dedicated notebook:\n",
    "\n",
    "<strong><code>01_api_annotation_pipeline.ipynb</code></strong>\n",
    "\n",
    "That notebook takes <code>aoi_dict.json</code> as input,\n",
    "runs the API-based enrichment pipeline, and outputs the collapsed and\n",
    "frequency-annotated materials file:\n",
    "\n",
    "<strong><code>materials_parsed_collapsed.csv</code></strong>\n",
    "\n",
    "Once the API notebook has completed, return here to continue with\n",
    "<strong>Step 7: Merging Enriched Materials with Eye-Tracking Data.</strong>\n",
    "\n",
    "</div>"
   ],
   "id": "e82e9c15abb73d9b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 7: Compute Lemma Frequencies per Text and Screen\n",
    "\n",
    "**Input**\n",
    "- `raw/materials_parsed_collapsed.csv`\n",
    "  AOI-level annotated dataset containing merged tokens, lemmas, and POS tags.\n",
    "\n",
    "**Process**\n",
    "This step enriches the AOI-level dataset with lemma- and word-frequency statistics.\n",
    "Each AOI may contain one or more lemmas (split by `\" | \"`), which are analyzed to compute:\n",
    "- **Counts:** `lemma_count_text`, `lemma_count_screen`\n",
    "- **Frequencies:** `lemma_frequency_text`, `lemma_frequency_screen`\n",
    "- **Word-level frequencies:** `word_frequency_text`, `word_frequency_screen`\n",
    "\n",
    "These measures describe how frequent each lemma or word is within its text (`filename.ann`)\n",
    "and within its corresponding screen (`screenid`).\n",
    "\n",
    "**Output**\n",
    "- `raw/materials_parsed_enriched.csv`\n",
    "  Enriched dataset containing all original AOI-level fields plus additional\n",
    "  frequency statistics for lemmas and words."
   ],
   "id": "2200baa2854e7b3f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T15:39:48.327777Z",
     "start_time": "2025-10-20T15:39:48.295725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_lemma_frequencies(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add lemma and word frequency columns directly to the provided dataframe.\n",
    "\n",
    "    Expected columns:\n",
    "      - filename.ann : text identifier\n",
    "      - screenid     : screen identifier\n",
    "      - lemma_merged : string, possibly with multiple lemmas separated by '|'\n",
    "      - word_count_screen, word_count_text, total_word_count_text : numeric counts\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Rename for easier internal reference\n",
    "    df.rename(\n",
    "        columns={\n",
    "            \"filename.ann\": \"text\",\n",
    "            \"screenid\": \"screen\",\n",
    "            \"lemma_merged\": \"lemma\",\n",
    "            \"word_count_screen\": \"word_count_screen\",\n",
    "            \"total_word_count_text\": \"total_words_per_text\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # --- Expand multi-lemma AOIs -----------------------------------------\n",
    "    df_exploded = df.assign(\n",
    "        lemma_individual=df[\"lemma\"].str.split(r\"\\s*\\|\\s*\")\n",
    "    ).explode(\"lemma_individual\")\n",
    "\n",
    "    # --- Lemma-level counts and frequencies -------------------------------\n",
    "    df_exploded[\"lemma_count_text\"] = df_exploded.groupby(\n",
    "        [\"text\", \"lemma_individual\"]\n",
    "    )[\"lemma_individual\"].transform(\"count\")\n",
    "\n",
    "    df_exploded[\"lemma_count_screen\"] = df_exploded.groupby(\n",
    "        [\"text\", \"screen\", \"lemma_individual\"]\n",
    "    )[\"lemma_individual\"].transform(\"count\")\n",
    "\n",
    "    df_exploded[\"total_lemma_count_text\"] = df_exploded.groupby(\"text\")[\n",
    "        \"lemma_individual\"\n",
    "    ].transform(\"count\")\n",
    "\n",
    "    df_exploded[\"total_lemma_count_screen\"] = df_exploded.groupby(\n",
    "        [\"text\", \"screen\"]\n",
    "    )[\"lemma_individual\"].transform(\"count\")\n",
    "\n",
    "    df_exploded[\"lemma_frequency_text\"] = (\n",
    "        df_exploded[\"lemma_count_text\"] / df_exploded[\"total_lemma_count_text\"]\n",
    "    )\n",
    "    df_exploded[\"lemma_frequency_screen\"] = (\n",
    "        df_exploded[\"lemma_count_screen\"] / df_exploded[\"total_lemma_count_screen\"]\n",
    "    )\n",
    "\n",
    "    # --- Collapse lemma stats back to AOI level ---------------------------\n",
    "    lemma_stats = (\n",
    "        df_exploded.groupby([\"text\", \"screen\", \"lemma\"], as_index=False)\n",
    "        .agg({\n",
    "            \"lemma_individual\": list,\n",
    "            \"lemma_count_screen\": list,\n",
    "            \"lemma_count_text\": list,\n",
    "            \"total_lemma_count_screen\": \"first\",\n",
    "            \"total_lemma_count_text\": \"first\",\n",
    "            \"lemma_frequency_screen\": list,\n",
    "            \"lemma_frequency_text\": list,\n",
    "        })\n",
    "    )\n",
    "\n",
    "    # --- Merge stats back into the original dataframe ---------------------\n",
    "    df_merged = df.merge(lemma_stats, on=[\"text\", \"screen\", \"lemma\"], how=\"left\")\n",
    "\n",
    "    # --- Word-level frequencies ------------------------------------------\n",
    "    df_merged[\"word_frequency_text\"] = (\n",
    "        df_merged[\"word_count_text\"] / df_merged[\"total_words_per_text\"]\n",
    "    )\n",
    "    df_merged[\"total_word_count_screen\"] = df_merged.groupby(\n",
    "        [\"text\", \"screen\"]\n",
    "    )[\"screen\"].transform(\"count\")\n",
    "    df_merged[\"word_frequency_screen\"] = (\n",
    "        df_merged[\"word_count_screen\"] / df_merged[\"total_word_count_screen\"]\n",
    "    )\n",
    "\n",
    "    # Restore original column names\n",
    "    df_merged.rename(\n",
    "        columns={\n",
    "            \"text\": \"filename.ann\",\n",
    "            \"screen\": \"screenid\",\n",
    "            \"lemma\": \"lemma_merged\",\n",
    "            \"word_count_screen\": \"word_count_screen\",\n",
    "            \"total_words_per_text\": \"total_word_count_text\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    return df_merged\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Apply to the collapsed materials file and save enriched dataset\n",
    "# ----------------------------------------------------------------------\n",
    "df_collapsed = pd.read_csv(\"raw/10_materials_parsed_collapsed.csv\", sep=\"\\t\", low_memory=False)\n",
    "df_enriched = add_lemma_frequencies(df_collapsed)\n",
    "\n",
    "# Save the enriched dataset\n",
    "output_path = \"raw/11_materials_parsed_enriched.csv\"\n",
    "Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "df_enriched.to_csv(output_path, sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"Enriched dataset saved to {output_path} (shape: {df_enriched.shape})\")"
   ],
   "id": "a4eb69d4855faf4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched dataset saved to raw/11_materials_parsed_enriched.csv (shape: (10, 49))\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 8: Merge Eye-Tracking Data with Enriched Linguistic Annotations\n",
    "\n",
    "**Overview**\n",
    "\n",
    "This step integrates the linguistic annotation results with the eye-tracking dataset, producing a unified table that aligns fixation-based behavioral measures with lexical and syntactic information.\n",
    "It corresponds to the stage in the original preprocessing workflow where `ETdata_oksana_for_constantin_2025-04-22-old.csv` was merged with the supervisor-corrected materials.\n",
    "Here, the same structure is reproduced using the synthetic datasets.\n",
    "\n",
    "**Input**\n",
    "- `00_input_eye_tracking_data_with_metrics_dummy.csv`\n",
    "  Synthetic eye-tracking dataset containing AOI-level fixation and visit metrics for several participants.\n",
    "- `materials_parsed_collapsed.csv`\n",
    "  Collapsed and enriched materials dataset, representing AOI-level linguistic annotations (tokens, lemmas, POS).\n",
    "\n",
    "**Process**\n",
    "Both datasets share a common identifier, `id.global.aoi`, which uniquely specifies each Area of Interest (AOI).\n",
    "The merging procedure combines linguistic and behavioral features into a single record per AOI:\n",
    "\n",
    "- merges AOI-level gaze measures (e.g., fixation durations, saccades, regressions)\n",
    "  with linguistic information (e.g., lemma, POS, token);\n",
    "- performs a *left join* to ensure that every gaze observation is preserved,\n",
    "  even when no linguistic annotation is available.\n",
    "\n",
    "**Output**\n",
    "- `et_data_merged_with_ann_materials_dummy.csv`\n",
    "  Unified dataset combining eye-tracking and linguistic features.\n",
    "  This file serves as the basis for subsequent feature aggregation, frequency analysis,\n",
    "  and statistical modeling.\n"
   ],
   "id": "3fd28e57d8ddf9cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T15:39:49.454625Z",
     "start_time": "2025-10-20T15:39:49.413464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# File paths\n",
    "# ---------------------------------------------------------------------\n",
    "ET_FILE = \"raw/00_input_eye_tracking_data_with_metrics_dummy.csv\"\n",
    "MATERIALS_FILE = \"raw/11_materials_parsed_enriched.csv\"\n",
    "OUTPUT_FILE = \"raw/et_data_merged_with_ann_materials_dummy.csv\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Utility: Compare column structures\n",
    "# ---------------------------------------------------------------------\n",
    "def compare_columns(df1: pd.DataFrame, df2: pd.DataFrame, name1: str, name2: str):\n",
    "    \"\"\"Compare column sets between two DataFrames and print unique columns per file.\"\"\"\n",
    "    cols1, cols2 = set(df1.columns), set(df2.columns)\n",
    "    print(f\"\\nColumns unique to {name1}: {sorted(list(cols1 - cols2))}\")\n",
    "    print(f\"Columns unique to {name2}: {sorted(list(cols2 - cols1))}\")\n",
    "    print(f\"Shared columns: {len(cols1 & cols2)}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. Load input datasets\n",
    "# ---------------------------------------------------------------------\n",
    "et_df = pd.read_csv(ET_FILE, sep=\"\\t\", dtype=str, low_memory=False)\n",
    "materials_df = pd.read_csv(MATERIALS_FILE, sep=\"\\t\", dtype=str, low_memory=False)\n",
    "\n",
    "print(f\"Loaded eye-tracking data: {et_df.shape}\")\n",
    "print(f\"Loaded annotated materials: {materials_df.shape}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. Inspect column structures (optional)\n",
    "# ---------------------------------------------------------------------\n",
    "compare_columns(et_df, materials_df, \"eye_tracking_data\", \"materials_data\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3. Remove redundant metadata columns from materials before merge\n",
    "# ---------------------------------------------------------------------\n",
    "redundant_cols = [\n",
    "    \"Media\", \"textid\", \"text.type\", \"text_version\", \"screenid\",\n",
    "    \"Sentence_index\", \"Word_index\"\n",
    "]\n",
    "materials_subset = materials_df.drop(columns=[c for c in redundant_cols if c in materials_df.columns], errors=\"ignore\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4. Merge on global AOI identifier with clear suffixes\n",
    "# ---------------------------------------------------------------------\n",
    "if \"id.global.aoi\" not in et_df.columns or \"id.global.aoi\" not in materials_subset.columns:\n",
    "    raise KeyError(\"Both datasets must contain the column 'id.global.aoi' for merging.\")\n",
    "\n",
    "merged_df = pd.merge(\n",
    "    et_df,\n",
    "    materials_subset,\n",
    "    on=\"id.global.aoi\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"_et\", \"_mat\"),\n",
    "    validate=\"m:1\"  # ensures one annotation per AOI\n",
    ")\n",
    "\n",
    "print(f\"\\nMerged dataset created with shape: {merged_df.shape}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5. Resolve duplicate columns (keep material version if not empty)\n",
    "# ---------------------------------------------------------------------\n",
    "for col in merged_df.columns:\n",
    "    if col.endswith(\"_et\") and col[:-3] + \"_mat\" in merged_df.columns:\n",
    "        base = col[:-3]\n",
    "        et_col = col\n",
    "        mat_col = base + \"_mat\"\n",
    "\n",
    "        # Prefer non-empty material value; fallback to ET if missing\n",
    "        merged_df[base] = merged_df[mat_col].where(\n",
    "            merged_df[mat_col].notna() & (merged_df[mat_col] != \"\"),\n",
    "            merged_df[et_col]\n",
    "        )\n",
    "\n",
    "# Drop all suffixed columns (_et and _mat)\n",
    "merged_df = merged_df.drop(columns=[c for c in merged_df.columns if c.endswith((\"_et\", \"_mat\"))])\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6. Save merged output\n",
    "# ---------------------------------------------------------------------\n",
    "Path(OUTPUT_FILE).parent.mkdir(parents=True, exist_ok=True)\n",
    "merged_df.to_csv(OUTPUT_FILE, sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"Merged dataset saved to {OUTPUT_FILE}\")\n",
    "print(\"\\nPreview of merged data:\")\n",
    "display(merged_df.head(10))\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 7. Integrity check: AOI overlap diagnostics\n",
    "# ---------------------------------------------------------------------\n",
    "et_ids = set(et_df[\"id.global.aoi\"].unique())\n",
    "mat_ids = set(materials_df[\"id.global.aoi\"].unique())\n",
    "common_ids = et_ids & mat_ids\n",
    "\n",
    "print(\"\\n--- Merge Key Diagnostics ---\")\n",
    "print(f\"AOI IDs in ET data: {len(et_ids)}\")\n",
    "print(f\"AOI IDs in materials: {len(mat_ids)}\")\n",
    "print(f\"Common AOI IDs: {len(common_ids)}\")\n",
    "\n",
    "if len(common_ids) == 0:\n",
    "    print(\"No matching AOI identifiers found — check ID construction in earlier steps.\")\n",
    "else:\n",
    "    print(\"Example matched AOI IDs:\", list(common_ids)[:5])\n",
    "\n",
    "print(\"\\nExample ET-only AOIs:\", list(et_ids - mat_ids)[:5])\n",
    "print(\"Example materials-only AOIs:\", list(mat_ids - et_ids)[:5])\n"
   ],
   "id": "17c070c79416d3e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded eye-tracking data: (30, 78)\n",
      "Loaded annotated materials: (10, 49)\n",
      "\n",
      "Columns unique to eye_tracking_data: ['AOI_nopunct', 'Average_duration_of_Visit', 'Average_duration_of_fixations', 'Average_duration_of_whole_fixations', 'Average_eye_openness', 'Average_pupil_diameter', 'Average_whole-fixation_eye_openness', 'Average_whole-fixation_pupil_diameter', 'Duration_of_first_Visit', 'Duration_of_first_fixation', 'Duration_of_first_whole_fixation', 'First-pass_duration', 'First-pass_first_fixation_duration', 'First-pass_regression', 'Maximum_duration_of_Visit', 'Maximum_duration_of_fixations', 'Maximum_duration_of_whole_fixations', 'Media_old', 'Minimum_duration_of_Visit', 'Minimum_duration_of_fixations', 'Minimum_duration_of_whole_fixations', 'Number_of_Visits', 'Number_of_fixations', 'Number_of_saccades_in_AOI', 'Number_of_whole_fixations', 'Participant', 'Participant_commnts_new', 'Participant_unique', 'Payment', 'Peak_velocity_of_entry_saccade', 'Peak_velocity_of_exit_saccade', 'Re-reading_duration', 'Recording', 'Regression-path_duration', 'Selective_regression-path_duration', 'Time_to_entry_saccade', 'Time_to_exit_saccade', 'Time_to_first_Visit', 'Time_to_first_fixation', 'Time_to_first_whole_fixation', 'Timeline', 'Total_duration_of_Visit', 'Total_duration_of_fixations', 'Total_duration_of_whole_fixations', 'age', 'current_situation', 'current_situation_old', 'educational_background', 'field_of_study', 'filename.et', 'id.global.aoi.participant', 'is.expert', 'list', 'sex', 'situation_actuelle']\n",
      "Columns unique to materials_data: ['AOI_unif_quot', 'flag', 'id.ann', 'id.ann.global', 'id.line', 'is.compound.hyphen', 'is.hyphenated', 'left_dependents', 'lemma_count_screen', 'lemma_count_text', 'lemma_frequency_screen', 'lemma_frequency_text', 'lemma_individual', 'lemma_merged', 'pos_merged', 'right_dependents', 'token_id_merged', 'token_merged', 'total_lemma_count_screen', 'total_lemma_count_text', 'total_word_count_screen', 'total_word_count_text', 'word_count_screen', 'word_count_text', 'word_frequency_screen', 'word_frequency_text']\n",
      "Shared columns: 23\n",
      "\n",
      "Merged dataset created with shape: (30, 119)\n",
      "Merged dataset saved to raw/et_data_merged_with_ann_materials_dummy.csv\n",
      "\n",
      "Preview of merged data:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                      Media textid text.type text_version screenid  \\\n",
       "0   clinical_001_original_1      1  clinical     original        1   \n",
       "1   clinical_001_original_1      1  clinical     original        1   \n",
       "2   clinical_001_original_1      1  clinical     original        1   \n",
       "3   clinical_001_original_1      1  clinical     original        1   \n",
       "4   clinical_001_original_2      1  clinical     original        2   \n",
       "5  medical_002_simplified_1      2   medical   simplified        1   \n",
       "6  medical_002_simplified_1      2   medical   simplified        1   \n",
       "7  medical_002_simplified_1      2   medical   simplified        1   \n",
       "8  medical_002_simplified_1      2   medical   simplified        1   \n",
       "9  medical_002_simplified_1      2   medical   simplified        1   \n",
       "\n",
       "  Sentence_index Word_index   AOI_nopunct                 id.global.aoi list  \\\n",
       "0              1          1           Cas   clinical_001_original_1-1-1  1_A   \n",
       "1              1          2      clinique   clinical_001_original_1-1-2  1_A   \n",
       "2              2          1            Un   clinical_001_original_1-2-1  1_A   \n",
       "3              2          2       patient   clinical_001_original_1-2-2  1_A   \n",
       "4              2          3           âgé   clinical_001_original_2-2-3  1_A   \n",
       "5              1          1            Le  medical_002_simplified_1-1-1  1_A   \n",
       "6              1          2       patient  medical_002_simplified_1-1-2  1_A   \n",
       "7              1          3       souffre  medical_002_simplified_1-1-3  1_A   \n",
       "8              2          1  d’orthopédie  medical_002_simplified_1-2-1  1_A   \n",
       "9              2          2      fracture  medical_002_simplified_1-2-2  1_A   \n",
       "\n",
       "   ... id.phrase.in.brackets AOI.that.in.fact.should.be  tag tag.type tag.id  \\\n",
       "0  ...                   NaN                        NaN  NaN      1.0    NaN   \n",
       "1  ...                   NaN                        NaN  NaN      NaN    NaN   \n",
       "2  ...                   NaN                        NaN  NaN      NaN    NaN   \n",
       "3  ...                   NaN                        NaN  NaN      NaN    NaN   \n",
       "4  ...                   NaN                        NaN  NaN      NaN    NaN   \n",
       "5  ...                   NaN                        NaN  NaN      2.0    NaN   \n",
       "6  ...                   NaN                        NaN  NaN      NaN    NaN   \n",
       "7  ...                   NaN                        NaN  NaN      5.0    NaN   \n",
       "8  ...                   NaN                        NaN  NaN      6.0    NaN   \n",
       "9  ...                   NaN                        NaN  NaN      7.0    NaN   \n",
       "\n",
       "  annotated_text Sentence_id_current Sentence_id_match id.piece  \\\n",
       "0            NaN                   1                 1        0   \n",
       "1            NaN                   1                 1        1   \n",
       "2            NaN                   2                 2        2   \n",
       "3            NaN                   2                 2        3   \n",
       "4            NaN                   2                 2        4   \n",
       "5            NaN                   1                 1        5   \n",
       "6            NaN                   1                 1        6   \n",
       "7            NaN                   1                 1        7   \n",
       "8     orthopédie                   2                 2        8   \n",
       "9       fracture                   2                 2        9   \n",
       "\n",
       "                               filename.ann  \n",
       "0    001_tagged_finished_words_original.csv  \n",
       "1    001_tagged_finished_words_original.csv  \n",
       "2    001_tagged_finished_words_original.csv  \n",
       "3    001_tagged_finished_words_original.csv  \n",
       "4    001_tagged_finished_words_original.csv  \n",
       "5  002_tagged_finished_words_simplified.csv  \n",
       "6  002_tagged_finished_words_simplified.csv  \n",
       "7  002_tagged_finished_words_simplified.csv  \n",
       "8  002_tagged_finished_words_simplified.csv  \n",
       "9  002_tagged_finished_words_simplified.csv  \n",
       "\n",
       "[10 rows x 104 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Media</th>\n",
       "      <th>textid</th>\n",
       "      <th>text.type</th>\n",
       "      <th>text_version</th>\n",
       "      <th>screenid</th>\n",
       "      <th>Sentence_index</th>\n",
       "      <th>Word_index</th>\n",
       "      <th>AOI_nopunct</th>\n",
       "      <th>id.global.aoi</th>\n",
       "      <th>list</th>\n",
       "      <th>...</th>\n",
       "      <th>id.phrase.in.brackets</th>\n",
       "      <th>AOI.that.in.fact.should.be</th>\n",
       "      <th>tag</th>\n",
       "      <th>tag.type</th>\n",
       "      <th>tag.id</th>\n",
       "      <th>annotated_text</th>\n",
       "      <th>Sentence_id_current</th>\n",
       "      <th>Sentence_id_match</th>\n",
       "      <th>id.piece</th>\n",
       "      <th>filename.ann</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clinical_001_original_1</td>\n",
       "      <td>1</td>\n",
       "      <td>clinical</td>\n",
       "      <td>original</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cas</td>\n",
       "      <td>clinical_001_original_1-1-1</td>\n",
       "      <td>1_A</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>001_tagged_finished_words_original.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clinical_001_original_1</td>\n",
       "      <td>1</td>\n",
       "      <td>clinical</td>\n",
       "      <td>original</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>clinique</td>\n",
       "      <td>clinical_001_original_1-1-2</td>\n",
       "      <td>1_A</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>001_tagged_finished_words_original.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clinical_001_original_1</td>\n",
       "      <td>1</td>\n",
       "      <td>clinical</td>\n",
       "      <td>original</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Un</td>\n",
       "      <td>clinical_001_original_1-2-1</td>\n",
       "      <td>1_A</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>001_tagged_finished_words_original.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clinical_001_original_1</td>\n",
       "      <td>1</td>\n",
       "      <td>clinical</td>\n",
       "      <td>original</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>patient</td>\n",
       "      <td>clinical_001_original_1-2-2</td>\n",
       "      <td>1_A</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>001_tagged_finished_words_original.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clinical_001_original_2</td>\n",
       "      <td>1</td>\n",
       "      <td>clinical</td>\n",
       "      <td>original</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>âgé</td>\n",
       "      <td>clinical_001_original_2-2-3</td>\n",
       "      <td>1_A</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>001_tagged_finished_words_original.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>medical_002_simplified_1</td>\n",
       "      <td>2</td>\n",
       "      <td>medical</td>\n",
       "      <td>simplified</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Le</td>\n",
       "      <td>medical_002_simplified_1-1-1</td>\n",
       "      <td>1_A</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>002_tagged_finished_words_simplified.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>medical_002_simplified_1</td>\n",
       "      <td>2</td>\n",
       "      <td>medical</td>\n",
       "      <td>simplified</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>patient</td>\n",
       "      <td>medical_002_simplified_1-1-2</td>\n",
       "      <td>1_A</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>002_tagged_finished_words_simplified.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>medical_002_simplified_1</td>\n",
       "      <td>2</td>\n",
       "      <td>medical</td>\n",
       "      <td>simplified</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>souffre</td>\n",
       "      <td>medical_002_simplified_1-1-3</td>\n",
       "      <td>1_A</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>002_tagged_finished_words_simplified.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>medical_002_simplified_1</td>\n",
       "      <td>2</td>\n",
       "      <td>medical</td>\n",
       "      <td>simplified</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>d’orthopédie</td>\n",
       "      <td>medical_002_simplified_1-2-1</td>\n",
       "      <td>1_A</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>orthopédie</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>002_tagged_finished_words_simplified.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>medical_002_simplified_1</td>\n",
       "      <td>2</td>\n",
       "      <td>medical</td>\n",
       "      <td>simplified</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>fracture</td>\n",
       "      <td>medical_002_simplified_1-2-2</td>\n",
       "      <td>1_A</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fracture</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>002_tagged_finished_words_simplified.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 104 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Merge Key Diagnostics ---\n",
      "AOI IDs in ET data: 10\n",
      "AOI IDs in materials: 10\n",
      "Common AOI IDs: 10\n",
      "Example matched AOI IDs: ['clinical_001_original_1-1-1', 'clinical_001_original_1-1-2', 'medical_002_simplified_1-1-1', 'clinical_001_original_1-2-1', 'medical_002_simplified_1-1-3']\n",
      "\n",
      "Example ET-only AOIs: []\n",
      "Example materials-only AOIs: []\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Machine Learning Feature Aggregation Pipeline\n",
    "\n",
    "**Overview**\n",
    "\n",
    "This pipeline represents the final stage of preprocessing, transforming the merged\n",
    "eye-tracking × linguistic dataset into structured input for machine-learning models.\n",
    "\n",
    "It aggregates numerical eye-tracking and linguistic metrics per participant\n",
    "and text subset, producing a compact representation of each participant’s reading behavior.\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs\n",
    "- `et_data_merged_with_ann_materials_dummy.csv`\n",
    "  (originally `et_data_merged_with_ann_materials_25_06_17.csv`)\n",
    "  Unified dataset containing both linguistic and eye-tracking features at the AOI level.\n",
    "  The dummy file reproduces the same structure and column schema as the real dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Outputs\n",
    "- `agg_data/ml_features_<scope>_<timestamp>.csv`\n",
    "  Aggregated machine-learning feature tables.\n",
    "  The `<scope>` token reflects the subset of features selected\n",
    "  (e.g., `all`, `medical`, `non_medical`, `content`).\n",
    "\n",
    "---\n",
    "\n",
    "## Process Summary\n",
    "\n",
    "1. **Load data**\n",
    "   Reads the merged AOI-level dataset and verifies file integrity.\n",
    "\n",
    "2. **Automatic feature discovery**\n",
    "   Identifies all numerical columns not listed in the exclusion set.\n",
    "   Ensures future extensibility by automatically including new numeric features.\n",
    "\n",
    "3. **Aggregation**\n",
    "   Groups features by participant (`Participant_unique`) and text subset,\n",
    "   computing summary statistics (`mean`, `std`, `min`, `max`).\n",
    "\n",
    "4. **Subset-specific feature creation**\n",
    "   Builds filtered subsets (e.g., *medical*, *non-medical*, *content words*),\n",
    "   aggregates each independently, and merges them into the main feature table.\n",
    "\n",
    "5. **Label assignment and imputation**\n",
    "   Adds the target variable (`is.expert`) and fills missing feature values with zeros.\n",
    "\n",
    "6. **Export**\n",
    "   Saves a timestamped, tab-delimited file in `agg_data/` containing all aggregated features.\n",
    "\n",
    "---\n",
    "\n",
    "**Example scopes**\n",
    "- `\"feature_scope\": \"all\"` → Aggregate all AOIs.\n",
    "- `\"feature_scope\": \"medical,non_medical\"` → Generate medical and non-medical subsets.\n",
    "- `\"feature_scope\": \"content\"` → Restrict aggregation to content-word AOIs (NOUN, VERB, ADJ)."
   ],
   "id": "7fff9bc48b4abd6f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T15:39:51.548414Z",
     "start_time": "2025-10-20T15:39:51.463013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"Loads data from the specified filepath.\"\"\"\n",
    "    print(f\"Loading data from '{filepath}'...\")\n",
    "    try:\n",
    "        return pd.read_csv(filepath, sep=\"\\t\", low_memory=False)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{filepath}' was not found.\")\n",
    "        return None\n",
    "\n",
    "def discover_features(df, exclude_cols):\n",
    "    \"\"\"Automatically discovers numerical features to aggregate, excluding specified columns.\"\"\"\n",
    "    print(\"Automatically discovering numerical features...\")\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    features = [col for col in numerical_cols if col not in exclude_cols]\n",
    "    print(f\"Found {len(features)} features to aggregate.\")\n",
    "    return features\n",
    "\n",
    "def aggregate_data(data, group_by_cols, feature_cols, agg_funcs):\n",
    "    \"\"\"Performs data aggregation.\"\"\"\n",
    "    print(f\"Aggregating {len(feature_cols)} features by {group_by_cols}...\")\n",
    "    return data.groupby(group_by_cols)[feature_cols].agg(agg_funcs)\n",
    "\n",
    "def process_feature_subsets(df, features_to_agg, agg_funcs, subset_configs):\n",
    "    \"\"\"Creates and returns a dictionary of aggregated feature subsets.\"\"\"\n",
    "    subset_dfs = {}\n",
    "    if not subset_configs:\n",
    "        return subset_dfs\n",
    "\n",
    "    print(\"\\nProcessing feature subsets...\")\n",
    "    for config in subset_configs:\n",
    "        name = config['name']\n",
    "        filter_col = config['filter_col']\n",
    "        filter_vals = config['filter_values']\n",
    "        group_by = config['group_by']\n",
    "        # Default to 'exact' if method is not specified\n",
    "        filter_method = config.get('filter_method', 'exact')\n",
    "\n",
    "        if filter_col not in df.columns:\n",
    "            print(f\"Warning: Subset '{name}' requires column '{filter_col}', which was not found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"--- Creating subset: '{name}' (using '{filter_method}' filter) ---\")\n",
    "\n",
    "        if filter_method == 'contains':\n",
    "            # Create a regex pattern to find any of the values in the list\n",
    "            # e.g., 'NOUN|VERB|ADJ'\n",
    "            regex_pattern = '|'.join(filter_vals)\n",
    "            # Filter rows where the column string contains any of the POS tags\n",
    "            subset_data = df[df[filter_col].str.contains(regex_pattern, na=False)].copy()\n",
    "        elif filter_method == 'exclude':\n",
    "            # Select rows where the value in filter_col is NOT in the filter_values list.\n",
    "            subset_data = df[~df[filter_col].isin(filter_vals)].copy()\n",
    "        else: # Default to 'exact' matching\n",
    "            subset_data = df[df[filter_col].isin(filter_vals)].copy()\n",
    "\n",
    "        if subset_data.empty:\n",
    "            print(f\"Warning: No data found for subset '{name}'. No features will be added.\")\n",
    "            continue\n",
    "\n",
    "        agg_df = aggregate_data(subset_data, group_by, features_to_agg, agg_funcs)\n",
    "        agg_df.columns = ['_'.join(map(str, col)).strip('_') + f'_{name}' for col in agg_df.columns]\n",
    "        subset_dfs[name] = agg_df.reset_index()\n",
    "        print(f\"Successfully created {len(agg_df.columns)} features for subset '{name}'.\")\n",
    "\n",
    "    return subset_dfs\n",
    "\n",
    "def flatten_and_reshape(df, widen, group_by_cols):\n",
    "    \"\"\"Flattens columns and optionally reshapes the DataFrame to a wide format.\"\"\"\n",
    "    if widen:\n",
    "        print(\"Reshaping data to wide format...\")\n",
    "        df = df.unstack(level=group_by_cols[1:])\n",
    "\n",
    "    print(\"Flattening column names...\")\n",
    "    df.columns = ['_'.join(map(str, col)).strip('_') for col in df.columns]\n",
    "    return df.reset_index()\n",
    "\n",
    "def merge_dataframes(main_df, subset_dfs):\n",
    "    \"\"\"Merges the main aggregated DataFrame with all subset DataFrames.\"\"\"\n",
    "    if not subset_dfs:\n",
    "        return main_df\n",
    "\n",
    "    print(\"\\nMerging data with subset features...\")\n",
    "    for name, df_to_merge in subset_dfs.items():\n",
    "        merge_key = df_to_merge.columns[0]\n",
    "        main_df = pd.merge(main_df, df_to_merge, on=merge_key, how='left')\n",
    "        print(f\"Merged '{name}' features.\")\n",
    "    return main_df\n",
    "\n",
    "def save_data(df, output_path):\n",
    "    \"\"\"Saves the final DataFrame to a specified path.\"\"\"\n",
    "    print(f\"\\n--- Preprocessing Complete! ---\")\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(output_path, sep=\"\\t\", index=False)\n",
    "    print(f\"Final preprocessed data saved to: {output_path}\")\n",
    "    print(f\"Shape of the final DataFrame: {df.shape}\")\n",
    "\n",
    "def main(config):\n",
    "    \"\"\"Main function to orchestrate the data aggregation pipeline.\"\"\"\n",
    "    # Unpack config for clarity\n",
    "    paths = config['paths']\n",
    "    params = config['parameters']\n",
    "    columns = config['columns']\n",
    "    funcs = config['aggregation_functions']\n",
    "    subsets_config = config.get('feature_subsets', [])\n",
    "\n",
    "    # Load data and identify numeric variables to aggregate\n",
    "    raw_data = load_data(paths['input_filepath'])\n",
    "    if raw_data is None:\n",
    "        return # Stop execution if data loading failed\n",
    "    features_to_agg = discover_features(raw_data, columns['exclude_from_aggregation'])\n",
    "\n",
    "    # Determine which aggregation functions to use\n",
    "    agg_funcs_to_use = funcs.get('full')\n",
    "    if params.get('use_reduced_aggs', False):\n",
    "        agg_funcs_to_use = funcs.get('reduced')\n",
    "        print(\"Using reduced aggregation functions.\")\n",
    "    else:\n",
    "        print(\"Using full aggregation functions.\")\n",
    "\n",
    "    # 1. Create a base DataFrame with unique participant IDs to merge everything into\n",
    "    id_col = columns['id_column']\n",
    "    merged_data = raw_data[[id_col]].drop_duplicates().reset_index(drop=True)\n",
    "    print(f\"\\nCreated base DataFrame with {len(merged_data)} unique participants.\")\n",
    "\n",
    "    # Get the feature scope and parse it into components for robust checking\n",
    "    feature_scope = params.get('feature_scope', '')\n",
    "    scope_parts = feature_scope.split(',') # Use comma as the delimiter\n",
    "    print(f\"Parsed feature scope: {scope_parts}\")\n",
    "\n",
    "    # 2. Conditionally perform the main aggregation if \"all\" is in the scope parts\n",
    "    if 'all' in scope_parts:\n",
    "        print(\"\\nPerforming main aggregation on all data...\")\n",
    "        main_agg = aggregate_data(raw_data, columns['main_group_by'], features_to_agg, agg_funcs_to_use)\n",
    "        main_agg_flat = flatten_and_reshape(main_agg, params['widen_format'], columns['main_group_by'])\n",
    "        merged_data = pd.merge(merged_data, main_agg_flat, on=id_col, how='left')\n",
    "        print(\"Main aggregation features merged.\")\n",
    "    else:\n",
    "        print(\"\\nSkipping main aggregation as 'all' is not in feature_scope.\")\n",
    "\n",
    "    # 3. Process and merge special feature subsets that are mentioned in the scope parts\n",
    "    active_subsets = [s for s in subsets_config if s['name'] in scope_parts]\n",
    "    subset_dfs = process_feature_subsets(raw_data, features_to_agg, agg_funcs_to_use, active_subsets)\n",
    "    merged_data = merge_dataframes(merged_data, subset_dfs)\n",
    "\n",
    "    # 4. Merge expertise labels (if available) and impute missing values\n",
    "    if columns['label_column'] in raw_data.columns:\n",
    "        expertise_df = raw_data.groupby(id_col)[columns['label_column']].first().reset_index()\n",
    "        merged_data = pd.merge(merged_data, expertise_df, on=id_col, how='left')\n",
    "        print(\"\\nExpertise labels merged.\")\n",
    "    else:\n",
    "        print(f\"\\nWarning: Label column '{columns['label_column']}' not found. Skipping label merge.\")\n",
    "\n",
    "    # Impute NaNs for all columns except the main ID column\n",
    "    feature_cols_to_impute = [col for col in merged_data.columns if col != id_col]\n",
    "    merged_data[feature_cols_to_impute] = merged_data[feature_cols_to_impute].fillna(0)\n",
    "    print(f\"Missing values imputed. Remaining nulls: {merged_data.isnull().sum().sum()}\")\n",
    "    final_data = merged_data\n",
    "\n",
    "    # 5. Construct dynamic output filename and save\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    # shape_str = 'wide' if params['widen_format'] else 'long'\n",
    "    # agg_str = 'reduced_aggs' if params['use_reduced_aggs'] else 'full_aggs'\n",
    "    # output_filename = f\"{paths['output_prefix']}_{feature_scope}_{shape_str}_{agg_str}_{timestamp}.csv\"\n",
    "\n",
    "    output_filename = f\"{paths['output_prefix']}_{feature_scope}_{timestamp}.csv\"\n",
    "    output_path = Path(paths['output_directory']) / output_filename\n",
    "\n",
    "    # Save the final aggregated dataset with a timestamped filename\n",
    "    save_data(final_data, output_path)\n",
    "\n",
    "\n",
    "config = {\n",
    "    # --- File Paths ---\n",
    "    \"paths\": {\n",
    "        \"input_filepath\": \"raw/et_data_merged_with_ann_materials_dummy.csv\",\n",
    "        \"output_directory\": \"agg_data\",\n",
    "        \"output_prefix\": \"ml_features\"\n",
    "    },\n",
    "\n",
    "    # --- Pipeline Parameters ---\n",
    "    \"parameters\": {\n",
    "        \"widen_format\": True,        # true for wide format, false for long format\n",
    "        \"use_reduced_aggs\": False,     # true for [mean, std], false for [mean, std, min, max]\n",
    "        # Controls which features are generated by matching names from 'feature_subsets'\n",
    "        # Examples:\n",
    "        # \"feature_scope\": \"medical\"                --> will produce ['medical']\n",
    "        # \"feature_scope\": \"medical,non_medical\"    --> will produce ['medical', 'non_medical']\n",
    "        # \"feature_scope\": \"all,content\"            --> will produce ['all', 'content']\n",
    "        \"feature_scope\": \"non_medical\"\n",
    "    },\n",
    "\n",
    "    # --- Column Definitions ---\n",
    "    \"columns\": {\n",
    "        \"id_column\": \"Participant_unique\",\n",
    "        \"label_column\": \"is.expert\",\n",
    "        \"main_group_by\": [\n",
    "            \"Participant_unique\",\n",
    "            \"text.type\",\n",
    "            \"text_version\"\n",
    "        ],\n",
    "        # Columns to EXCLUDE from automatic feature discovery\n",
    "        \"exclude_from_aggregation\": [\n",
    "            \"Participant_unique\", \"text.type\", \"text_version\", \"Participant\",\n",
    "            \"list\", \"textid\", \"screenid\", \"Sentence_index\", \"Word_index\",\n",
    "            \"is.expert\", \"age\", \"sex\", \"Payment\", \"educational_background\",\n",
    "            \"field_of_study\", \"current_situation\", \"current_situation_old\",\n",
    "            \"Participant_commnts_new\", \"Recording\", \"situation_actuelle\",\n",
    "            \"Timeline\", \"Media\", \"Media_old\", \"tag.type\"\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # --- Aggregation Function Sets ---\n",
    "    \"aggregation_functions\": {\n",
    "        \"full\": [\"mean\", \"std\", \"min\", \"max\"],\n",
    "        \"reduced\": [\"mean\", \"std\"]\n",
    "    },\n",
    "\n",
    "    # --- Feature Subset Extraction ---\n",
    "    # Define all possible feature subsets here.\n",
    "    # The pipeline will only generate the ones whose 'name' is in 'feature_scope'\n",
    "    \"feature_subsets\": [\n",
    "        {\n",
    "            \"name\": \"medical\",\n",
    "            \"filter_col\": \"tag.type\",\n",
    "            \"filter_values\": [1, 2, 3, 4, 5, 7],\n",
    "            \"group_by\": [\"Participant_unique\"],\n",
    "            \"filter_method\": \"exact\" # Use exact matching for numerical tags\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"non_medical\",\n",
    "            \"filter_col\": \"tag.type\",\n",
    "            # Using the same list of medical tags, but to EXCLUDE them\n",
    "            \"filter_values\": [1, 2, 3, 4, 5, 7],\n",
    "            \"group_by\": [\"Participant_unique\"],\n",
    "            \"filter_method\": \"exclude\" # Exclude rows with these tags\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"content\",\n",
    "          \"filter_col\": \"pos_merged\",\n",
    "          \"filter_values\": [\"NOUN\", \"VERB\", \"ADJ\"],\n",
    "          \"group_by\": [\"Participant_unique\"],\n",
    "          \"filter_method\": \"contains\" # Use 'contains' for concatenated strings\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- EXECUTE THE PIPELINE ---\n",
    "main(config)"
   ],
   "id": "6b8782c3c49f6949",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from 'raw/et_data_merged_with_ann_materials_dummy.csv'...\n",
      "Automatically discovering numerical features...\n",
      "Found 64 features to aggregate.\n",
      "Using full aggregation functions.\n",
      "\n",
      "Created base DataFrame with 3 unique participants.\n",
      "Parsed feature scope: ['non_medical']\n",
      "\n",
      "Skipping main aggregation as 'all' is not in feature_scope.\n",
      "\n",
      "Processing feature subsets...\n",
      "--- Creating subset: 'non_medical' (using 'exclude' filter) ---\n",
      "Aggregating 64 features by ['Participant_unique']...\n",
      "Successfully created 256 features for subset 'non_medical'.\n",
      "\n",
      "Merging data with subset features...\n",
      "Merged 'non_medical' features.\n",
      "\n",
      "Expertise labels merged.\n",
      "Missing values imputed. Remaining nulls: 0\n",
      "\n",
      "--- Preprocessing Complete! ---\n",
      "Final preprocessed data saved to: agg_data/ml_features_non_medical_20251020_173951.csv\n",
      "Shape of the final DataFrame: (3, 258)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Appendix – Diagnostic and Validation Utilities\n",
    "\n",
    "This appendix includes two simple diagnostic tools used to verify data quality and schema consistency\n",
    "across the pipeline. These checks are not part of the main preprocessing workflow but ensure\n",
    "transparency, reproducibility, and data integrity before statistical modeling.\n",
    "\n",
    "### 1. Compare Dataset Schemas\n",
    "\n",
    "Verifies that datasets from different processing stages contain the expected column structures.\n",
    "This check helps confirm that merging and enrichment steps preserve required variables."
   ],
   "id": "b87422fae544998c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T15:39:55.857081Z",
     "start_time": "2025-10-20T15:39:55.853333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Utility: Compare column sets between two DataFrames\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def compare_columns(df1: pd.DataFrame, name1: str, df2: pd.DataFrame, name2: str):\n",
    "    \"\"\"\n",
    "    Compare the columns of two DataFrames and print overlaps and differences.\n",
    "\n",
    "    Args:\n",
    "        df1: First DataFrame.\n",
    "        name1: Display name for df1.\n",
    "        df2: Second DataFrame.\n",
    "        name2: Display name for df2.\n",
    "    \"\"\"\n",
    "    cols1 = list(df1.columns)\n",
    "    cols2 = list(df2.columns)\n",
    "\n",
    "    set1, set2 = set(cols1), set(cols2)\n",
    "\n",
    "    only_in_1 = sorted(set1 - set2)\n",
    "    only_in_2 = sorted(set2 - set1)\n",
    "    common    = sorted(set1 & set2)\n",
    "\n",
    "    print(f\"Columns in {name1} ({len(cols1)}):\")\n",
    "    print(cols1)\n",
    "    print()\n",
    "    print(f\"Columns in {name2} ({len(cols2)}):\")\n",
    "    print(cols2)\n",
    "    print(\"\\n— Differences —\")\n",
    "    print(f\"Only in {name1} ({len(only_in_1)}): {only_in_1}\")\n",
    "    print(f\"Only in {name2} ({len(only_in_2)}): {only_in_2}\")\n",
    "    print(f\"Common ({len(common)}): {common}\")"
   ],
   "id": "e02c8f334c445e",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T15:39:56.163172Z",
     "start_time": "2025-10-20T15:39:56.151238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ann_materials = pd.read_csv('raw/10_materials_parsed_collapsed.csv', sep=\"\\t\", low_memory=False)\n",
    "et_data       = pd.read_csv('raw/00_input_eye_tracking_data_with_metrics_dummy.csv', sep=\"\\t\", low_memory=False)\n",
    "\n",
    "compare_columns(ann_materials, \"ann_materials\", et_data, \"et_data\")"
   ],
   "id": "1a0ec1980b72c78d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in ann_materials (39):\n",
      "['Media', 'textid', 'text.type', 'text_version', 'screenid', 'Sentence_index', 'Word_index', 'word_id_screen', 'word_id_text', 'AOI_ann', 'AOI_length', 'is.in.bracket', 'id.phrase.in.brackets', 'AOI.that.in.fact.should.be', 'tag', 'tag.type', 'tag.id', 'annotated_text', 'flag', 'id.ann', 'id.ann.global', 'Sentence_id_current', 'Sentence_id_match', 'id.piece', 'lemma_merged', 'pos_merged', 'token_merged', 'token_id_merged', 'is.hyphenated', 'is.compound.hyphen', 'word_count_text', 'word_count_screen', 'total_word_count_text', 'left_dependents', 'right_dependents', 'id.global.aoi', 'AOI_unif_quot', 'filename.ann', 'id.line']\n",
      "\n",
      "Columns in et_data (78):\n",
      "['Media', 'textid', 'text.type', 'text_version', 'screenid', 'Sentence_index', 'Word_index', 'word_id_screen', 'word_id_text', 'AOI_ann', 'AOI_nopunct', 'AOI_length', 'is.in.bracket', 'id.phrase.in.brackets', 'AOI.that.in.fact.should.be', 'tag', 'tag.type', 'tag.id', 'annotated_text', 'Sentence_id_current', 'Sentence_id_match', 'id.piece', 'filename.ann', 'id.global.aoi', 'list', 'Participant', 'Participant_unique', 'age', 'sex', 'Payment', 'is.expert', 'educational_background', 'field_of_study', 'current_situation', 'current_situation_old', 'Participant_commnts_new', 'Recording', 'situation_actuelle', 'Timeline', 'Media_old', 'id.global.aoi.participant', 'Total_duration_of_fixations', 'Average_duration_of_fixations', 'Minimum_duration_of_fixations', 'Maximum_duration_of_fixations', 'Number_of_fixations', 'Time_to_first_fixation', 'Duration_of_first_fixation', 'Average_pupil_diameter', 'Total_duration_of_whole_fixations', 'Average_duration_of_whole_fixations', 'Minimum_duration_of_whole_fixations', 'Maximum_duration_of_whole_fixations', 'Number_of_whole_fixations', 'Time_to_first_whole_fixation', 'Duration_of_first_whole_fixation', 'Average_whole-fixation_pupil_diameter', 'Total_duration_of_Visit', 'Average_duration_of_Visit', 'Minimum_duration_of_Visit', 'Maximum_duration_of_Visit', 'Number_of_Visits', 'Time_to_first_Visit', 'Duration_of_first_Visit', 'Number_of_saccades_in_AOI', 'Time_to_entry_saccade', 'Time_to_exit_saccade', 'Peak_velocity_of_entry_saccade', 'Peak_velocity_of_exit_saccade', 'First-pass_first_fixation_duration', 'First-pass_duration', 'Regression-path_duration', 'Selective_regression-path_duration', 'First-pass_regression', 'Re-reading_duration', 'Average_eye_openness', 'Average_whole-fixation_eye_openness', 'filename.et']\n",
      "\n",
      "— Differences —\n",
      "Only in ann_materials (16): ['AOI_unif_quot', 'flag', 'id.ann', 'id.ann.global', 'id.line', 'is.compound.hyphen', 'is.hyphenated', 'left_dependents', 'lemma_merged', 'pos_merged', 'right_dependents', 'token_id_merged', 'token_merged', 'total_word_count_text', 'word_count_screen', 'word_count_text']\n",
      "Only in et_data (55): ['AOI_nopunct', 'Average_duration_of_Visit', 'Average_duration_of_fixations', 'Average_duration_of_whole_fixations', 'Average_eye_openness', 'Average_pupil_diameter', 'Average_whole-fixation_eye_openness', 'Average_whole-fixation_pupil_diameter', 'Duration_of_first_Visit', 'Duration_of_first_fixation', 'Duration_of_first_whole_fixation', 'First-pass_duration', 'First-pass_first_fixation_duration', 'First-pass_regression', 'Maximum_duration_of_Visit', 'Maximum_duration_of_fixations', 'Maximum_duration_of_whole_fixations', 'Media_old', 'Minimum_duration_of_Visit', 'Minimum_duration_of_fixations', 'Minimum_duration_of_whole_fixations', 'Number_of_Visits', 'Number_of_fixations', 'Number_of_saccades_in_AOI', 'Number_of_whole_fixations', 'Participant', 'Participant_commnts_new', 'Participant_unique', 'Payment', 'Peak_velocity_of_entry_saccade', 'Peak_velocity_of_exit_saccade', 'Re-reading_duration', 'Recording', 'Regression-path_duration', 'Selective_regression-path_duration', 'Time_to_entry_saccade', 'Time_to_exit_saccade', 'Time_to_first_Visit', 'Time_to_first_fixation', 'Time_to_first_whole_fixation', 'Timeline', 'Total_duration_of_Visit', 'Total_duration_of_fixations', 'Total_duration_of_whole_fixations', 'age', 'current_situation', 'current_situation_old', 'educational_background', 'field_of_study', 'filename.et', 'id.global.aoi.participant', 'is.expert', 'list', 'sex', 'situation_actuelle']\n",
      "Common (23): ['AOI.that.in.fact.should.be', 'AOI_ann', 'AOI_length', 'Media', 'Sentence_id_current', 'Sentence_id_match', 'Sentence_index', 'Word_index', 'annotated_text', 'filename.ann', 'id.global.aoi', 'id.phrase.in.brackets', 'id.piece', 'is.in.bracket', 'screenid', 'tag', 'tag.id', 'tag.type', 'text.type', 'text_version', 'textid', 'word_id_screen', 'word_id_text']\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Check Average Data Length per Participant\n",
    "\n",
    "Evaluates the number of AOI entries per participant to assess dataset balance.\n",
    "This helps ensure that each participant contributed a similar amount of valid data,\n",
    "and can highlight missing or incomplete participant recordings.\n"
   ],
   "id": "1ff441649c47990b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T15:40:07.061508Z",
     "start_time": "2025-10-20T15:40:07.051152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def average_data_length_per_participant(df: pd.DataFrame, participant_col: str = \"Participant_unique\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the number of AOI-level entries (data length) per participant.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Merged dataset containing participant data.\n",
    "        participant_col (str): Column identifying participants.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Participant IDs with their respective AOI counts.\n",
    "    \"\"\"\n",
    "    return df.groupby(participant_col).size().reset_index(name=\"average_data_length\")\n",
    "\n",
    "# Example usage on the merged ET–annotation dataset\n",
    "merged_data = pd.read_csv('raw/et_data_merged_with_ann_materials_dummy.csv', sep=\"\\t\", low_memory=False)\n",
    "participant_col = \"Participant_unique\"\n",
    "average_length = average_data_length_per_participant(merged_data)\n",
    "\n",
    "# Display range of participant data lengths\n",
    "min_participant = average_length.loc[average_length[\"average_data_length\"].idxmin()]\n",
    "max_participant = average_length.loc[average_length[\"average_data_length\"].idxmax()]\n",
    "\n",
    "print(\n",
    "    f\"{min_participant[participant_col]} has a minimum data length of {min_participant['average_data_length']}\"\n",
    ")\n",
    "print(\n",
    "    f\"{max_participant[participant_col]} has a maximum data length of {max_participant['average_data_length']}\"\n",
    ")"
   ],
   "id": "b58d07b56e4e9dc5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participant10-1_A has a minimum data length of 10\n",
      "Participant10-1_A has a maximum data length of 10\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# API Annotation Pipeline\n",
    "\n",
    "This notebook performs the full annotation process described in Section 3.5.1 of the thesis.\n",
    "It uses external API (LLM-based) calls to obtain tokenization, lemmatization, and POS tagging\n",
    "for AOI-level texts derived from the lemma-enriched materials.\n",
    "\n",
    "**Input**\n",
    "- `data/05_indexed_lemmas.csv` — lemma-enriched materials (from 03_data_prep.ipynb)\n",
    "\n",
    "**Intermediate files created in this notebook**\n",
    "- `data/05_aoi_dictionary_simplified.json` — normalized AOI sentences for API prompting\n",
    "\n",
    "**Outputs (in order)**\n",
    "1. `data/06_api_responses.csv` — raw API tokenization responses\n",
    "2. `data/07_api_responses_with_lemmas.csv` — extended responses with lemmas and POS tags\n",
    "3. `data/08_api_parsed_responses.csv` — structured token–lemma–POS table\n",
    "4. `data/09_api_materials_parsed.csv` — AOI-aligned parsed materials\n",
    "5. `data/10_api_materials_corrected.csv` — manually verified version\n",
    "6. `data/11_api_materials_supervisor_corrected.csv` — supervisor-checked dataset\n",
    "7. `data/12_api_materials_enriched.csv` — final enriched annotation dataset for merging\n"
   ],
   "id": "e84a949893ef32aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup: Imports and File Paths\n",
    "\n",
    "Import all required libraries and define standardized input and output paths for this notebook.\n",
    "All file paths are centralized in a single dictionary (`PATHS`) to ensure reproducibility and\n",
    "consistent file handling across the entire data preparation pipeline.\n"
   ],
   "id": "38cedaaaf4d86fdf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T11:32:39.196095Z",
     "start_time": "2025-10-13T11:32:38.833771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import openai\n",
    "\n",
    "# Define canonical data paths for this notebook\n",
    "PATHS = {\n",
    "    # Inputs\n",
    "    \"lemmas\": \"data/05_indexed_lemmas.csv\",\n",
    "    \"aoi_dict_simplified\": \"data/05_aoi_dictionary_simplified.json\",\n",
    "\n",
    "    # API Annotation Outputs\n",
    "    \"responses\": \"data/06_api_responses.csv\",\n",
    "    \"responses_with_lemmas\": \"data/07_api_responses_with_lemmas.csv\",\n",
    "    \"parsed\": \"data/08_api_parsed_responses.csv\",\n",
    "    \"materials_parsed\": \"data/09_api_materials_parsed.csv\",\n",
    "    \"final\": \"data/10_materials_parsed_collapsed.csv\"\n",
    "}\n",
    "\n",
    "# Ensure all required directories exist\n",
    "for path in PATHS.values():\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)"
   ],
   "id": "ddcbdd6e1f80520c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2: Create Simplified AOI Dictionary for API Annotation\n",
    "\n",
    "This step constructs a simplified AOI dictionary from the lemma-enriched materials (`05_indexed_lemmas.csv`).\n",
    "Each entry groups AOIs by their base key and stores:\n",
    "- `ids`: list of `[Word_index, AOI_ann]` pairs\n",
    "- `concat`: concatenated AOI text\n",
    "\n",
    "This simplified version is used for efficient API prompting and can optionally be reduced further\n",
    "to a slim subset (`slim_dict.json`) for testing or cost estimation.\n"
   ],
   "id": "3d5de5d213df547a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T09:55:37.040166Z",
     "start_time": "2025-10-07T09:55:37.019610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\n",
    "    PATHS[\"lemmas\"],\n",
    "    sep=\"\\t\",\n",
    "    quoting=0,\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "def get_base_key(aoi_id: str) -> str:\n",
    "    \"\"\"Extract the base key from an AOI ID by stripping trailing digits.\"\"\"\n",
    "    match = re.search(r\"(.*?)(\\d+)$\", aoi_id)\n",
    "    return match.group(1) if match else aoi_id\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize apostrophes and spacing; remove unwanted punctuation.\"\"\"\n",
    "    text = re.sub(r\"[‘’´′]\", \"'\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "result_dict = {}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    aoi_id = row[\"id.global.aoi\"]\n",
    "    word_id = row[\"Word_index\"]\n",
    "    aoi_content = normalize_text(row[\"AOI_ann\"])\n",
    "\n",
    "    base_key = get_base_key(aoi_id)\n",
    "\n",
    "    if base_key not in result_dict:\n",
    "        result_dict[base_key] = {\"ids\": [], \"concat\": \"\"}\n",
    "\n",
    "    result_dict[base_key][\"ids\"].append([word_id, aoi_content])\n",
    "\n",
    "    if result_dict[base_key][\"concat\"]:\n",
    "        result_dict[base_key][\"concat\"] += \" \" + aoi_content\n",
    "    else:\n",
    "        result_dict[base_key][\"concat\"] = aoi_content\n",
    "\n",
    "output_path = Path(PATHS[\"aoi_dict_simplified\"])\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Simplified AOI dictionary saved to {output_path} with {len(result_dict)} entries.\")"
   ],
   "id": "17129a74329311e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified AOI dictionary saved to data/05_aoi_dictionary_simplified.json with 5 entries.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 3: Define Helper Functions for API Annotation\n",
    "\n",
    "This section defines the reusable helper functions and prompts used for API-based\n",
    "tokenization and lemmatization. These functions are shared across all subsequent steps."
   ],
   "id": "8dd9dca53fef8e3b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T09:55:37.055384Z",
     "start_time": "2025-10-07T09:55:37.047014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def batch_tokens_and_sentences(data, batch_size=120):\n",
    "    \"\"\"Create batches of tokens and sentences for API processing.\"\"\"\n",
    "    token_batch, sentence_batch = [], []\n",
    "    for key, value in data.items():\n",
    "        tokens = [item[1] for item in value[\"ids\"]]\n",
    "        sentence = value[\"concat\"]\n",
    "        if len(token_batch) + len(tokens) <= batch_size:\n",
    "            token_batch.extend(tokens)\n",
    "            sentence_batch.append(sentence)\n",
    "        else:\n",
    "            yield token_batch, sentence_batch\n",
    "            token_batch, sentence_batch = tokens, [sentence]\n",
    "    if token_batch:\n",
    "        yield token_batch, sentence_batch\n",
    "\n",
    "\n",
    "def extract_tokens(batch):\n",
    "    \"\"\"Extract tokens and assign an index within the batch.\"\"\"\n",
    "    return [(token, i) for i, token in enumerate(batch[\"tokens\"])]\n",
    "\n",
    "\n",
    "def format_tokens_for_prompt(token_id_list):\n",
    "    \"\"\"Format token–ID pairs into a string for inclusion in the API prompt.\"\"\"\n",
    "    return \"\\n\".join([f\"{token} {token_id}\" for token, token_id in token_id_list])\n",
    "\n",
    "\n",
    "class LLMCaller:\n",
    "    \"\"\"Wrapper for OpenAI ChatCompletion calls.\"\"\"\n",
    "    def __init__(self, api_key, model_name=\"gpt-4o\"):\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "        openai.api_key = api_key\n",
    "\n",
    "    def call_llm_openai(self, system_prompt, user_prompt):\n",
    "        \"\"\"Send a formatted system/user prompt to the API and return the model response.\"\"\"\n",
    "        response = openai.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def remove_triple_backticks_with_newlines(text):\n",
    "    \"\"\"Remove leading/trailing triple backticks and adjacent newlines.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"^```(?:\\n)?\", \"\", text)\n",
    "    text = re.sub(r\"(?:\\n)?```$\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def simulate_tokenization_response(tokens):\n",
    "    \"\"\"\n",
    "    Generate a realistic synthetic tokenization response that mimics\n",
    "    LLM tokenization behavior, including punctuation splitting and contractions.\n",
    "    Tokens within a batch are separated by tabs ('\\\\t') instead of newlines.\n",
    "    \"\"\"\n",
    "    punct = [\".\", \",\", \";\", \":\", \"!\", \"?\"]\n",
    "    result = []\n",
    "    word_id = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        token = str(token).strip()\n",
    "\n",
    "        # handle French contractions manually: e.g., \"d’orthopédie\", \"l'os\"\n",
    "        if \"’\" in token or \"'\" in token:\n",
    "            for apos in [\"’\", \"'\"]:\n",
    "                if apos in token:\n",
    "                    split_idx = token.index(apos)\n",
    "                    prefix = token[:split_idx].strip()\n",
    "                    rest = token[split_idx + 1 :].strip()\n",
    "                    if prefix:\n",
    "                        result.append(f\"{prefix}|{word_id}\")\n",
    "                    result.append(f\"'|{word_id}\")\n",
    "                    if rest:\n",
    "                        result.append(f\"{rest}|{word_id}\")\n",
    "                    break\n",
    "\n",
    "        # simulate punctuation split (e.g. \"clinique.\")\n",
    "        elif any(token.endswith(p) for p in punct):\n",
    "            word = token[:-1].strip()\n",
    "            mark = token[-1]\n",
    "            if word:\n",
    "                result.append(f\"{word}|{word_id}\")\n",
    "            result.append(f\"{mark}|{word_id}\")\n",
    "\n",
    "        # normal case\n",
    "        else:\n",
    "            result.append(f\"{token}|{word_id}\")\n",
    "\n",
    "        word_id += 1\n",
    "\n",
    "    # join tokens with spaces (safe for TSV export)\n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "\n",
    "# Prompts for tokenization and lemmatization\n",
    "\n",
    "system_prompt_tokenization = \"\"\"You are an NLP model specialized in processing French medical texts. Tokenize this sentence into \"Token|WordID\" pairs, one per line.\n",
    "Assign IDs to the token based on the given Word IDs.\n",
    " **ABSOLUTELY ALWAYS** break contractions, compound words, and punctuation into separate tokens.\n",
    "Repeat the original ID for all split parts; do not concatenate tokens.\n",
    "Output only valid \"Token|WordID\" pairs - no extra text, no missing or extra IDs.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_lemmatization = \"\"\"You are an NLP model specialized in processing French medical text tokens. Lemmatize these token and annotate them with POS tags while preserving original word IDs.\n",
    "Ensure lemmatization and POS tags are context-appropriate and original IDs are maintained. Output format: Token|ID|Lemma|POS. Output only the list of lemmatized and annotated items.\"\"\""
   ],
   "id": "80ffea8292eaef65",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 4: Tokenization API Calls\n",
    "\n",
    "This step sends batches of AOI text to the API for tokenization using the defined\n",
    "`system_prompt_tokenization`. Each batch is constructed from the simplified AOI\n",
    "dictionary (`05_aoi_dictionary_simplified.json`) and processed through the LLM API.\n",
    "In debug mode, the notebook will print example prompts instead of sending requests.\n"
   ],
   "id": "21cdfe3ec3897892"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### API Key Configuration\n",
    "\n",
    "The notebook can run in two modes:\n",
    "\n",
    "- **Live mode:** Requires a valid OpenAI API key to send requests.\n",
    "- **Debug mode:** Skips API calls and uses placeholder responses.\n",
    "\n",
    "To run this notebook with live API calls, you must provide a valid OpenAI API key.\n",
    "The recommended approach is to store the key in a local `.env` file (not under version control).\n",
    "\n",
    "Create a file named `.env` in the project root with the following line:\n",
    "\n",
    "`OPENAI_API_KEY=sk-your_api_key_here`\n",
    "\n",
    "This notebook will automatically read the key from that file.\n",
    "If no `.env` file is found, it will fall back to a plain `API_KEY` file.\n",
    "\n",
    "\n",
    "If no key is found and `DEBUG_MODE` is `True`, the notebook will continue in debug mode automatically."
   ],
   "id": "495cc273cc4132ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T09:55:37.066131Z",
     "start_time": "2025-10-07T09:55:37.061137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DEBUG_MODE = True  # Set to False for live API calls\n",
    "\n",
    "def load_api_key(debug_mode: bool = False):\n",
    "    \"\"\"\n",
    "    Load the OpenAI API key from a .env file, API_KEY file, or environment variable.\n",
    "    If none is found and debug mode is enabled, continue without raising an error.\n",
    "    \"\"\"\n",
    "    key = None\n",
    "    env_path = Path(\".env\")\n",
    "\n",
    "    # Option 1: .env file\n",
    "    if env_path.exists():\n",
    "        with open(env_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\"OPENAI_API_KEY=\"):\n",
    "                    key = line.strip().split(\"=\", 1)[1]\n",
    "                    break\n",
    "\n",
    "    # Option 2: API_KEY file\n",
    "    if not key and Path(\"API_KEY\").exists():\n",
    "        with open(\"API_KEY\", \"r\", encoding=\"utf-8\") as f:\n",
    "            key = f.read().strip()\n",
    "\n",
    "    # Option 3: environment variable\n",
    "    if not key:\n",
    "        key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    # Handle missing key\n",
    "    if not key:\n",
    "        if debug_mode:\n",
    "            print(\"No API key found. Continuing in debug mode (no API calls will be made).\")\n",
    "            return None\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                \"OpenAI API key not found. Please create a .env file or API_KEY file in the project root.\"\n",
    "            )\n",
    "\n",
    "    os.environ[\"OPENAI_API_KEY\"] = key\n",
    "    print(\"OpenAI API key loaded successfully.\")\n",
    "    return key\n",
    "\n",
    "\n",
    "api_key = load_api_key(DEBUG_MODE)"
   ],
   "id": "2087475674dc5c68",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No API key found. Continuing in debug mode (no API calls will be made).\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T09:55:37.089242Z",
     "start_time": "2025-10-07T09:55:37.075555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the simplified AOI dictionary\n",
    "with open(PATHS[\"aoi_dict_simplified\"], \"r\", encoding=\"utf-8\") as f:\n",
    "    sentence_dict = json.load(f)\n",
    "\n",
    "print(f\"Simplified AOI dictionary loaded with {len(sentence_dict)} entries.\")\n",
    "\n",
    "# Create batches for tokenization\n",
    "batches = [\n",
    "    {\"tokens\": tokens, \"sentences\": sentences}\n",
    "    for tokens, sentences in batch_tokens_and_sentences(sentence_dict, batch_size=120)\n",
    "]\n",
    "print(f\"Created {len(batches)} batches for API processing.\")\n",
    "\n",
    "# Initialize API caller (safe for both debug and live)\n",
    "llm_caller = LLMCaller(api_key if api_key else \"\", model_name=\"gpt-4o\")\n",
    "\n",
    "responses = []\n",
    "\n",
    "for i, batch in enumerate(batches):\n",
    "    sentences_text = \" \".join(batch[\"sentences\"])\n",
    "    token_id_list = extract_tokens(batch)\n",
    "    formatted_tokens = format_tokens_for_prompt(token_id_list)\n",
    "\n",
    "    user_prompt = \"Sentences:\\n{sentences}\\n\\nWordIDs:\\n{tokens}\"\n",
    "    user_prompt_formatted = user_prompt.format(tokens=formatted_tokens, sentences=sentences_text)\n",
    "\n",
    "    if DEBUG_MODE or not api_key:\n",
    "        fake_response = simulate_tokenization_response(batch[\"tokens\"])\n",
    "        responses.append(fake_response)\n",
    "        if i == 0:\n",
    "            print(\"Debug Mode: Example synthetic tokenization response:\\n\")\n",
    "            print(fake_response)\n",
    "    else:\n",
    "        response = llm_caller.call_llm_openai(system_prompt_tokenization, user_prompt_formatted)\n",
    "        cleaned_response = remove_triple_backticks_with_newlines(response)\n",
    "        responses.append(cleaned_response)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processed {i} batches\")\n",
    "\n",
    "# Combine batches and responses into a single DataFrame\n",
    "df = pd.DataFrame(batches)\n",
    "df[\"response\"] = responses\n",
    "\n",
    "# Save to TSV without altering the response text\n",
    "df.to_csv(\n",
    "    PATHS[\"responses\"],\n",
    "    sep=\"\\t\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(f\"Tokenization responses saved to {PATHS['responses']} (shape: {df.shape})\")"
   ],
   "id": "92d76cb2a8f52782",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified AOI dictionary loaded with 5 entries.\n",
      "Created 1 batches for API processing.\n",
      "Debug Mode: Example synthetic tokenization response:\n",
      "\n",
      "Cas|0 clinique|1 .|1 Un|2 patient|3 âgé|4 Le|5 patient|6 souffre|7 d|8 '|8 orthopédie|8 fracture|9 .|9\n",
      "Processed 0 batches\n",
      "Tokenization responses saved to data/06_api_responses.csv (shape: (1, 3))\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 5: Lemmatization and POS Tagging API Calls\n",
    "\n",
    "This step processes the tokenized responses produced in Step 4 (`06_api_responses.csv`)\n",
    "and sends them to the API for lemmatization and POS tagging using the\n",
    "`system_prompt_lemmatization`.\n",
    "\n",
    "When `DEBUG_MODE` is enabled or no API key is available, the notebook\n",
    "prints an example prompt and skips actual API requests.\n"
   ],
   "id": "3fc3043f1eaaac17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T09:55:37.113976Z",
     "start_time": "2025-10-07T09:55:37.101837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load tokenization responses\n",
    "df_tokenized = pd.read_csv(PATHS[\"responses\"], sep=\"\\t\", quoting=0, low_memory=False)\n",
    "print(f\"Loaded {PATHS['responses']} with {len(df_tokenized)} entries.\")\n",
    "\n",
    "lemma_responses = []\n",
    "\n",
    "for i, response in enumerate(df_tokenized[\"response\"]):\n",
    "    if not isinstance(response, str) or not response.strip():\n",
    "        lemma_responses.append(None)\n",
    "        continue\n",
    "\n",
    "    cleaned_response = remove_triple_backticks_with_newlines(response)\n",
    "\n",
    "    if DEBUG_MODE or not api_key:\n",
    "        lines = []\n",
    "        for line in cleaned_response.split():  # split on whitespace, not tabs\n",
    "            if \"|\" not in line:\n",
    "                continue\n",
    "            token, token_id = line.split(\"|\", 1)\n",
    "            token = token.strip()\n",
    "            lemma = token.lower()\n",
    "\n",
    "            if token.strip() in [\".\", \",\", \";\", \":\", \"!\", \"?\", \"'\", \"’\"]:\n",
    "                pos = \"PUNCT\"\n",
    "                lemma = token.strip()\n",
    "            elif token.lower() in [\"et\", \"ou\", \"mais\"]:\n",
    "                pos = \"CCONJ\"\n",
    "            elif token.istitle():\n",
    "                pos = \"NOUN\"\n",
    "            else:\n",
    "                pos = random.choice([\"NOUN\", \"VERB\", \"ADJ\", \"DET\"])\n",
    "\n",
    "            lines.append(f\"{token}|{token_id}|{lemma}|{pos}\")\n",
    "\n",
    "        fake_output = \"\\t\".join(lines)\n",
    "        lemma_responses.append(fake_output)\n",
    "\n",
    "        if i == 0:\n",
    "            print(\"Debug Mode: Example synthetic lemma/POS response:\\n\")\n",
    "            print(fake_output)\n",
    "    else:\n",
    "        lemmatization_response = llm_caller.call_llm_openai(\n",
    "            system_prompt_lemmatization, cleaned_response\n",
    "        )\n",
    "        cleaned_output = remove_triple_backticks_with_newlines(lemmatization_response)\n",
    "        lemma_responses.append(cleaned_output)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processed {i} responses\")\n",
    "\n",
    "df_tokenized[\"lemma_response\"] = lemma_responses\n",
    "\n",
    "# Save lemmatization results\n",
    "df_tokenized.to_csv(\n",
    "    PATHS[\"responses_with_lemmas\"],\n",
    "    sep=\"\\t\",\n",
    "    index=False,\n",
    "    quoting=csv.QUOTE_ALL\n",
    ")\n",
    "\n",
    "print(f\"Lemmatization responses saved to {PATHS['responses_with_lemmas']} (shape: {df_tokenized.shape})\")"
   ],
   "id": "9e5fb25b4c9fa9d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data/06_api_responses.csv with 1 entries.\n",
      "Debug Mode: Example synthetic lemma/POS response:\n",
      "\n",
      "Cas|0|cas|NOUN\tclinique|1|clinique|DET\t.|1|.|PUNCT\tUn|2|un|NOUN\tpatient|3|patient|VERB\tâgé|4|âgé|VERB\tLe|5|le|NOUN\tpatient|6|patient|ADJ\tsouffre|7|souffre|DET\td|8|d|DET\t'|8|'|PUNCT\torthopédie|8|orthopédie|VERB\tfracture|9|fracture|VERB\t.|9|.|PUNCT\n",
      "Processed 0 responses\n",
      "Lemmatization responses saved to data/07_api_responses_with_lemmas.csv (shape: (1, 4))\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 6: Parse Lemmatization Responses into Structured Format\n",
    "\n",
    "This step parses the raw API responses from Step 5 (`07_api_responses_with_lemmas.csv`)\n",
    "into a structured table with one row per token and explicit columns:\n",
    "`ID`, `Token`, `Lemma`, and `POS`.\n",
    "\n",
    "The parsed data are saved to `08_api_parsed_responses.csv`, which forms the basis\n",
    "for later alignment with AOI annotations.\n"
   ],
   "id": "2aeb54f30f40549"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T09:55:37.139439Z",
     "start_time": "2025-10-07T09:55:37.126680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load lemmatization responses\n",
    "df_lemmas = pd.read_csv(PATHS[\"responses_with_lemmas\"], sep=\"\\t\", quoting=0, low_memory=False)\n",
    "print(f\"Loaded {PATHS['responses_with_lemmas']} with {len(df_lemmas)} entries.\")\n",
    "\n",
    "def reformat_lemma_response(lemma_response):\n",
    "    \"\"\"\n",
    "    Reformats the lemma response string to switch the ID and word,\n",
    "    such that the format is ID|Word|Lemma|POS.\n",
    "    Handles both tab- and space-separated entries.\n",
    "    \"\"\"\n",
    "    if not isinstance(lemma_response, str):\n",
    "        return lemma_response\n",
    "\n",
    "    parts = re.split(r\"\\s+\", lemma_response.strip())  # split on any whitespace\n",
    "    reformatted_parts = []\n",
    "    for part in parts:\n",
    "        fields = part.split(\"|\")\n",
    "        if len(fields) == 4:\n",
    "            token, id_, lemma, pos = fields\n",
    "            reformatted_parts.append(f\"{id_}|{token}|{lemma}|{pos}\")\n",
    "    return \"\\n\".join(reformatted_parts)\n",
    "\n",
    "\n",
    "def parse_reformatted_response(df):\n",
    "    \"\"\"\n",
    "    Parses the 'reformatted_lemma_response' column of the input DataFrame\n",
    "    into a new DataFrame with columns ID, Token, Lemma, POS.\n",
    "    Works regardless of whether entries are separated by tabs, spaces, or newlines.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for _, row in df.iterrows():\n",
    "        response = row.get(\"reformatted_lemma_response\")\n",
    "        if isinstance(response, str):\n",
    "            # split on any whitespace\n",
    "            lines = re.split(r\"\\s+\", response.strip())\n",
    "            for line in lines:\n",
    "                match = re.match(r\"(\\d+)\\|([^|]+)\\|([^|]+)\\|([^|]+)$\", line)\n",
    "                if match:\n",
    "                    id_, token, lemma, pos = match.groups()\n",
    "                    data.append({\n",
    "                        \"ID\": id_.strip(),\n",
    "                        \"Token\": token.strip(),\n",
    "                        \"Lemma\": lemma.strip(),\n",
    "                        \"POS\": pos.strip()\n",
    "                    })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Reformat and parse\n",
    "df_lemmas[\"reformatted_lemma_response\"] = df_lemmas[\"lemma_response\"].apply(\n",
    "    lambda x: reformat_lemma_response(x) if isinstance(x, str) else x\n",
    ")\n",
    "df_parsed = parse_reformatted_response(df_lemmas)\n",
    "\n",
    "# Save structured output\n",
    "df_parsed.to_csv(PATHS[\"parsed\"], sep=\"\\t\", index=False, quoting=0)\n",
    "print(f\"Parsed responses saved to {PATHS['parsed']} (shape: {df_parsed.shape})\")"
   ],
   "id": "83d8f55cc6ab80b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data/07_api_responses_with_lemmas.csv with 1 entries.\n",
      "Parsed responses saved to data/08_api_parsed_responses.csv (shape: (14, 4))\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T23:30:14.841482Z",
     "start_time": "2025-10-06T23:30:14.839758Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "## Step 7: Align Parsed Tokens Back to AOIs\n",
    "\n",
    "This step aligns the parsed token–lemma–POS table from the API\n",
    "with the AOI entries stored in the simplified AOI dictionary.\n",
    "\n",
    "**Inputs**\n",
    "- `data/08_api_parsed_responses.tsv` — structured token, lemma, and POS output\n",
    "- `data/05_aoi_dictionary_simplified.json` — simplified AOI dictionary\n",
    "\n",
    "**Process**\n",
    "Tokens are sequentially mapped to AOIs in the order they appear in the dictionary.\n",
    "This allows reconstruction of the full AOI-level text with linguistic annotations.\n",
    "\n",
    "**Output**\n",
    "- `data/09_api_materials_parsed.tsv` — token–lemma–POS entries aligned to AOI annotations\n",
    "\n",
    "**Note:**\n",
    "Each AOI remains token-level at this stage.\n",
    "Multi-token AOIs (e.g., `d’orthopédie`) are intentionally kept as multiple rows\n",
    "sharing the same `aoi_id`.\n",
    "The manual and supervisor correction steps (Steps 10–12) later combine these\n",
    "entries into merged AOIs for the final annotated dataset.\n"
   ],
   "id": "78c99a471ec20178"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T09:55:37.161871Z",
     "start_time": "2025-10-07T09:55:37.146483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load parsed token–lemma–POS data\n",
    "df_parsed = pd.read_csv(\n",
    "    PATHS[\"parsed\"],\n",
    "    sep=\"\\t\",\n",
    "    quoting=0,\n",
    "    low_memory=False\n",
    ")\n",
    "print(f\"Loaded parsed responses with {df_parsed.shape[0]} tokens.\")\n",
    "\n",
    "# Load simplified AOI dictionary\n",
    "with open(PATHS[\"aoi_dict_simplified\"], \"r\", encoding=\"utf-8\") as f:\n",
    "    aoi_dict = json.load(f)\n",
    "print(f\"Loaded AOI dictionary with {len(aoi_dict)} entries.\")\n",
    "\n",
    "\n",
    "def align_tokens_to_aois(df, aoi_dict):\n",
    "    \"\"\"\n",
    "    Aligns parsed tokens to AOIs based on ID transitions in the DataFrame.\n",
    "    The AOI dictionary is traversed sequentially, but repeated token IDs\n",
    "    reuse the same AOI entry until the ID changes (mimicking the old logic).\n",
    "    \"\"\"\n",
    "\n",
    "    aligned_rows = []\n",
    "    dict_keys = list(aoi_dict.keys())\n",
    "    key_idx = 0\n",
    "    id_idx = 0\n",
    "    prev_df_id = None\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        token_id = row[\"ID\"]\n",
    "        token = row[\"Token\"]\n",
    "        lemma = row[\"Lemma\"]\n",
    "        pos = row[\"POS\"]\n",
    "\n",
    "        # On new ID → move to next AOI token in dictionary\n",
    "        if prev_df_id is None or token_id != prev_df_id:\n",
    "            if key_idx < len(dict_keys):\n",
    "                ids = aoi_dict[dict_keys[key_idx]][\"ids\"]\n",
    "                if id_idx < len(ids):\n",
    "                    aoi_id, aoi_word = ids[id_idx]\n",
    "                    id_idx += 1\n",
    "                else:\n",
    "                    # move to next AOI entry\n",
    "                    key_idx += 1\n",
    "                    id_idx = 0\n",
    "                    if key_idx < len(dict_keys):\n",
    "                        ids = aoi_dict[dict_keys[key_idx]][\"ids\"]\n",
    "                        if ids:\n",
    "                            aoi_id, aoi_word = ids[id_idx]\n",
    "                            id_idx += 1\n",
    "                        else:\n",
    "                            aoi_id, aoi_word = None, None\n",
    "            else:\n",
    "                aoi_id, aoi_word = None, None\n",
    "        else:\n",
    "            # same ID → reuse previous AOI\n",
    "            aoi_id, aoi_word = aligned_rows[-1][\"aoi_id\"], aligned_rows[-1][\"aoi_text\"]\n",
    "\n",
    "        # Add aligned row\n",
    "        aligned_rows.append({\n",
    "            \"aoi_base\": dict_keys[key_idx] if key_idx < len(dict_keys) else None,\n",
    "            \"aoi_id\": aoi_id,\n",
    "            \"aoi_text\": aoi_word,\n",
    "            \"token_id\": token_id,\n",
    "            \"token\": token,\n",
    "            \"lemma\": lemma,\n",
    "            \"pos\": pos\n",
    "        })\n",
    "\n",
    "        prev_df_id = token_id\n",
    "\n",
    "    return pd.DataFrame(aligned_rows)\n",
    "\n",
    "\n",
    "# Run alignment\n",
    "df_aligned = align_tokens_to_aois(df_parsed, aoi_dict)\n",
    "print(f\"Aligned {df_aligned.shape[0]} tokens to AOI entries.\")\n",
    "\n",
    "# Save aligned data\n",
    "df_aligned.to_csv(\n",
    "    PATHS[\"materials_parsed\"],\n",
    "    sep=\"\\t\",\n",
    "    index=False,\n",
    "    quoting=csv.QUOTE_ALL  # safe for special characters\n",
    ")\n",
    "print(f\"Aligned AOI–token data saved to {PATHS['materials_parsed']} (shape: {df_aligned.shape})\")"
   ],
   "id": "3810d0b27ea26006",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parsed responses with 14 tokens.\n",
      "Loaded AOI dictionary with 5 entries.\n",
      "Aligned 14 tokens to AOI entries.\n",
      "Aligned AOI–token data saved to data/09_api_materials_parsed.csv (shape: (14, 7))\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 7.5: Manual and Supervisor Corrections\n",
    "\n",
    "After the automatic API-based parsing, the file `09_api_materials_parsed.csv`\n",
    "contained a machine-generated mapping of AOIs to their corresponding tokens,\n",
    "lemmas, and POS tags. While structurally aligned, this dataset occasionally\n",
    "contained multiple tokens per AOI or inconsistent lemmatizations.\n",
    "\n",
    "#### Manual Correction Phase\n",
    "A manually reviewed version (`corrected_parsed_materials.csv`) was created by\n",
    "fixing parsing errors and ensuring that each `id.global.aoi` corresponded to\n",
    "a linguistically valid annotation. This guaranteed that every AOI had a clean\n",
    "token–lemma–POS mapping.\n",
    "\n",
    "#### Supervisor-Corrected Integration\n",
    "The project supervisor then integrated this manually corrected data with\n",
    "`04_spacy_annotations.csv`, which contained additional linguistic metrics such as\n",
    "word frequencies and syntactic dependency counts (`left_dependents`,\n",
    "`right_dependents`). This integration produced the consolidated file\n",
    "`ann_materials_with-lemmas_2025-04-22.csv`, combining the following sources:\n",
    "\n",
    "- from **09_api_materials_parsed.csv**: core lexical annotations\n",
    "  (`aoi_text`, `token`, `lemma`, `pos`);\n",
    "- from **04_spacy_annotations.csv**: structural, frequency, and dependency-based\n",
    "  features (`word_count_*`, `word_text_frequency`, `left_dependents`,\n",
    "  `right_dependents`, etc.).\n",
    "\n",
    "This supervisor-corrected dataset serves as the **final authoritative annotation\n",
    "resource**, aligning token-level linguistic information with higher-level textual\n",
    "and syntactic features. It is used as the input for the automatic collapsing and\n",
    "enrichment process described in Step 7.6."
   ],
   "id": "2d6d10208bda6d5b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 7.6: Automatic Collapsing and Enrichment of Parsed Materials\n",
    "\n",
    "This step reproduces the structure of the supervisor-corrected dataset\n",
    "(`ann_materials_with-lemmas_2025-04-22.csv`) by automatically combining\n",
    "the API-based annotations (`09_api_materials_parsed.csv`) with the\n",
    "linguistic and structural information from the spaCy-based file\n",
    "(`04_spacy_annotations.csv`).\n",
    "\n",
    "**Input**\n",
    "- `09_api_materials_parsed.csv`\n",
    "  (automatically parsed AOI-token–lemma–POS mappings)\n",
    "- `04_spacy_annotations.csv`\n",
    "  (linguistic and structural features such as frequency counts and dependency metrics)\n",
    "\n",
    "**Process**\n",
    "The function performs a deterministic AOI-level merge and collapse:\n",
    "- uses `id.global.aoi` as the unique key,\n",
    "- joins the lexical information from the API output (token, lemma, POS)\n",
    "  with the syntactic and frequency information from the spaCy annotations,\n",
    "- ensures that each AOI corresponds to exactly one row with unified\n",
    "  linguistic and structural descriptors,\n",
    "- reconstructs filename and indexing information to mirror the structure\n",
    "  of the supervisor-corrected dataset.\n",
    "\n",
    "This procedure yields a single enriched dataset that can stand in for the\n",
    "unpublished supervisor-corrected file, maintaining the same column schema\n",
    "and ready for downstream merging with the eye-tracking metrics.\n",
    "\n",
    "**Output**\n",
    "- `materials_parsed_collapsed.csv`\n",
    "  (automatically enriched and collapsed AOI-level dataset)\n"
   ],
   "id": "7ff2d3f59be60e08"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T11:41:03.376932Z",
     "start_time": "2025-10-13T11:41:03.336752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 1. Load both datasets\n",
    "# ---------------------------------------------------------------------\n",
    "df09 = pd.read_csv(PATHS[\"materials_parsed\"], sep=\"\\t\", low_memory=False)\n",
    "df05 = pd.read_csv(PATHS[\"lemmas\"], sep=\"\\t\", low_memory=False)\n",
    "\n",
    "print(f\"Loaded 09 data: {df09.shape}, 05 data: {df05.shape}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. Rebuild and normalize AOI identifiers in 09 data\n",
    "# ---------------------------------------------------------------------\n",
    "df09[\"aoi_base\"] = (\n",
    "    df09[\"aoi_base\"]\n",
    "    .astype(str)\n",
    "    .str.replace(r\"[-_]+$\", \"\", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# Canonical AOI ID\n",
    "df09[\"id.global.aoi\"] = df09[\"aoi_base\"].astype(str) + \"-\" + df09[\"aoi_id\"].astype(str)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3. Collapse multiple tokens per AOI (one row per unique AOI)\n",
    "# ---------------------------------------------------------------------\n",
    "def concat_nonempty(values):\n",
    "    \"\"\"Join unique non-empty strings with ' | ', preserving first-occurrence order.\"\"\"\n",
    "    vals = [str(v).strip() for v in values if pd.notna(v) and str(v).strip()]\n",
    "    vals_unique = list(dict.fromkeys(vals))  # order-preserving de-duplication\n",
    "    return \" | \".join(vals_unique) if vals_unique else \"\"\n",
    "\n",
    "collapse_cols = [\"token\", \"token_id\", \"lemma\", \"pos\", \"aoi_text\"]\n",
    "\n",
    "df09_collapsed = (\n",
    "    df09.groupby([\"aoi_base\", \"aoi_id\", \"id.global.aoi\"], as_index=False)\n",
    "        .agg({col: concat_nonempty for col in collapse_cols})\n",
    ")\n",
    "\n",
    "print(f\"Collapsed 09 data: {df09_collapsed.shape}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4. Merge collapsed linguistic info into the 05 (lemmas) dataset\n",
    "# ---------------------------------------------------------------------\n",
    "df_merged = pd.merge(\n",
    "    df05,\n",
    "    df09_collapsed,\n",
    "    on=\"id.global.aoi\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_from09\")\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5. Add unified linguistic fields (merged)\n",
    "# ---------------------------------------------------------------------\n",
    "# AOI text (prefer collapsed 09 version if present)\n",
    "aoi_text_col = \"aoi_text_from09\" if \"aoi_text_from09\" in df_merged.columns else \"aoi_text\"\n",
    "\n",
    "if \"AOI_ann\" in df_merged.columns:\n",
    "    if aoi_text_col in df_merged.columns:\n",
    "        df_merged[\"AOI_ann\"] = df_merged[\"AOI_ann\"].fillna(df_merged[aoi_text_col])\n",
    "    else:\n",
    "        df_merged[\"AOI_ann\"] = df_merged[\"AOI_ann\"].fillna(\"\")\n",
    "else:\n",
    "    df_merged[\"AOI_ann\"] = df_merged[aoi_text_col] if aoi_text_col in df_merged.columns else \"\"\n",
    "\n",
    "\n",
    "# Lemma/POS merged from collapsed 09 (fallback: original 05 columns)\n",
    "df_merged[\"lemma_merged\"] = df_merged.get(\"lemma_from09\", \"\")\n",
    "if \"lemma\" in df_merged.columns:\n",
    "    df_merged[\"lemma_merged\"] = df_merged[\"lemma_merged\"].where(df_merged[\"lemma_merged\"].ne(\"\"), df_merged[\"lemma\"])\n",
    "\n",
    "df_merged[\"pos_merged\"] = df_merged.get(\"pos_from09\", \"\")\n",
    "if \"pos\" in df_merged.columns:\n",
    "    df_merged[\"pos_merged\"] = df_merged[\"pos_merged\"].where(df_merged[\"pos_merged\"].ne(\"\"), df_merged[\"pos\"])\n",
    "\n",
    "# token and token_id merged\n",
    "if \"token_from09\" in df_merged.columns:\n",
    "    df_merged[\"token_merged\"] = df_merged[\"token_from09\"]\n",
    "else:\n",
    "    df_merged[\"token_merged\"] = df_merged.get(\"token\", \"\")\n",
    "\n",
    "if \"token_id_from09\" in df_merged.columns:\n",
    "    df_merged[\"token_id_merged\"] = df_merged[\"token_id_from09\"]\n",
    "else:\n",
    "    df_merged[\"token_id_merged\"] = df_merged.get(\"token_id\", \"\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6. Drop helper columns (post-merge)\n",
    "# ---------------------------------------------------------------------\n",
    "df_merged.drop(columns=[c for c in df_merged.columns if c.endswith(\"_from09\")], inplace=True, errors=\"ignore\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 7. Reorder and complete columns to match supervisor-style schema\n",
    "# ---------------------------------------------------------------------\n",
    "expected_cols = [\n",
    "    \"Media\", \"textid\", \"text.type\", \"text_version\", \"screenid\",\n",
    "    \"Sentence_index\", \"Word_index\", \"word_id_screen\", \"word_id_text\",\n",
    "    \"AOI_ann\", \"AOI_length\", \"is.in.bracket\", \"id.phrase.in.brackets\",\n",
    "    \"AOI.that.in.fact.should.be\", \"tag\", \"tag.type\", \"tag.id\",\n",
    "    \"annotated_text\", \"flag\", \"id.ann\", \"id.ann.global\",\n",
    "    \"Sentence_id_current\", \"Sentence_id_match\", \"id.piece\",\n",
    "    \"lemma_merged\", \"pos_merged\", \"token_merged\", \"token_id_merged\",\n",
    "    \"is.hyphenated\", \"is.compound.hyphen\",\n",
    "    \"word_count_text\", \"word_count_page\", \"total_word_count_text\",\n",
    "    \"left_dependents\", \"right_dependents\", \"id.global.aoi\",\n",
    "    \"AOI_unif_quot\", \"filename.ann\", \"id.line\"\n",
    "]\n",
    "\n",
    "for col in expected_cols:\n",
    "    if col not in df_merged.columns:\n",
    "        df_merged[col] = \"\"\n",
    "\n",
    "df_final = df_merged[expected_cols]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 8. Save final collapsed + enriched dataset\n",
    "# ---------------------------------------------------------------------\n",
    "Path(PATHS[\"final\"]).parent.mkdir(parents=True, exist_ok=True)\n",
    "df_final.to_csv(PATHS[\"final\"], sep=\"\\t\", index=False, quoting=0)\n",
    "\n",
    "print(f\"\\nEnriched materials dataset saved to {PATHS['final']} (shape: {df_final.shape})\")\n",
    "print(\"\\nSample with linguistic features:\")\n",
    "print(df_final[[\n",
    "    \"id.global.aoi\", \"token_merged\", \"token_id_merged\", \"lemma_merged\", \"pos_merged\"\n",
    "]].head(10))"
   ],
   "id": "1ed580f7fc4ee40f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 09 data: (14, 7), 05 data: (10, 38)\n",
      "Collapsed 09 data: (10, 8)\n",
      "\n",
      "Enriched materials dataset saved to data/materials_parsed_collapsed.csv (shape: (10, 39))\n",
      "\n",
      "Sample with linguistic features:\n",
      "                  id.global.aoi        token_merged token_id_merged  \\\n",
      "0   clinical_001_original_1-1-1                 Cas               0   \n",
      "1   clinical_001_original_1-1-2        clinique | .               1   \n",
      "2   clinical_001_original_1-2-1                  Un               2   \n",
      "3   clinical_001_original_1-2-2             patient               3   \n",
      "4   clinical_001_original_2-2-3                 âgé               4   \n",
      "5  medical_002_simplified_1-1-1                  Le               5   \n",
      "6  medical_002_simplified_1-1-2             patient               6   \n",
      "7  medical_002_simplified_1-1-3             souffre               7   \n",
      "8  medical_002_simplified_1-2-1  d | ' | orthopédie               8   \n",
      "9  medical_002_simplified_1-2-2        fracture | .               9   \n",
      "\n",
      "         lemma_merged          pos_merged  \n",
      "0                 cas                NOUN  \n",
      "1        clinique | .         DET | PUNCT  \n",
      "2                  un                NOUN  \n",
      "3             patient                VERB  \n",
      "4                 âgé                VERB  \n",
      "5                  le                NOUN  \n",
      "6             patient                 ADJ  \n",
      "7             souffre                 DET  \n",
      "8  d | ' | orthopédie  DET | PUNCT | VERB  \n",
      "9        fracture | .        VERB | PUNCT  \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"\n",
    "  background-color:#2b3a42;\n",
    "  color:#f1f1f1;\n",
    "  border-left: 6px solid #5da3a3;\n",
    "  padding: 14px;\n",
    "  border-radius: 5px;\n",
    "  line-height: 1.6;\n",
    "\">\n",
    "\n",
    "<h3 style=\"color:#ffffff;\">Step 8: Return to <code>data_prep.ipynb</code></h3>\n",
    "\n",
    "The API annotation and automatic collapsing steps are now complete.\n",
    "You have generated the reproducible, frequency-enriched annotation dataset:\n",
    "\n",
    "<strong><code>materials_parsed_collapsed.csv</code></strong>\n",
    "\n",
    "To continue the preprocessing workflow, switch back to the main data preparation notebook\n",
    "(<code>data_prep.ipynb</code>) and proceed with:\n",
    "\n",
    "<strong>Step&nbsp;7: Merge Enriched Materials with Eye-Tracking Features</strong>\n",
    "\n",
    "This next step combines the API-enriched linguistic data with the experimental\n",
    "eye-tracking dataset (<code>00_input_eye_tracking_data_with_metrics_dummy.csv</code>)\n",
    "to form a unified corpus for subsequent aggregation and model training.\n",
    "\n",
    "</div>"
   ],
   "id": "dce328ea8520c23b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "59131cf0e3e7296f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

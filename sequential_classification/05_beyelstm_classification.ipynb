{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 0: Experiment Directory Structure and Naming Convention\n",
    "\n",
    "**Overview**\n",
    "Each experiment variant is organized in its own subdirectory within `sequential_classification/`.\n",
    "This ensures that all inputs, models, and logs remain self-contained and reproducible.\n",
    "\n",
    "Two main experiment pipelines are used:\n",
    "\n",
    "- `experiment_manual_selection/`: Uses manually selected gaze and linguistic features informed by prior literature.\n",
    "- `experiment_feature_analysis/`: Uses an extended feature set derived for exploratory and ablation studies.\n",
    "\n",
    "**Directory Layout**\n",
    "````text\n",
    "sequential_classification/\n",
    "├── 05_BEyeLSTM_variation.ipynb # This notebook implementing the hybrid BiLSTM model\n",
    "├── experiment_manual_selection/\n",
    "│   ├── inputs/     # Preprocessed feature arrays and label files\n",
    "│   ├── outputs/    # Trained models, metrics, and result tables\n",
    "│   └── logs/       # Training logs and configuration metadata\n",
    "└── experiment_feature_analysis/\n",
    "    ├── inputs/\n",
    "    ├── outputs/\n",
    "    └── logs/\n",
    "````\n",
    "\n",
    "**Contents of Each `inputs/` Folder**\n",
    "- `X_fix.npy` – Fixation-level feature tensor (duration, saccades, regressions).\n",
    "- `X_pos.npy` – Encoded part-of-speech sequences.\n",
    "- `X_con.npy` – Encoded content-word indices.\n",
    "- `y_labels.npy` – Binary class labels (1 = expert, 0 = non-expert).\n",
    "- `participant_ids.npy` – Participant identifiers matching input sequences.\n",
    "- `screen_ids.npy` – Screen or trial identifiers for grouped cross-validation.\n",
    "\n",
    "**Usage**\n",
    "To select an experiment configuration, specify the folder name in the configuration cell:"
   ],
   "id": "540ce4fa856c4c63"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T14:21:53.293973Z",
     "start_time": "2025-10-25T14:21:51.555771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Step 0: Imports, Experiment Configuration and Global Paths ---\n",
    "\n",
    "# --- Standard Library ---\n",
    "import ast\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "# --- Third-Party Libraries ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import (\n",
    "    Bidirectional,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Embedding,\n",
    "    LSTM,\n",
    "    concatenate,\n",
    ")\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Select active experiment variant\n",
    "# -------------------------------------------------------------------------\n",
    "EXPERIMENT = \"experiment_manual_selection\"  # \"experiment_feature_analysis\" or \"experiment_manual_selection\"\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Base directory resolution\n",
    "# -------------------------------------------------------------------------\n",
    "# If run from inside sequential_classification/, go one level up for project root\n",
    "CWD = os.getcwd()\n",
    "if os.path.basename(CWD) == \"sequential_classification\":\n",
    "    PROJECT_ROOT = os.path.dirname(CWD)\n",
    "else:\n",
    "    PROJECT_ROOT = CWD\n",
    "\n",
    "# Experiment directory lives inside sequential_classification/\n",
    "EXPERIMENT_DIR = os.path.join(PROJECT_ROOT, \"sequential_classification\", EXPERIMENT)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Define core folders\n",
    "# -------------------------------------------------------------------------\n",
    "PATHS = {\n",
    "    \"project_root\": PROJECT_ROOT,\n",
    "    \"experiment\": EXPERIMENT_DIR,\n",
    "    \"inputs\": os.path.join(EXPERIMENT_DIR, \"inputs\"),\n",
    "    \"outputs\": os.path.join(EXPERIMENT_DIR, \"outputs\"),\n",
    "    \"logs\": os.path.join(EXPERIMENT_DIR, \"logs\"),\n",
    "}\n",
    "\n",
    "# Ensure required folders exist\n",
    "for key in [\"inputs\", \"outputs\", \"logs\"]:\n",
    "    os.makedirs(PATHS[key], exist_ok=True)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Define key file paths used across the pipeline\n",
    "# -------------------------------------------------------------------------\n",
    "PATHS.update({\n",
    "    # Raw merged data (project_root/data/raw/)\n",
    "    \"data\": os.path.join(PROJECT_ROOT, \"data\", \"raw\", \"et_data_merged_with_ann_materials_dummy.csv\"),\n",
    "\n",
    "    # Label-related files\n",
    "    \"labels\": os.path.join(PATHS[\"inputs\"], \"labels.csv\"),\n",
    "    \"participant_ids\": os.path.join(PATHS[\"inputs\"], \"participant_ids.npy\"),\n",
    "    \"screen_ids\": os.path.join(PATHS[\"inputs\"], \"screen_ids.npy\"),\n",
    "    \"y_labels\": os.path.join(PATHS[\"inputs\"], \"y_labels.npy\"),\n",
    "\n",
    "    # Feature arrays (to be generated later)\n",
    "    \"X_fix\": os.path.join(PATHS[\"inputs\"], \"X_fix.npy\"),\n",
    "    \"X_pos\": os.path.join(PATHS[\"inputs\"], \"X_pos.npy\"),\n",
    "    \"X_con\": os.path.join(PATHS[\"inputs\"], \"X_con.npy\"),\n",
    "\n",
    "    # Model artifacts\n",
    "    \"results_csv\": os.path.join(PATHS[\"outputs\"], \"results.csv\"),\n",
    "    \"final_model\": os.path.join(PATHS[\"outputs\"], \"final_model.keras\"),\n",
    "})\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Summary\n",
    "# -------------------------------------------------------------------------\n",
    "print(f\"Active experiment: {EXPERIMENT}\")\n",
    "for name, path in PATHS.items():\n",
    "    print(f\"{name:>18}: {path}\")"
   ],
   "id": "9fc8b51b84a8ffad",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 16:21:51.707777: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-25 16:21:51.709107: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-25 16:21:51.736454: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-25 16:21:51.737086: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-25 16:21:52.425566: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active experiment: experiment_manual_selection\n",
      "      project_root: /mnt/c/Users/Consti/PycharmProjects/BachelorCode\n",
      "        experiment: /mnt/c/Users/Consti/PycharmProjects/BachelorCode/sequential_classification/experiment_manual_selection\n",
      "            inputs: /mnt/c/Users/Consti/PycharmProjects/BachelorCode/sequential_classification/experiment_manual_selection/inputs\n",
      "           outputs: /mnt/c/Users/Consti/PycharmProjects/BachelorCode/sequential_classification/experiment_manual_selection/outputs\n",
      "              logs: /mnt/c/Users/Consti/PycharmProjects/BachelorCode/sequential_classification/experiment_manual_selection/logs\n",
      "              data: /mnt/c/Users/Consti/PycharmProjects/BachelorCode/data/raw/et_data_merged_with_ann_materials_dummy.csv\n",
      "            labels: /mnt/c/Users/Consti/PycharmProjects/BachelorCode/sequential_classification/experiment_manual_selection/inputs/labels.csv\n",
      "   participant_ids: /mnt/c/Users/Consti/PycharmProjects/BachelorCode/sequential_classification/experiment_manual_selection/inputs/participant_ids.npy\n",
      "        screen_ids: /mnt/c/Users/Consti/PycharmProjects/BachelorCode/sequential_classification/experiment_manual_selection/inputs/screen_ids.npy\n",
      "          y_labels: /mnt/c/Users/Consti/PycharmProjects/BachelorCode/sequential_classification/experiment_manual_selection/inputs/y_labels.npy\n",
      "             X_fix: /mnt/c/Users/Consti/PycharmProjects/BachelorCode/sequential_classification/experiment_manual_selection/inputs/X_fix.npy\n",
      "             X_pos: /mnt/c/Users/Consti/PycharmProjects/BachelorCode/sequential_classification/experiment_manual_selection/inputs/X_pos.npy\n",
      "             X_con: /mnt/c/Users/Consti/PycharmProjects/BachelorCode/sequential_classification/experiment_manual_selection/inputs/X_con.npy\n",
      "       results_csv: /mnt/c/Users/Consti/PycharmProjects/BachelorCode/sequential_classification/experiment_manual_selection/outputs/results.csv\n",
      "       final_model: /mnt/c/Users/Consti/PycharmProjects/BachelorCode/sequential_classification/experiment_manual_selection/outputs/final_model.keras\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 0.5: Enrich Dataset with Linguistic and Structural Columns\n",
    "\n",
    "**Overview**\n",
    "\n",
    "This step enriches the merged eye-tracking dataset with the linguistic and categorical\n",
    "columns required for the BEyeLSTM manual-feature pipeline.\n",
    "It aligns the raw fixation data with linguistic annotations and prepares the feature space\n",
    "used in subsequent preprocessing and modeling steps.\n",
    "\n",
    "**Process**\n",
    "1. Adds simplified part-of-speech (PoS) tags and a PoS list per AOI.\n",
    "2. Identifies content-word and medical-term occurrences.\n",
    "3. Assigns AOI composition categories (`Content-Only`, `Function-Only`, `Mixed`).\n",
    "4. Computes prioritized lemma frequencies (`N` > `V` > `A`).\n",
    "5. Ensures all columns required for subsequent feature engineering and model preparation exist.\n",
    "\n",
    "**Input**\n",
    "- Merged eye-tracking and annotation dataset (`PATHS[\"data\"]`).\n",
    "\n",
    "**Output**\n",
    "- Enriched dataset with linguistic and structural columns, stored under the current experiment’s `inputs/` directory.\n"
   ],
   "id": "e983955846f696b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T15:41:20.563161Z",
     "start_time": "2025-10-20T15:41:20.528831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Step 0.5: Enrich dummy dataset with unified linguistic columns ---\n",
    "\n",
    "dummy_path = PATHS[\"data\"]\n",
    "\n",
    "if not os.path.exists(dummy_path):\n",
    "    raise FileNotFoundError(f\"Dummy dataset not found at {dummy_path}\")\n",
    "\n",
    "print(f\"Loading dummy dataset from {dummy_path}...\")\n",
    "df = pd.read_csv(dummy_path, sep=\"\\t\", low_memory=False)\n",
    "print(f\"Initial shape: {df.shape}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1. POS mappings\n",
    "# -------------------------------------------------------------------------\n",
    "simplified_pos_mapping = {\n",
    "    'PROPN': 'N', 'NOUN': 'N', 'VERB': 'V', 'ADV': 'A', 'ADJ': 'A',\n",
    "    'PUNCT': 'FUNC', 'PRON': 'FUNC', 'SCONJ': 'FUNC', 'NUM': 'FUNC',\n",
    "    'DET': 'FUNC', 'CCONJ': 'FUNC', 'ADP': 'FUNC', 'AUX': 'FUNC',\n",
    "    'INTJ': 'FUNC', 'X': 'FUNC', 'PART': 'FUNC'\n",
    "}\n",
    "content_word_mapping = {\n",
    "    'PROPN': True, 'NOUN': True, 'VERB': True, 'ADV': True, 'ADJ': True,\n",
    "    'PUNCT': False, 'PRON': False, 'SCONJ': False, 'NUM': False,\n",
    "    'DET': False, 'CCONJ': False, 'ADP': False, 'AUX': False,\n",
    "    'INTJ': False, 'X': False, 'PART': False\n",
    "}\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2. Derived columns: simplified_pos, is_content_word, simplified_pos_list\n",
    "# -------------------------------------------------------------------------\n",
    "def aggregate_pos_features(raw_pos_string: str):\n",
    "    if not isinstance(raw_pos_string, str):\n",
    "        return pd.Series({'simplified_pos': 'UNK', 'is_content_word': False})\n",
    "    tags = [t.strip() for t in raw_pos_string.replace('+', '|').split('|') if t.strip()]\n",
    "    if not tags:\n",
    "        return pd.Series({'simplified_pos': 'UNK', 'is_content_word': False})\n",
    "    simplified_tags = [simplified_pos_mapping.get(t, 'UNK') for t in tags]\n",
    "    is_content = any(content_word_mapping.get(t, False) for t in tags)\n",
    "    tags_to_consider = [t for t in simplified_tags if t != 'FUNC'] or simplified_tags\n",
    "    counts = Counter(tags_to_consider)\n",
    "    final_tag = counts.most_common(1)[0][0] if counts else 'UNK'\n",
    "    return pd.Series({'simplified_pos': final_tag, 'is_content_word': is_content})\n",
    "\n",
    "def create_simplified_pos_list(pos_string: str):\n",
    "    if not isinstance(pos_string, str):\n",
    "        return []\n",
    "    original_tags = [tag.strip() for tag in pos_string.replace('+', '|').split('|') if tag.strip()]\n",
    "    if not original_tags:\n",
    "        return []\n",
    "    simplified_tags = [simplified_pos_mapping.get(tag, 'UNK') for tag in original_tags]\n",
    "    return simplified_tags\n",
    "\n",
    "if 'pos_merged' in df.columns:\n",
    "    print(\"Deriving simplified POS features and lists...\")\n",
    "    enriched = df['pos_merged'].apply(aggregate_pos_features)\n",
    "    df = pd.concat([df, enriched], axis=1)\n",
    "    df['simplified_pos_list'] = df['pos_merged'].apply(create_simplified_pos_list)\n",
    "else:\n",
    "    df['simplified_pos'] = 'UNK'\n",
    "    df['is_content_word'] = False\n",
    "    df['simplified_pos_list'] = [[] for _ in range(len(df))]\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3. AOI composition and medical term flag\n",
    "# -------------------------------------------------------------------------\n",
    "def get_aoi_composition(raw_pos_string: str) -> str:\n",
    "    if not isinstance(raw_pos_string, str):\n",
    "        return 'Unknown'\n",
    "    tags = [t.strip() for t in raw_pos_string.replace('+', '|').split('|') if t.strip()]\n",
    "    if not tags:\n",
    "        return 'Unknown'\n",
    "    flags = [content_word_mapping.get(t, False) for t in tags]\n",
    "    if all(flags):\n",
    "        return 'Content-Only'\n",
    "    elif not any(flags):\n",
    "        return 'Function-Only'\n",
    "    return 'Mixed'\n",
    "\n",
    "df['aoi_composition'] = df.get('pos_merged', '').apply(get_aoi_composition)\n",
    "if 'tag.type' in df.columns:\n",
    "    df['is_medical_term'] = df['tag.type'].isin([1, 2, 3, 4, 5, 7])\n",
    "else:\n",
    "    df['is_medical_term'] = False\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4. Prioritized lemma frequency\n",
    "# -------------------------------------------------------------------------\n",
    "POS_TAGS_COLUMN = 'simplified_pos_list'\n",
    "LEMMA_FREQUENCIES_COLUMN = 'lemma_frequency_text'\n",
    "NOUN_TAGS, VERB_TAGS, ADJECTIVE_TAGS = ['N'], ['V'], ['A']\n",
    "\n",
    "def safe_literal_eval(val):\n",
    "    try:\n",
    "        if not isinstance(val, str):\n",
    "            return []\n",
    "        return ast.literal_eval(val)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "\n",
    "def get_prioritized_frequency(row):\n",
    "    try:\n",
    "        pos_tags = row[POS_TAGS_COLUMN]\n",
    "        freq_strings = row[LEMMA_FREQUENCIES_COLUMN]\n",
    "        if not isinstance(pos_tags, list) or not isinstance(freq_strings, list) or len(pos_tags) != len(freq_strings):\n",
    "            return 0\n",
    "        tagged_freqs = []\n",
    "        for tag, freq_str in zip(pos_tags, freq_strings):\n",
    "            try:\n",
    "                tagged_freqs.append((tag, float(freq_str)))\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "        noun_freqs = [f for t, f in tagged_freqs if t in NOUN_TAGS]\n",
    "        verb_freqs = [f for t, f in tagged_freqs if t in VERB_TAGS]\n",
    "        adj_freqs  = [f for t, f in tagged_freqs if t in ADJECTIVE_TAGS]\n",
    "        if noun_freqs: return min(noun_freqs)\n",
    "        if verb_freqs: return min(verb_freqs)\n",
    "        if adj_freqs:  return min(adj_freqs)\n",
    "        return 0\n",
    "    except (TypeError, AttributeError):\n",
    "        return 0\n",
    "\n",
    "if LEMMA_FREQUENCIES_COLUMN in df.columns:\n",
    "    print(\"Computing prioritized lemma frequencies...\")\n",
    "    df[LEMMA_FREQUENCIES_COLUMN] = df[LEMMA_FREQUENCIES_COLUMN].apply(safe_literal_eval)\n",
    "    df['simplified_pos_list'] = df['simplified_pos_list'].apply(lambda pos_list: [tag for tag in pos_list if tag != 'UNK'])\n",
    "    df['prioritized_lemma_frequency'] = df.apply(get_prioritized_frequency, axis=1)\n",
    "else:\n",
    "    print(\"No lemma frequency column found; initializing prioritized_lemma_frequency = 0.0\")\n",
    "    df['prioritized_lemma_frequency'] = 0.0\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5. Save the enriched dummy dataset\n",
    "# -------------------------------------------------------------------------\n",
    "expected_cols = [\n",
    "    'simplified_pos', 'simplified_pos_list', 'is_content_word',\n",
    "    'is_medical_term', 'aoi_composition', 'prioritized_lemma_frequency'\n",
    "]\n",
    "for col in expected_cols:\n",
    "    if col not in df.columns:\n",
    "        df[col] = 0 if 'is_' not in col else False\n",
    "\n",
    "df.to_csv(dummy_path, sep=\"\\t\", index=False)\n",
    "print(f\"Enriched dummy dataset saved to: {dummy_path}\")\n",
    "print(\"\\nPreview of added/verified columns:\")\n",
    "print(df[expected_cols].head())"
   ],
   "id": "981c2310b80d1062",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dummy dataset from /mnt/c/Users/Consti/PycharmProjects/BachelorCode/data/raw/et_data_merged_with_ann_materials_dummy.csv...\n",
      "Initial shape: (30, 110)\n",
      "Deriving simplified POS features and lists...\n",
      "Computing prioritized lemma frequencies...\n",
      "Enriched dummy dataset saved to: /mnt/c/Users/Consti/PycharmProjects/BachelorCode/data/raw/et_data_merged_with_ann_materials_dummy.csv\n",
      "\n",
      "Preview of added/verified columns:\n",
      "  simplified_pos simplified_pos simplified_pos_list  is_content_word  \\\n",
      "0              N              N                 [N]             True   \n",
      "1              N              N           [N, FUNC]             True   \n",
      "2              N              N                 [N]             True   \n",
      "3           FUNC           FUNC              [FUNC]            False   \n",
      "4              N              N                 [N]             True   \n",
      "\n",
      "   is_content_word  is_medical_term aoi_composition  \\\n",
      "0             True             True    Content-Only   \n",
      "1             True            False           Mixed   \n",
      "2             True            False    Content-Only   \n",
      "3            False            False   Function-Only   \n",
      "4             True            False    Content-Only   \n",
      "\n",
      "   prioritized_lemma_frequency  \n",
      "0                     0.166667  \n",
      "1                     0.166667  \n",
      "2                     0.166667  \n",
      "3                     0.000000  \n",
      "4                     0.166667  \n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 1: Create Participant-Level Labels (`labels.csv`)\n",
    "\n",
    "**Overview**\n",
    "\n",
    "This step extracts and binarizes participant expertise information from the merged\n",
    "eye-tracking and annotation dataset.\n",
    "It produces a compact label table containing one entry per participant, which is\n",
    "used to align participant-level expertise labels with session- or screen-level\n",
    "data in subsequent stages of the pipeline.\n",
    "\n",
    "**Input**\n",
    "- Enriched eye-tracking and annotation dataset (`PATHS[\"data\"]`).\n",
    "\n",
    "**Output**\n",
    "- Participant-level label file (`labels.csv`), stored in the current experiment’s `inputs/` directory.\n"
   ],
   "id": "93600f2757a05ff4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T15:41:20.816252Z",
     "start_time": "2025-10-20T15:41:20.801222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Step 1: Create participant-level label file ---\n",
    "\n",
    "print(\"Loading merged participant annotation data...\")\n",
    "main_df = pd.read_csv(PATHS[\"data\"], sep=\"\\t\", low_memory=False)\n",
    "\n",
    "# Select and binarize participant-level labels\n",
    "labels_df = main_df[[\"Participant_unique\", \"is.expert\"]].copy()\n",
    "labels_df[\"is.expert\"] = labels_df[\"is.expert\"].map({\"expert\": 1, \"non-expert\": 0})\n",
    "\n",
    "# Standardize column names\n",
    "labels_df.rename(columns={\n",
    "    \"Participant_unique\": \"participant_id\",\n",
    "    \"is.expert\": \"class_label\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Remove duplicates and reset index\n",
    "labels_df.drop_duplicates(inplace=True)\n",
    "labels_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save participant-level labels\n",
    "labels_df.to_csv(PATHS[\"labels\"], sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"Saved participant labels to: {PATHS['labels']}\")\n",
    "print(f\"Total unique participants: {len(labels_df)}\")\n",
    "print(labels_df.head())"
   ],
   "id": "9a0f08e428a4dd49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading merged participant annotation data...\n",
      "Saved participant labels to: /mnt/c/Users/Consti/PycharmProjects/BachelorCode/sequential_classification/experiment_manual_selection/inputs/labels.csv\n",
      "Total unique participants: 3\n",
      "      participant_id  class_label\n",
      "0  Participant10-1_A            0\n",
      "1  Participant11-1_B            1\n",
      "2  Participant12-2_A            0\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2: Prepare Sequential Fixation Features (`X_fix.npy`, `participant_ids.npy`, `screen_ids.npy`)\n",
    "\n",
    "**Overview**\n",
    "\n",
    "This step transforms fixation-level behavioral data into fixed-length sequences suitable\n",
    "for the Hybrid BiLSTM architecture.\n",
    "It groups fixations by participant and screen, extracts relevant gaze and linguistic\n",
    "metrics, and standardizes sequence length through padding to ensure consistent input\n",
    "dimensions across all samples.\n",
    "\n",
    "**Input**\n",
    "- Merged fixation-level dataset containing per-word gaze features (`PATHS[\"data\"]`).\n",
    "- Experiment configuration from Step 0 (defines output directories and file naming).\n",
    "\n",
    "**Process**\n",
    "1. Selects and prepares the relevant fixation and gaze-related feature columns.\n",
    "2. Encodes categorical and boolean features as numerical arrays.\n",
    "3. Groups data by participant × screen and pads all sequences to a fixed length.\n",
    "4. Saves the resulting NumPy arrays for model input.\n",
    "\n",
    "**Output**\n",
    "- `PATHS[\"inputs\"]/X_fix.npy`\n",
    "- `PATHS[\"inputs\"]/participant_ids.npy`\n",
    "- `PATHS[\"inputs\"]/screen_ids.npy`\n"
   ],
   "id": "c3b0ed263e623ce0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T15:41:21.108023Z",
     "start_time": "2025-10-20T15:41:21.077199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Step 2: Prepare sequential fixation data arrays ---\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Utility: Sequence preparation\n",
    "# -------------------------------------------------------------------------\n",
    "def prepare_sequential_data(\n",
    "    df: pd.DataFrame,\n",
    "    feature_cols: List[str],\n",
    "    participant_col: str,\n",
    "    screen_col: str,\n",
    "    sequence_length: int = 128,\n",
    "    sort_col: Optional[str] = None\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "\n",
    "    if sort_col:\n",
    "        print(f\"Sorting by '{sort_col}'...\")\n",
    "        df = df.sort_values(by=sort_col).reset_index(drop=True)\n",
    "\n",
    "    print(\"Grouping by participant × screen...\")\n",
    "    X_list, p_list, s_list = [], [], []\n",
    "\n",
    "    for (pid, sid), group in tqdm(df.groupby([participant_col, screen_col]), desc=\"Screens\"):\n",
    "        arr = group[feature_cols].to_numpy(dtype=np.float32)\n",
    "        X_list.append(arr)\n",
    "        p_list.append(pid)\n",
    "        s_list.append(sid)\n",
    "\n",
    "    print(f\"Padding {len(X_list)} sequences to length {sequence_length}...\")\n",
    "    X_padded = pad_sequences(\n",
    "        X_list, maxlen=sequence_length, padding=\"post\", truncating=\"post\", dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    return X_padded, np.array(p_list), np.array(s_list)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "RAW_FILE = PATHS[\"data\"]\n",
    "PARTICIPANT_COLUMN = \"Participant_unique\"\n",
    "SCREEN_COLUMN = \"screenid\"\n",
    "SORT_COLUMN = \"index\"\n",
    "SEQUENCE_LENGTH = 128\n",
    "\n",
    "# Define feature sets\n",
    "FEATURE_COLUMNS_MANUAL = [\n",
    "    \"Total_duration_of_fixations\", \"First-pass_duration\", \"First-pass_regression\",\n",
    "    \"text_version\", \"text.type\", \"is_medical_term\", \"simplified_pos\",\n",
    "    \"is_content_word\", \"prioritized_lemma_frequency\"\n",
    "]\n",
    "\n",
    "FEATURE_COLUMNS_FEAT_ANALYSIS = [\n",
    "    \"Average_pupil_diameter\", \"word_frequency_screen\", \"Peak_velocity_of_entry_saccade\",\n",
    "    \"Peak_velocity_of_exit_saccade\", \"Total_duration_of_Visit\", \"Time_to_exit_saccade\",\n",
    "    \"word_count_text\", \"AOI_length\", \"Duration_of_first_Visit\", \"Regression-path_duration\",\n",
    "    \"tag.id\", \"First-pass_first_fixation_duration\", \"Maximum_duration_of_fixations\",\n",
    "    \"Duration_of_first_whole_fixation\"\n",
    "]\n",
    "\n",
    "BOOLEAN_COLS = [\"is_medical_term\", \"is_content_word\"]\n",
    "CATEGORICAL_COLS = [\"text_version\", \"text.type\", \"simplified_pos\"]\n",
    "\n",
    "# Determine mode automatically from global EXPERIMENT variable\n",
    "if \"manual\" in EXPERIMENT.lower():\n",
    "    feature_mode = \"manual\"\n",
    "    FEATURE_COLUMNS = FEATURE_COLUMNS_MANUAL\n",
    "elif \"feature\" in EXPERIMENT.lower():\n",
    "    feature_mode = \"feature_analysis\"\n",
    "    FEATURE_COLUMNS = FEATURE_COLUMNS_FEAT_ANALYSIS\n",
    "else:\n",
    "    sys.exit(f\"Error: Could not infer feature mode from EXPERIMENT='{EXPERIMENT}'.\")\n",
    "\n",
    "print(f\"\\n--- Preparing sequential data in '{feature_mode}' mode ---\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Load and validate data\n",
    "# -------------------------------------------------------------------------\n",
    "print(f\"Loading raw data from {RAW_FILE} ...\")\n",
    "try:\n",
    "    df = pd.read_csv(RAW_FILE, sep=\"\\t\", low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    sys.exit(f\"Error: File not found at {RAW_FILE}\")\n",
    "\n",
    "missing = [c for c in [PARTICIPANT_COLUMN, SCREEN_COLUMN] + FEATURE_COLUMNS if c not in df.columns]\n",
    "if missing:\n",
    "    sys.exit(f\"Error: Missing required columns: {missing}\")\n",
    "print(\"All required columns present.\")\n",
    "\n",
    "# Create an artificial index column if not present\n",
    "if 'index' not in df.columns:\n",
    "    df['index'] = range(len(df))\n",
    "# This step assumes the raw data is already sorted by participant, screen, and word order and should be removed / altered accordingly.\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Encoding and feature engineering\n",
    "# -------------------------------------------------------------------------\n",
    "if feature_mode == \"manual\":\n",
    "    print(\"Encoding boolean and categorical features...\")\n",
    "    df[BOOLEAN_COLS] = df[BOOLEAN_COLS].astype(int)\n",
    "    before = set(df.columns)\n",
    "    df = pd.get_dummies(df, columns=CATEGORICAL_COLS, dtype=int)\n",
    "    new_cols = list(set(df.columns) - before)\n",
    "\n",
    "    numerical_cols = [c for c in FEATURE_COLUMNS_MANUAL if c not in BOOLEAN_COLS + CATEGORICAL_COLS]\n",
    "    final_features = numerical_cols + BOOLEAN_COLS + new_cols\n",
    "else:\n",
    "    final_features = FEATURE_COLUMNS_FEAT_ANALYSIS\n",
    "\n",
    "print(f\"Total features used: {len(final_features)}\")\n",
    "df[final_features] = df[final_features].fillna(0)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Run sequence preparation\n",
    "# -------------------------------------------------------------------------\n",
    "print(\"\\nGenerating padded fixation sequences...\")\n",
    "X_fix, participant_ids, screen_ids = prepare_sequential_data(\n",
    "    df=df,\n",
    "    feature_cols=final_features,\n",
    "    participant_col=PARTICIPANT_COLUMN,\n",
    "    screen_col=SCREEN_COLUMN,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    sort_col=SORT_COLUMN\n",
    ")\n",
    "\n",
    "print(\"\\n--- Verification ---\")\n",
    "print(f\"X_fix shape: {X_fix.shape}\")\n",
    "print(f\"Participants: {len(np.unique(participant_ids))}, Screens: {len(screen_ids)}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Save arrays\n",
    "# -------------------------------------------------------------------------\n",
    "np.save(PATHS[\"X_fix\"], X_fix)\n",
    "np.save(PATHS[\"participant_ids\"], participant_ids)\n",
    "np.save(PATHS[\"screen_ids\"], screen_ids)\n",
    "print(f\"\\nSaved prepared arrays to: {PATHS['inputs']}\")\n"
   ],
   "id": "2f4653f5d36b1da5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing sequential data in 'manual' mode ---\n",
      "Loading raw data from /mnt/c/Users/Consti/PycharmProjects/BachelorCode/data/raw/et_data_merged_with_ann_materials_dummy.csv ...\n",
      "All required columns present.\n",
      "Encoding boolean and categorical features...\n",
      "Total features used: 13\n",
      "\n",
      "Generating padded fixation sequences...\n",
      "Sorting by 'index'...\n",
      "Grouping by participant × screen...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Screens: 100%|██████████| 6/6 [00:00<00:00, 2223.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding 6 sequences to length 128...\n",
      "\n",
      "--- Verification ---\n",
      "X_fix shape: (6, 128, 13)\n",
      "Participants: 3, Screens: 6\n",
      "\n",
      "Saved prepared arrays to: /mnt/c/Users/Consti/PycharmProjects/BachelorCode/sequential_classification/experiment_manual_selection/inputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 3: Create Session-Aligned Ground Truth Labels (`y_labels.npy`)\n",
    "\n",
    "**Overview**\n",
    "\n",
    "This step aligns participant-level expertise annotations with the session-level fixation\n",
    "feature arrays.\n",
    "It merges the participant expertise information from `labels.csv` with the unique\n",
    "`(participant_id, screen_id)` pairs extracted from the prepared fixation sequences,\n",
    "ensuring exact alignment between behavioral features and target labels.\n",
    "\n",
    "**Input**\n",
    "- Participant-level label file (`PATHS[\"inputs\"]/labels.csv`)\n",
    "- Participant identifiers (`PATHS[\"inputs\"]/participant_ids.npy`)\n",
    "- Screen identifiers (`PATHS[\"inputs\"]/screen_ids.npy`)\n",
    "\n",
    "**Process**\n",
    "1. Load participant-level expertise labels.\n",
    "2. Load the participant and screen identifiers corresponding to each fixation sequence.\n",
    "3. Create one entry per unique session (`participant_id × screen_id`).\n",
    "4. Merge the data sources to produce a label array aligned with the model inputs.\n",
    "\n",
    "**Output**\n",
    "- Session-aligned label array (`PATHS[\"inputs\"]/y_labels.npy`)"
   ],
   "id": "a5ab16d3d1cd835f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T15:41:21.383379Z",
     "start_time": "2025-10-20T15:41:21.360656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Step 3: Create session-aligned y_labels.npy ---\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1. Load participant-level labels\n",
    "# -------------------------------------------------------------------------\n",
    "if not os.path.exists(PATHS[\"labels\"]):\n",
    "    raise FileNotFoundError(f\"labels.csv not found at {PATHS['labels']}. Run Step 1 first.\")\n",
    "\n",
    "labels_df = pd.read_csv(PATHS[\"labels\"], sep=\"\\t\", low_memory=False)\n",
    "print(f\"Loaded labels.csv with {len(labels_df)} participants.\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2. Load participant and screen IDs\n",
    "# -------------------------------------------------------------------------\n",
    "if not (os.path.exists(PATHS[\"participant_ids\"]) and os.path.exists(PATHS[\"screen_ids\"])):\n",
    "    raise FileNotFoundError(\n",
    "        \"participant_ids.npy or screen_ids.npy not found in inputs/. \"\n",
    "        \"Run Step 2 (sequence preparation) before this step.\"\n",
    "    )\n",
    "\n",
    "participant_ids = np.load(PATHS[\"participant_ids\"])\n",
    "screen_ids = np.load(PATHS[\"screen_ids\"])\n",
    "\n",
    "print(f\"Loaded {len(participant_ids)} participant IDs and {len(screen_ids)} screen IDs.\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3. Build session DataFrame and merge with labels\n",
    "# -------------------------------------------------------------------------\n",
    "events_df = pd.DataFrame({\n",
    "    \"participant_id\": participant_ids,\n",
    "    \"screen_id\": screen_ids\n",
    "})\n",
    "\n",
    "# Drop duplicate sessions (participant × screen)\n",
    "sessions_df = (\n",
    "    events_df\n",
    "    .drop_duplicates(subset=[\"participant_id\", \"screen_id\"])\n",
    "    .sort_values(by=[\"participant_id\", \"screen_id\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"Identified {len(sessions_df)} unique participant-screen sessions.\")\n",
    "\n",
    "# Merge session list with expertise labels\n",
    "merged_df = pd.merge(\n",
    "    sessions_df,\n",
    "    labels_df,\n",
    "    on=\"participant_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4. Extract and verify y_labels array\n",
    "# -------------------------------------------------------------------------\n",
    "if merged_df[\"class_label\"].isnull().any():\n",
    "    missing = merged_df[\"class_label\"].isnull().sum()\n",
    "    print(f\"Warning: {missing} sessions missing labels (filled with 0).\")\n",
    "    merged_df[\"class_label\"].fillna(0, inplace=True)\n",
    "\n",
    "y_labels = merged_df[\"class_label\"].astype(int).values\n",
    "\n",
    "print(\"\\n--- Verification ---\")\n",
    "print(f\"y_labels shape: {y_labels.shape}\")\n",
    "unique, counts = np.unique(y_labels, return_counts=True)\n",
    "print(f\"Class distribution: {dict(zip(unique, counts))}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5. Save aligned labels\n",
    "# -------------------------------------------------------------------------\n",
    "np.save(PATHS[\"y_labels\"], y_labels)\n",
    "print(f\"\\nSaved session-aligned labels to: {PATHS['y_labels']}\")"
   ],
   "id": "456a9cbac0eca3ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded labels.csv with 3 participants.\n",
      "Loaded 6 participant IDs and 6 screen IDs.\n",
      "Identified 6 unique participant-screen sessions.\n",
      "\n",
      "--- Verification ---\n",
      "y_labels shape: (6,)\n",
      "Class distribution: {0: 4, 1: 2}\n",
      "\n",
      "Saved session-aligned labels to: /mnt/c/Users/Consti/PycharmProjects/BachelorCode/sequential_classification/experiment_manual_selection/inputs/y_labels.npy\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Validation: Label Alignment Check\n",
    "\n",
    "**Purpose**\n",
    "Ensures that:\n",
    "1. The number of labels matches the number of sessions in `X_fix.npy`.\n",
    "2. The order of participants and screens is consistent across arrays.\n",
    "3. No missing or unexpected label values are present."
   ],
   "id": "e1797380de0ea8dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T15:41:21.658102Z",
     "start_time": "2025-10-20T15:41:21.639969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# 1. LOAD ARRAYS\n",
    "# ==============================================================================\n",
    "X_fix_path = PATHS[\"X_fix\"]\n",
    "y_labels_path = PATHS[\"y_labels\"]\n",
    "p_ids_path = PATHS[\"participant_ids\"]\n",
    "s_ids_path = PATHS[\"screen_ids\"]\n",
    "\n",
    "X_fix = np.load(X_fix_path)\n",
    "y_labels = np.load(y_labels_path)\n",
    "participant_ids = np.load(p_ids_path)\n",
    "screen_ids = np.load(s_ids_path)\n",
    "\n",
    "print(f\"Loaded X_fix: {X_fix.shape}\")\n",
    "print(f\"Loaded y_labels: {y_labels.shape}\")\n",
    "print(f\"Loaded participant_ids: {participant_ids.shape}\")\n",
    "print(f\"Loaded screen_ids: {screen_ids.shape}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. BASIC CONSISTENCY CHECKS\n",
    "# ==============================================================================\n",
    "n_screens_fix = X_fix.shape[0]\n",
    "n_labels = y_labels.shape[0]\n",
    "n_participants = participant_ids.shape[0]\n",
    "\n",
    "assert n_screens_fix == n_labels == n_participants, (\n",
    "    f\"Mismatch detected:\\n\"\n",
    "    f\"X_fix = {n_screens_fix}, y_labels = {n_labels}, participant_ids = {n_participants}\"\n",
    ")\n",
    "print(\"Array lengths are perfectly aligned.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. CLASS BALANCE & LABEL VALIDATION\n",
    "# ==============================================================================\n",
    "unique, counts = np.unique(y_labels, return_counts=True)\n",
    "print(\"\\nLabel distribution:\")\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"  Class {u}: {c} ({(c / len(y_labels)) * 100:.1f}%)\")\n",
    "\n",
    "if np.isnan(y_labels).any():\n",
    "    print(\"Warning: NaN values detected in labels.\")\n",
    "else:\n",
    "    print(\"No missing label values found.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. PARTICIPANT–SCREEN ORDER CHECK\n",
    "# ==============================================================================\n",
    "unique_pairs = len({tuple(x) for x in zip(participant_ids, screen_ids)})\n",
    "if unique_pairs != len(y_labels):\n",
    "    print(f\"Warning: Found {len(y_labels) - unique_pairs} duplicate session pairs.\")\n",
    "else:\n",
    "    print(\"Each (participant_id, screen_id) pair is unique and aligned.\")"
   ],
   "id": "4fe3d6bdc4ac85cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded X_fix: (6, 128, 13)\n",
      "Loaded y_labels: (6,)\n",
      "Loaded participant_ids: (6,)\n",
      "Loaded screen_ids: (6,)\n",
      "Array lengths are perfectly aligned.\n",
      "\n",
      "Label distribution:\n",
      "  Class 0: 4 (66.7%)\n",
      "  Class 1: 2 (33.3%)\n",
      "No missing label values found.\n",
      "Each (participant_id, screen_id) pair is unique and aligned.\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 4: Prepare Integer-Encoded Linguistic Input Sequences\n",
    "\n",
    "**Purpose**\n",
    "\n",
    "This step prepares the categorical linguistic input features used by the Hybrid BiLSTM model:\n",
    "- `X_pos.npy` → Integer-encoded simplified Part-of-Speech (PoS) sequences\n",
    "- `X_con.npy` → Integer-encoded content-word indicator sequences\n",
    "\n",
    "These complement the fixation-level features (`X_fix.npy`) and label arrays (`y_labels.npy`) created earlier.\n",
    "\n",
    "**Outputs**\n",
    "| File | Description | Shape |\n",
    "|------|--------------|--------|\n",
    "| `X_pos.npy` | Integer-encoded PoS tag sequences | `(n_screens, 128)` |\n",
    "| `X_con.npy` | Integer-encoded content-word sequences | `(n_screens, 128)` |\n",
    "| `pos_vocab.json` | PoS vocabulary mapping | `{tag: index}` |\n",
    "| `con_vocab.json` | Content-word vocabulary mapping | `{category: index}` |\n",
    "\n",
    "All output files are stored in the current experiment’s `inputs/` directory\n",
    "(e.g., `experiment_manual_selection/inputs/`)."
   ],
   "id": "963e7a7f46d2c0a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T15:41:21.964504Z",
     "start_time": "2025-10-20T15:41:21.936108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 1. Configuration\n",
    "# --------------------------------------------------------------------------\n",
    "DATA_DIR = PATHS[\"inputs\"]  # your experiment’s input directory\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Source file (either real merged ET data or dummy file)\n",
    "RAW_FILE = PATHS[\"data\"]  # Use dummy dataset for reproducibility\n",
    "\n",
    "# Key columns\n",
    "PARTICIPANT_COLUMN = \"Participant_unique\"\n",
    "SCREEN_COLUMN = \"screenid\"\n",
    "SORT_COLUMN = \"index\"  # ensures within-screen fixation order\n",
    "SIMPL_POS_COLUMN = \"simplified_pos\"\n",
    "CONTENT_COLUMN = \"is_content_word\"\n",
    "SEQUENCE_LENGTH = 128\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2. Helper Function\n",
    "# --------------------------------------------------------------------------\n",
    "def prepare_categorical_sequence(df, category_col, participant_col, screen_col,\n",
    "                                 sequence_length=128, sort_col=None):\n",
    "    \"\"\"\n",
    "    Converts a categorical text column to integer sequences grouped by\n",
    "    participant × screen, padded to uniform length.\n",
    "    \"\"\"\n",
    "    if sort_col and sort_col in df.columns:\n",
    "        df = df.sort_values(by=sort_col).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Encoding categorical column: {category_col}\")\n",
    "    integer_codes, unique_categories = pd.factorize(df[category_col].astype(str))\n",
    "    df[f\"{category_col}_id\"] = integer_codes\n",
    "    vocab = {cat: idx for idx, cat in enumerate(unique_categories)}\n",
    "    print(f\" → {len(vocab)} unique categories\")\n",
    "\n",
    "    # Group by participant × screen\n",
    "    grouped = df.groupby([participant_col, screen_col])[f\"{category_col}_id\"].apply(list)\n",
    "\n",
    "    # Pad each screen’s sequence\n",
    "    X_padded = pad_sequences(\n",
    "        grouped.tolist(),\n",
    "        maxlen=sequence_length,\n",
    "        padding=\"post\",\n",
    "        truncating=\"post\"\n",
    "    )\n",
    "    return X_padded, vocab\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3. Load DataFrame and Apply Encoding\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"Loading dataset...\")\n",
    "main_df = pd.read_csv(RAW_FILE, sep=\"\\t\", low_memory=False)\n",
    "if \"index\" not in df.columns:\n",
    "    df[\"index\"] = range(len(df))\n",
    "\n",
    "# --- Define required linguistic columns ---\n",
    "REQUIRED_COLS = [\"pos_merged\", \"aoi_composition\"]\n",
    "missing_cols = [c for c in REQUIRED_COLS if c not in df.columns]\n",
    "if missing_cols:\n",
    "    raise KeyError(f\"Missing required columns: {missing_cols}\")\n",
    "\n",
    "print(f\"All required columns present: {REQUIRED_COLS}\")\n",
    "\n",
    "print(\"\\nPreparing integer-encoded POS sequences...\")\n",
    "X_pos, pos_vocab = prepare_categorical_sequence(\n",
    "    main_df, SIMPL_POS_COLUMN,\n",
    "    participant_col=PARTICIPANT_COLUMN,\n",
    "    screen_col=SCREEN_COLUMN,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    sort_col=SORT_COLUMN\n",
    ")\n",
    "\n",
    "print(\"\\nPreparing integer-encoded Content-Word sequences...\")\n",
    "X_con, con_vocab = prepare_categorical_sequence(\n",
    "    main_df, CONTENT_COLUMN,\n",
    "    participant_col=PARTICIPANT_COLUMN,\n",
    "    screen_col=SCREEN_COLUMN,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    sort_col=SORT_COLUMN\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 4. Save Outputs\n",
    "# --------------------------------------------------------------------------\n",
    "np.save(os.path.join(DATA_DIR, \"X_pos.npy\"), X_pos)\n",
    "np.save(os.path.join(DATA_DIR, \"X_con.npy\"), X_con)\n",
    "\n",
    "with open(os.path.join(DATA_DIR, \"pos_vocab.json\"), \"w\") as f:\n",
    "    json.dump(pos_vocab, f, indent=2)\n",
    "with open(os.path.join(DATA_DIR, \"con_vocab.json\"), \"w\") as f:\n",
    "    json.dump(con_vocab, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved integer-encoded arrays and vocabularies.\")\n",
    "print(f\"X_pos shape: {X_pos.shape}\")\n",
    "print(f\"X_con shape: {X_con.shape}\")"
   ],
   "id": "28377e252432ad3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "All required columns present: ['pos_merged', 'aoi_composition']\n",
      "\n",
      "Preparing integer-encoded POS sequences...\n",
      "Encoding categorical column: simplified_pos\n",
      " → 3 unique categories\n",
      "\n",
      "Preparing integer-encoded Content-Word sequences...\n",
      "Encoding categorical column: is_content_word\n",
      " → 2 unique categories\n",
      "\n",
      "Saved integer-encoded arrays and vocabularies.\n",
      "X_pos shape: (6, 128)\n",
      "X_con shape: (6, 128)\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 5: Model Training — Stacked Multi-Input BiLSTM (BEyeLSTM)\n",
    "\n",
    "**Overview**\n",
    "\n",
    "This step defines and trains the final Hybrid BiLSTM (BEyeLSTM) model,\n",
    "which integrates fixation-based, syntactic, and semantic information through\n",
    "three modality-specific subnetworks.\n",
    "Each branch (Fixation, Part-of-Speech, and Content-Word) consists of two\n",
    "stacked Bidirectional LSTM layers followed by fully connected layers,\n",
    "before being merged into a unified representation for classification.\n",
    "\n",
    "Training is performed using grouped cross-validation (`GroupKFold`) to ensure\n",
    "participant-level independence across folds.\n",
    "The evaluation includes standard performance metrics such as Accuracy, AUC,\n",
    "PR-AUC, Precision, Recall, and F1-score for both expert and non-expert classes.\n",
    "\n",
    "**Input**\n",
    "- Preprocessed sequential feature arrays and label data (`PATHS[\"inputs\"]`).\n",
    "\n",
    "**Output**\n",
    "- Cross-validation performance summary (`PATHS[\"outputs\"]`).\n",
    "- Trained model checkpoints and logs (`PATHS[\"outputs\"]`, `PATHS[\"logs\"]`)."
   ],
   "id": "607d2f7058dd38a4"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-20T15:42:00.423482Z",
     "start_time": "2025-10-20T15:41:22.232836Z"
    }
   },
   "source": [
    "# ==============================================================================\n",
    "# 1. REPRODUCIBILITY CONTROL\n",
    "# ==============================================================================\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # Suppress INFO and WARNING messages\n",
    "\n",
    "# Fix all random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. MODEL DEFINITION\n",
    "# ==============================================================================\n",
    "\n",
    "def build_hybrid_bilstm_model(\n",
    "    fixation_shape: tuple,\n",
    "    pos_vocab_size: int,\n",
    "    content_vocab_size: int,\n",
    "    sequence_length: int = 128,\n",
    "    lstm_units: int = 16,\n",
    "    embedding_dim: int = 8,\n",
    "    dropout_rate: float = 0.5,\n",
    "    dense_units_branch: tuple = (32, 16)\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Build a hybrid BiLSTM model combining fixation-based, syntactic, and\n",
    "    content-level information. The architecture corresponds to the \"Hybrid\n",
    "    BiLSTM\" described in Chapter 5.1 of the thesis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fixation_shape : tuple\n",
    "        Shape of fixation input (timesteps, features).\n",
    "    pos_vocab_size : int\n",
    "        Vocabulary size for POS tokens.\n",
    "    content_vocab_size : int\n",
    "        Vocabulary size for content word tokens.\n",
    "    sequence_length : int, optional\n",
    "        Sequence length for token inputs (default=128).\n",
    "    lstm_units : int, optional\n",
    "        Number of LSTM units per direction (default=16).\n",
    "    embedding_dim : int, optional\n",
    "        Embedding dimension for token branches (default=8).\n",
    "    dropout_rate : float, optional\n",
    "        Dropout rate for regularization (default=0.5).\n",
    "    dense_units_branch : tuple, optional\n",
    "        Dense-layer configuration for each subnetwork (default=(32,16)).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : tf.keras.Model\n",
    "        Compiled Keras model ready for training and evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    # ----- Fixation branch (AOI-level eye-tracking features) -----\n",
    "    fixation_input = Input(shape=fixation_shape, name=\"fixation_input\")\n",
    "    fix = Bidirectional(LSTM(lstm_units, return_sequences=True), name=\"fix_bilstm_1\")(fixation_input)\n",
    "    fix = Bidirectional(LSTM(lstm_units), name=\"fix_bilstm_2\")(fix)\n",
    "    fix = Dropout(dropout_rate)(fix)\n",
    "    fix = Dense(dense_units_branch[0], activation=\"relu\", name=\"fix_dense_1\")(fix)\n",
    "    fix = Dropout(dropout_rate)(fix)\n",
    "    fix = Dense(dense_units_branch[1], activation=\"relu\", name=\"fix_dense_2\")(fix)\n",
    "\n",
    "    # ----- POS branch (syntactic representation) -----\n",
    "    pos_input = Input(shape=(sequence_length,), name=\"pos_input\")\n",
    "    pos = Embedding(input_dim=pos_vocab_size, output_dim=embedding_dim, name=\"pos_embedding\")(pos_input)\n",
    "    pos = Bidirectional(LSTM(lstm_units, return_sequences=True), name=\"pos_bilstm_1\")(pos)\n",
    "    pos = Bidirectional(LSTM(lstm_units), name=\"pos_bilstm_2\")(pos)\n",
    "    pos = Dropout(dropout_rate)(pos)\n",
    "    pos = Dense(dense_units_branch[0], activation=\"relu\", name=\"pos_dense_1\")(pos)\n",
    "    pos = Dropout(dropout_rate)(pos)\n",
    "    pos = Dense(dense_units_branch[1], activation=\"relu\", name=\"pos_dense_2\")(pos)\n",
    "\n",
    "    # ----- Content-word branch (semantic information) -----\n",
    "    content_input = Input(shape=(sequence_length,), name=\"content_input\")\n",
    "    con = Embedding(input_dim=content_vocab_size, output_dim=embedding_dim, name=\"content_embedding\")(content_input)\n",
    "    con = Bidirectional(LSTM(lstm_units, return_sequences=True), name=\"content_bilstm_1\")(con)\n",
    "    con = Bidirectional(LSTM(lstm_units), name=\"content_bilstm_2\")(con)\n",
    "    con = Dropout(dropout_rate)(con)\n",
    "    con = Dense(dense_units_branch[0], activation=\"relu\", name=\"content_dense_1\")(con)\n",
    "    con = Dropout(dropout_rate)(con)\n",
    "    con = Dense(dense_units_branch[1], activation=\"relu\", name=\"content_dense_2\")(con)\n",
    "\n",
    "    # ----- Fusion and classification head -----\n",
    "    merged_features = concatenate([fix, pos, con], name=\"merged_features\")\n",
    "    classifier = Dense(32, activation=\"relu\", name=\"classifier_dense\")(merged_features)\n",
    "    output = Dense(1, activation=\"sigmoid\", name=\"output\")(classifier)\n",
    "\n",
    "    model = Model(inputs=[fixation_input, pos_input, content_input], outputs=output, name=\"HybridBiLSTM\")\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "DATA_DIR = PATHS[\"inputs\"]\n",
    "RESULTS_DIR = PATHS[\"outputs\"]\n",
    "LOG_DIR = PATHS[\"logs\"]\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# File paths (as described in Chapter 3: Data)\n",
    "FIXATION_PATH = os.path.join(DATA_DIR, \"X_fix.npy\")\n",
    "POS_PATH = os.path.join(DATA_DIR, \"X_pos.npy\")\n",
    "CONTENT_PATH = os.path.join(DATA_DIR, \"X_con.npy\")\n",
    "LABELS_PATH = os.path.join(DATA_DIR, \"y_labels.npy\")\n",
    "PARTICIPANT_IDS_PATH = os.path.join(DATA_DIR, \"participant_ids.npy\")\n",
    "\n",
    "# Cross-validation and training parameters (Chapter 4.3)\n",
    "NUM_FOLDS = 5\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "EARLY_STOP_PATIENCE = 10\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. DATA LOADING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Loading input arrays...\")\n",
    "X_fix = np.load(FIXATION_PATH)\n",
    "X_pos = np.load(POS_PATH)\n",
    "X_content = np.load(CONTENT_PATH)\n",
    "y = np.load(LABELS_PATH)\n",
    "participant_ids = np.load(PARTICIPANT_IDS_PATH)\n",
    "\n",
    "# Vocabulary sizes derived from maximum token ID\n",
    "pos_vocab_size = int(X_pos.max()) + 1\n",
    "content_vocab_size = int(X_content.max()) + 1\n",
    "\n",
    "print(f\"POS vocabulary size: {pos_vocab_size}\")\n",
    "print(f\"Content-word vocabulary size: {content_vocab_size}\")\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. GROUPED K-FOLD CROSS-VALIDATION TRAINING\n",
    "# ==============================================================================\n",
    "\n",
    "print(f\"\\nStarting {NUM_FOLDS}-fold grouped cross-validation...\")\n",
    "unique_participants = np.unique(participant_ids)\n",
    "NUM_FOLDS = min(NUM_FOLDS, len(unique_participants))\n",
    "\n",
    "gkf = GroupKFold(n_splits=NUM_FOLDS)\n",
    "print(f\"Using {NUM_FOLDS}-fold cross-validation for {len(unique_participants)} participants.\")\n",
    "cv_results = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(gkf.split(X_fix, y, groups=participant_ids), start=1):\n",
    "    print(f\"\\n--- Fold {fold_idx}/{NUM_FOLDS} ---\")\n",
    "\n",
    "    # Participant-independent data split\n",
    "    X_train = [X_fix[train_idx], X_pos[train_idx], X_content[train_idx]]\n",
    "    X_test = [X_fix[test_idx], X_pos[test_idx], X_content[test_idx]]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # Ensure model independence between folds\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = build_hybrid_bilstm_model(\n",
    "        fixation_shape=(X_fix.shape[1], X_fix.shape[2]),\n",
    "        pos_vocab_size=pos_vocab_size,\n",
    "        content_vocab_size=content_vocab_size\n",
    "    )\n",
    "\n",
    "    # Define callbacks\n",
    "    fold_log_dir = os.path.join(PATHS[\"logs\"], f\"fold_{fold_idx}\")\n",
    "    os.makedirs(fold_log_dir, exist_ok=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=EARLY_STOP_PATIENCE,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=fold_log_dir,\n",
    "        histogram_freq=0,\n",
    "        write_graph=False,\n",
    "        write_images=False\n",
    "    )\n",
    "\n",
    "    # ----- Training -----\n",
    "    print(\"Training model...\")\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[early_stopping, tensorboard_cb],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    # ----- Evaluation -----\n",
    "    print(\"Evaluating model...\")\n",
    "    y_pred_proba = model.predict(X_test).ravel()\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "    loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    # Handle missing-class folds\n",
    "    if len(np.unique(y_test)) < 2:\n",
    "        print(f\"Fold {fold_idx}: only one class present in y_test — skipping AUC.\")\n",
    "        auc = np.nan\n",
    "    else:\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, labels=[0, 1], zero_division=0\n",
    "    )\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "\n",
    "    # Ensure confusion matrix is 2×2 even if one class missing\n",
    "    if cm.shape != (2, 2):\n",
    "        full_cm = np.zeros((2, 2), dtype=int)\n",
    "        for i, label in enumerate(np.unique(y_test)):\n",
    "            full_cm[label, label] = cm[i, i]\n",
    "        cm = full_cm\n",
    "\n",
    "    # Pad metrics for absent classes\n",
    "    while len(prec) < 2:\n",
    "        prec, rec, f1 = np.append(prec, 0.0), np.append(rec, 0.0), np.append(f1, 0.0)\n",
    "\n",
    "    cv_results.append({\n",
    "        \"fold\": fold_idx,\n",
    "        \"accuracy\": round(acc, 4),\n",
    "        \"auc\": round(auc, 4) if not np.isnan(auc) else \"\",\n",
    "        \"loss\": round(loss, 4),\n",
    "        \"precision_expert\": round(prec[1], 4),\n",
    "        \"recall_expert\": round(rec[1], 4),\n",
    "        \"f1_expert\": round(f1[1], 4),\n",
    "        \"precision_nonexpert\": round(prec[0], 4),\n",
    "        \"recall_nonexpert\": round(rec[0], 4),\n",
    "        \"f1_nonexpert\": round(f1[0], 4),\n",
    "        \"cm_TN\": int(cm[0, 0]),\n",
    "        \"cm_FP\": int(cm[0, 1]),\n",
    "        \"cm_FN\": int(cm[1, 0]),\n",
    "        \"cm_TP\": int(cm[1, 1]),\n",
    "    })\n",
    "\n",
    "    fold_model_path = os.path.join(PATHS[\"outputs\"], f\"hybrid_bilstm_fold{fold_idx}.keras\")\n",
    "    model.save(fold_model_path)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. AGGREGATE RESULTS AND SAVE OUTPUTS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- Cross-validation complete ---\")\n",
    "results_df = pd.DataFrame(cv_results)\n",
    "\n",
    "# Compute summary stats\n",
    "summary = results_df.describe().transpose()[[\"mean\", \"std\"]].round(3)\n",
    "summary.index.name = \"metric\"\n",
    "print(\"\\n--- Summary ---\")\n",
    "print(summary)\n",
    "\n",
    "# Save detailed results safely\n",
    "results_csv_path = os.path.join(PATHS[\"outputs\"], \"hybrid_bilstm_cv_results.csv\")\n",
    "results_df.to_csv(\n",
    "    results_csv_path,\n",
    "    sep=\",\",\n",
    "    index=False,\n",
    "    quoting=csv.QUOTE_MINIMAL,\n",
    "    float_format=\"%.4f\"\n",
    ")\n",
    "print(f\"Detailed results saved to {results_csv_path}\")\n",
    "\n",
    "# Save summary table\n",
    "summary_csv_path = os.path.join(PATHS[\"outputs\"], \"hybrid_bilstm_summary.csv\")\n",
    "summary.to_csv(summary_csv_path)\n",
    "print(f\"Summary statistics saved to {summary_csv_path}\")\n",
    "\n",
    "# Save final model instance\n",
    "final_model_path = os.path.join(PATHS[\"outputs\"], \"hybrid_bilstm_final.keras\")\n",
    "model.save(final_model_path)\n",
    "print(f\"Final model saved to {final_model_path}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input arrays...\n",
      "POS vocabulary size: 3\n",
      "Content-word vocabulary size: 2\n",
      "\n",
      "Starting 5-fold grouped cross-validation...\n",
      "Using 3-fold cross-validation for 3 participants.\n",
      "\n",
      "--- Fold 1/3 ---\n",
      "Training model...\n",
      "Epoch 1/100\n",
      "1/1 - 9s - loss: 0.6820 - accuracy: 0.7500 - val_loss: 0.6750 - val_accuracy: 1.0000 - 9s/epoch - 9s/step\n",
      "Epoch 2/100\n",
      "1/1 - 0s - loss: 0.6621 - accuracy: 0.7500 - val_loss: 0.6740 - val_accuracy: 1.0000 - 74ms/epoch - 74ms/step\n",
      "Epoch 3/100\n",
      "1/1 - 0s - loss: 0.7029 - accuracy: 0.2500 - val_loss: 0.6748 - val_accuracy: 1.0000 - 69ms/epoch - 69ms/step\n",
      "Epoch 4/100\n",
      "1/1 - 0s - loss: 0.6579 - accuracy: 1.0000 - val_loss: 0.6742 - val_accuracy: 1.0000 - 68ms/epoch - 68ms/step\n",
      "Epoch 5/100\n",
      "1/1 - 0s - loss: 0.6986 - accuracy: 0.5000 - val_loss: 0.6733 - val_accuracy: 1.0000 - 68ms/epoch - 68ms/step\n",
      "Epoch 6/100\n",
      "1/1 - 0s - loss: 0.6581 - accuracy: 0.7500 - val_loss: 0.6714 - val_accuracy: 1.0000 - 68ms/epoch - 68ms/step\n",
      "Epoch 7/100\n",
      "1/1 - 0s - loss: 0.6880 - accuracy: 0.5000 - val_loss: 0.6704 - val_accuracy: 1.0000 - 72ms/epoch - 72ms/step\n",
      "Epoch 8/100\n",
      "1/1 - 0s - loss: 0.7082 - accuracy: 0.5000 - val_loss: 0.6689 - val_accuracy: 1.0000 - 71ms/epoch - 71ms/step\n",
      "Epoch 9/100\n",
      "1/1 - 0s - loss: 0.6726 - accuracy: 0.7500 - val_loss: 0.6677 - val_accuracy: 1.0000 - 69ms/epoch - 69ms/step\n",
      "Epoch 10/100\n",
      "1/1 - 0s - loss: 0.6997 - accuracy: 0.7500 - val_loss: 0.6667 - val_accuracy: 1.0000 - 69ms/epoch - 69ms/step\n",
      "Epoch 11/100\n",
      "1/1 - 0s - loss: 0.7061 - accuracy: 0.5000 - val_loss: 0.6670 - val_accuracy: 1.0000 - 67ms/epoch - 67ms/step\n",
      "Epoch 12/100\n",
      "1/1 - 0s - loss: 0.6733 - accuracy: 0.7500 - val_loss: 0.6672 - val_accuracy: 1.0000 - 64ms/epoch - 64ms/step\n",
      "Epoch 13/100\n",
      "1/1 - 0s - loss: 0.7053 - accuracy: 0.5000 - val_loss: 0.6684 - val_accuracy: 1.0000 - 63ms/epoch - 63ms/step\n",
      "Epoch 14/100\n",
      "1/1 - 0s - loss: 0.6894 - accuracy: 0.7500 - val_loss: 0.6697 - val_accuracy: 1.0000 - 69ms/epoch - 69ms/step\n",
      "Epoch 15/100\n",
      "1/1 - 0s - loss: 0.6667 - accuracy: 0.5000 - val_loss: 0.6713 - val_accuracy: 1.0000 - 65ms/epoch - 65ms/step\n",
      "Epoch 16/100\n",
      "1/1 - 0s - loss: 0.7283 - accuracy: 0.2500 - val_loss: 0.6724 - val_accuracy: 1.0000 - 67ms/epoch - 67ms/step\n",
      "Epoch 17/100\n",
      "1/1 - 0s - loss: 0.7230 - accuracy: 0.0000e+00 - val_loss: 0.6734 - val_accuracy: 1.0000 - 65ms/epoch - 65ms/step\n",
      "Epoch 18/100\n",
      "1/1 - 0s - loss: 0.7073 - accuracy: 0.5000 - val_loss: 0.6747 - val_accuracy: 1.0000 - 68ms/epoch - 68ms/step\n",
      "Epoch 19/100\n",
      "1/1 - 0s - loss: 0.6656 - accuracy: 0.5000 - val_loss: 0.6763 - val_accuracy: 1.0000 - 67ms/epoch - 67ms/step\n",
      "Epoch 20/100\n",
      "1/1 - 0s - loss: 0.7001 - accuracy: 0.2500 - val_loss: 0.6790 - val_accuracy: 0.5000 - 78ms/epoch - 78ms/step\n",
      "Evaluating model...\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Fold 1: only one class present in y_test — skipping AUC.\n",
      "\n",
      "--- Fold 2/3 ---\n",
      "Training model...\n",
      "Epoch 1/100\n",
      "1/1 - 9s - loss: 0.7186 - accuracy: 0.2500 - val_loss: 0.6810 - val_accuracy: 1.0000 - 9s/epoch - 9s/step\n",
      "Epoch 2/100\n",
      "1/1 - 0s - loss: 0.7008 - accuracy: 0.5000 - val_loss: 0.6913 - val_accuracy: 0.5000 - 70ms/epoch - 70ms/step\n",
      "Epoch 3/100\n",
      "1/1 - 0s - loss: 0.6753 - accuracy: 0.5000 - val_loss: 0.7024 - val_accuracy: 0.0000e+00 - 63ms/epoch - 63ms/step\n",
      "Epoch 4/100\n",
      "1/1 - 0s - loss: 0.6809 - accuracy: 0.7500 - val_loss: 0.7141 - val_accuracy: 0.0000e+00 - 62ms/epoch - 62ms/step\n",
      "Epoch 5/100\n",
      "1/1 - 0s - loss: 0.6537 - accuracy: 1.0000 - val_loss: 0.7264 - val_accuracy: 0.0000e+00 - 62ms/epoch - 62ms/step\n",
      "Epoch 6/100\n",
      "1/1 - 0s - loss: 0.6402 - accuracy: 1.0000 - val_loss: 0.7387 - val_accuracy: 0.0000e+00 - 65ms/epoch - 65ms/step\n",
      "Epoch 7/100\n",
      "1/1 - 0s - loss: 0.6224 - accuracy: 1.0000 - val_loss: 0.7525 - val_accuracy: 0.0000e+00 - 64ms/epoch - 64ms/step\n",
      "Epoch 8/100\n",
      "1/1 - 0s - loss: 0.5986 - accuracy: 1.0000 - val_loss: 0.7687 - val_accuracy: 0.0000e+00 - 63ms/epoch - 63ms/step\n",
      "Epoch 9/100\n",
      "1/1 - 0s - loss: 0.5932 - accuracy: 1.0000 - val_loss: 0.7873 - val_accuracy: 0.0000e+00 - 62ms/epoch - 62ms/step\n",
      "Epoch 10/100\n",
      "1/1 - 0s - loss: 0.5650 - accuracy: 1.0000 - val_loss: 0.8076 - val_accuracy: 0.0000e+00 - 62ms/epoch - 62ms/step\n",
      "Epoch 11/100\n",
      "1/1 - 0s - loss: 0.5708 - accuracy: 1.0000 - val_loss: 0.8315 - val_accuracy: 0.0000e+00 - 78ms/epoch - 78ms/step\n",
      "Evaluating model...\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "Fold 2: only one class present in y_test — skipping AUC.\n",
      "\n",
      "--- Fold 3/3 ---\n",
      "Training model...\n",
      "Epoch 1/100\n",
      "1/1 - 9s - loss: 0.6746 - accuracy: 0.5000 - val_loss: 0.6644 - val_accuracy: 1.0000 - 9s/epoch - 9s/step\n",
      "Epoch 2/100\n",
      "1/1 - 0s - loss: 0.6896 - accuracy: 0.5000 - val_loss: 0.6593 - val_accuracy: 1.0000 - 76ms/epoch - 76ms/step\n",
      "Epoch 3/100\n",
      "1/1 - 0s - loss: 0.6873 - accuracy: 0.5000 - val_loss: 0.6576 - val_accuracy: 1.0000 - 68ms/epoch - 68ms/step\n",
      "Epoch 4/100\n",
      "1/1 - 0s - loss: 0.6947 - accuracy: 0.7500 - val_loss: 0.6576 - val_accuracy: 1.0000 - 62ms/epoch - 62ms/step\n",
      "Epoch 5/100\n",
      "1/1 - 0s - loss: 0.6892 - accuracy: 0.5000 - val_loss: 0.6583 - val_accuracy: 1.0000 - 62ms/epoch - 62ms/step\n",
      "Epoch 6/100\n",
      "1/1 - 0s - loss: 0.7101 - accuracy: 0.5000 - val_loss: 0.6609 - val_accuracy: 1.0000 - 63ms/epoch - 63ms/step\n",
      "Epoch 7/100\n",
      "1/1 - 0s - loss: 0.6986 - accuracy: 0.5000 - val_loss: 0.6631 - val_accuracy: 1.0000 - 73ms/epoch - 73ms/step\n",
      "Epoch 8/100\n",
      "1/1 - 0s - loss: 0.6985 - accuracy: 0.2500 - val_loss: 0.6653 - val_accuracy: 1.0000 - 85ms/epoch - 85ms/step\n",
      "Epoch 9/100\n",
      "1/1 - 0s - loss: 0.7112 - accuracy: 0.5000 - val_loss: 0.6672 - val_accuracy: 1.0000 - 94ms/epoch - 94ms/step\n",
      "Epoch 10/100\n",
      "1/1 - 0s - loss: 0.7035 - accuracy: 0.5000 - val_loss: 0.6691 - val_accuracy: 1.0000 - 95ms/epoch - 95ms/step\n",
      "Epoch 11/100\n",
      "1/1 - 0s - loss: 0.6812 - accuracy: 0.5000 - val_loss: 0.6713 - val_accuracy: 1.0000 - 67ms/epoch - 67ms/step\n",
      "Epoch 12/100\n",
      "1/1 - 0s - loss: 0.6890 - accuracy: 0.5000 - val_loss: 0.6738 - val_accuracy: 1.0000 - 64ms/epoch - 64ms/step\n",
      "Epoch 13/100\n",
      "1/1 - 0s - loss: 0.7219 - accuracy: 0.5000 - val_loss: 0.6764 - val_accuracy: 1.0000 - 79ms/epoch - 79ms/step\n",
      "Evaluating model...\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Fold 3: only one class present in y_test — skipping AUC.\n",
      "\n",
      "--- Cross-validation complete ---\n",
      "\n",
      "--- Summary ---\n",
      "                      mean    std\n",
      "metric                           \n",
      "fold                 2.000  1.000\n",
      "accuracy             1.000  0.000\n",
      "loss                 0.668  0.012\n",
      "precision_expert     0.333  0.577\n",
      "recall_expert        0.333  0.577\n",
      "f1_expert            0.333  0.577\n",
      "precision_nonexpert  0.667  0.577\n",
      "recall_nonexpert     0.667  0.577\n",
      "f1_nonexpert         0.667  0.577\n",
      "cm_TN                1.333  1.155\n",
      "cm_FP                0.000  0.000\n",
      "cm_FN                0.000  0.000\n",
      "cm_TP                0.667  1.155\n",
      "Detailed results saved to /mnt/c/Users/Consti/PycharmProjects/BachelorCode/sequential_classification/experiment_manual_selection/outputs/hybrid_bilstm_cv_results.csv\n",
      "Summary statistics saved to /mnt/c/Users/Consti/PycharmProjects/BachelorCode/sequential_classification/experiment_manual_selection/outputs/hybrid_bilstm_summary.csv\n",
      "Final model saved to /mnt/c/Users/Consti/PycharmProjects/BachelorCode/sequential_classification/experiment_manual_selection/outputs/hybrid_bilstm_final.keras\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 6: Evaluation and Aggregation of Cross-Validation Results\n",
    "\n",
    "**Overview**\n",
    "\n",
    "This step aggregates and compares BiLSTM cross-validation results across all\n",
    "experiment versions (e.g., *manual selection* and *feature analysis*).\n",
    "It scans the `sequential_classification/experiment_*/outputs/` directories for\n",
    "`hybrid_bilstm_cv_results.csv` files, computes mean ± standard-deviation metrics,\n",
    "and exports a unified summary table.\n",
    "\n",
    "**Input**\n",
    "- `sequential_classification/experiment_manual_selection/outputs/hybrid_bilstm_cv_results.csv`\n",
    "- `sequential_classification/experiment_feature_analysis/outputs/hybrid_bilstm_cv_results.csv`\n",
    "\n",
    "**Output**\n",
    "- Combined comparison table\n",
    "  → `{PROJECT_ROOT}/sequential_classification/bilstm_summary_comparison.csv`"
   ],
   "id": "6e00278ba901c0c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T15:42:00.703083Z",
     "start_time": "2025-10-20T15:42:00.679645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# ==============================================================================\n",
    "PROJECT_ROOT = PATHS[\"project_root\"]\n",
    "SEQ_CLASS_DIR = os.path.join(PROJECT_ROOT, \"sequential_classification\")\n",
    "SUMMARY_FILE = os.path.join(SEQ_CLASS_DIR, \"bilstm_summary_comparison.csv\")\n",
    "\n",
    "# Pattern to find result CSVs in both experiment subdirectories\n",
    "pattern = os.path.join(SEQ_CLASS_DIR, \"experiment_*\", \"outputs\", \"hybrid_bilstm_cv_results.csv\")\n",
    "result_files = glob.glob(pattern)\n",
    "\n",
    "print(f\"Found {len(result_files)} result file(s) matching pattern:\\n  {pattern}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. LOAD AND SUMMARIZE\n",
    "# ==============================================================================\n",
    "if not result_files:\n",
    "    print(\"No result files found. Skipping summary generation.\")\n",
    "else:\n",
    "    summary_list = []\n",
    "\n",
    "    for file in result_files:\n",
    "        exp_name = os.path.basename(os.path.dirname(os.path.dirname(file)))  # e.g. 'experiment_manual_selection'\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        # Determine which metrics exist before summarizing\n",
    "        available_metrics = [col for col in [\"accuracy\", \"auc\", \"precision_expert\", \"recall_expert\", \"f1_expert\"] if col in df.columns]\n",
    "\n",
    "        summary = {\n",
    "            \"experiment\": exp_name,\n",
    "            \"file\": os.path.basename(file),\n",
    "        }\n",
    "        for metric in available_metrics:\n",
    "            summary[f\"mean_{metric}\"] = df[metric].mean()\n",
    "            summary[f\"std_{metric}\"] = df[metric].std()\n",
    "\n",
    "        summary_list.append(summary)\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_list)\n",
    "    print(\"\\n--- Aggregated Cross-Experiment Summary ---\")\n",
    "    print(summary_df.round(3))\n",
    "\n",
    "    # ==============================================================================\n",
    "    # 3. SAVE SUMMARY\n",
    "    # ==============================================================================\n",
    "    summary_df.to_csv(SUMMARY_FILE, index=False)\n",
    "    print(f\"\\nSummary comparison file saved to: {SUMMARY_FILE}\")\n"
   ],
   "id": "8c4091df879a3f19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 result file(s) matching pattern:\n",
      "  /mnt/c/Users/Consti/PycharmProjects/BachelorCode/sequential_classification/experiment_*/outputs/hybrid_bilstm_cv_results.csv\n",
      "\n",
      "--- Aggregated Cross-Experiment Summary ---\n",
      "                    experiment                          file  mean_accuracy  \\\n",
      "0  experiment_feature_analysis  hybrid_bilstm_cv_results.csv        0.50000   \n",
      "1  experiment_manual_selection  hybrid_bilstm_cv_results.csv        1.00000   \n",
      "\n",
      "   std_accuracy  mean_auc  std_auc  mean_precision_expert  \\\n",
      "0       0.50000       NaN      NaN                0.00000   \n",
      "1       0.00000       NaN      NaN                0.33300   \n",
      "\n",
      "   std_precision_expert  mean_recall_expert  std_recall_expert  \\\n",
      "0               0.00000             0.00000            0.00000   \n",
      "1               0.57700             0.33300            0.57700   \n",
      "\n",
      "   mean_f1_expert  std_f1_expert  \n",
      "0         0.00000        0.00000  \n",
      "1         0.33300        0.57700  \n",
      "\n",
      "Summary comparison file saved to: /mnt/c/Users/Consti/PycharmProjects/BachelorCode/sequential_classification/bilstm_summary_comparison.csv\n"
     ]
    }
   ],
   "execution_count": 58
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
